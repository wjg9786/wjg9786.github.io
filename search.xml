<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[基于frp通过shh访问家里内网机器linux系统]]></title>
    <url>%2F2019%2F10%2F01%2F%E5%9F%BA%E4%BA%8Efrp%E9%80%9A%E8%BF%87shh%E8%AE%BF%E9%97%AE%E5%AE%B6%E9%87%8C%E5%86%85%E7%BD%91%E6%9C%BA%E5%99%A8linux%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[基于frp通过shh访问家里内网机器(linux系统)一、本文目的如何连接家里没有固定公网的linux服务器。然后通过ssh远程操作跑实验（虽然还有 TeamViewer 这么好用的工具存在，但是ssh会比 TeamViewer 更稳定些，或者说2种方式都存在的话更保险一些）。要通过ssh链接不属于同一个内网的机器，需要一个外网IP来解决这个问题。同学介绍了一个工具，frp，可以解决这问题，下文将介绍基于frp通过ssh链接内网机器。 参考网址： github - frp：https://github.com/fatedier/frp/blob/master/README.md（有中文版） 二、相关说明&amp;&amp;介绍 frp ：frp 是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp, http, https 协议。frp的作用如下： 1）利用处于内网或防火墙后的机器，对外网环境提供 http 或 https 服务。 2）对于 http, https 服务支持基于域名的虚拟主机，支持自定义域名绑定，使多个域名可以共用一个80端口。 3）利用处于内网或防火墙后的机器，对外网环境提供 tcp 和 udp 服务，例如在家里通过 ssh 访问处于公司内网环境内的主机。 配置文件：frps.ini 和 frpc.ini 1）将 frps 及 frps.ini 放到具有公网 IP 的机器上（ 本人此处使用阿里云服务器提供的公网） 2）将 frpc 及 frpc.ini 放到处于内网环境的机器上（本人此处的机器就是放在家里且性能好的机器） 图示说明： 如下图所示，本文的目的就是实现“用户实际操作的PC”通过 ssh 去控制“服务器A”。其中，服务器B提供了外网IP，不可缺少。本文的前期准备包括：1）准备1台有外网的服务器B，本文选择阿里云服务器。2）一台你想要控制的服务器或者PC，本文是想要控制家里的一台性能较好的PC机。3）一台你实际操作的PC，如宿舍的笔记本电脑或者实验室普通的PC等。4）针对服务器A和服务器B，我们需要安装和配置go语言环境和frp环境。5）针对用户实际操作的PC，我们只需要安装ssh环境即可，如windows系统可以安装SecureCRT或者Putty；Ubuntu系统可以安装使用ssh或者安装putty。（注，下图最左侧框图中配置文件应该是：frpc.ini） 三、实现步骤：通过 ssh 访问内网机器1.安装&amp;&amp;配置go语言开发环境 参考网址： == https://www.runoob.com/go/go-environment.html == http://blog.csdn.net/tigerisland45/article/details/53447199 1）根据系统类型下载go语言安装包：https://www.golangtc.com/download 2）解压（ 此处的路径为/usr/local ）：tar -xvxf go1.9beta2.linux-amd64.tar.gz 3）建立Go工作空间： == 在/home目录下, 建立一个名为gopath(名字任意)的目录，在该目录中建立三个子目录(名字必须为src、pkg和bin)，其实只要创建src目录就可以了。 src – 里面每一个子目录，就是一个包。包内是Go的源码文件 pkg – 编译后生成的，包的目标文件 bin – 生成的可执行文件。 4）配置环境： 输入：gedit ~/.bashrc 或者 vim ~/.bashrc 在文件尾部添加以下命令： Go environmentexport PATH=$PATH:/usr/local/go/bin export GOPATH=/home/gopath 5）生效配置文件：source ~/.bashrc 6）测试go语言环境是否配置完成，输入：go version 下载frp源码并安装 1）下载源码 - 在 gopath 目录下执行命令：go get github.com/fatedier/frp （确保go语言已经安装和配置成功）执行命令后代码将会拷贝到 $GOPATH/src/github.com/fatedier/frp 目录下。 2）编译：进入下载后的源码根目录，执行【make】命令，等待编译完成。编译完成后， bin 目录下是编译好的可执行文件，conf 目录下是示例配置文件。 3）部署： == 将 ./bin/frps 和 ./conf/frps.ini 拷贝至服务器B任意目录。 == 将 ./bin/frpc 和 ./conf/frpc.ini 拷贝至服务器A任意目录。 == 根据要实现的功能修改两边的配置文件，详细内容见后续章节说明。 frps.ini [common]bind_addr = 0.0.0.0 用于接收 frpc 连接的端口bind_port = 7000log_file = ./frps.loglog_level = info ssh 为代理的自定义名称，可以有多个，不能重复，和frpc中名称对应[ssh]auth_token = 123bind_addr = 0.0.0.0 最后将通过此端口访问后端服务listen_port = 6000 frpc.ini [common]server_addr = XXX.XX.XX.XXserver_port = 7000log_file = ./frpc.loglog_level = infoauth_token = 123 [ssh]type = tcplocal_ip = 127.0.0.1local_port = 22remote_port = 6000use_encryption = true == 在服务器B执行 nohup ./frps &amp; 或者 nohup ./frps -c ./frps.ini &amp; 。 == 在服务器A执行 nohup ./frpc &amp; 或者 nohup ./frpc -c ./frpc.ini &amp; 。 == 通过 ssh -oPort=6000 {user}@x.x.x.x测试是否能够成功连接服务器A（{user}替换为服务器A上存在的真实用户） 四、总结 （服务器A + 服务器B）安装并配置go语言的环境 （服务器A + 服务器B）下载并安装frp （服务器A，被控制的PC）根据实际情况修改配置文件并运行：frpc.ini（nohup ./frpc -c ./frpc.ini &amp;） （服务器B，具有外网）根据实际情况修改配置文件并运行：frps.ini（nohup ./frps -c ./frps.ini &amp; ） （用户实际操作的PC）通过ssh链接服务器A，只需要执行语句【 ssh -oPort=6000 {user}@x.x.x.x 】即可。 注:ssh远程连接时是家里的用户名与密码，不是有公网那台的机器。 原文链接：https://blog.csdn.net/Houchaoqun_XMU/article/details/75226386]]></content>
      <categories>
        <category>测试工具</category>
      </categories>
      <tags>
        <tag>测试工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCPIP网络编程之优雅的断开套接字]]></title>
    <url>%2F2019%2F09%2F30%2FTCPIP%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B9%8B%E4%BC%98%E9%9B%85%E7%9A%84%E6%96%AD%E5%BC%80%E5%A5%97%E6%8E%A5%E5%AD%97%2F</url>
    <content type="text"><![CDATA[TCP/IP网络编程之优雅的断开套接字linux的close函数意味着完全断开连接，完全断开连接不仅指无法传输数据，而且也不能接收数据。因此，在某些情况下，通信一方调用close函数断开连接就显得不太优雅，如图： ​ 单方面断开连接 上图描述的是两台主机正在进行双向通信，主机A发送完最后的数据后，调用close函数断开连接，之后主机A无法再接收主机B传输的数据。实际上，是完全无法调用与接收数据相关的函数。最终，由主机B传输而主机A接收的数据也销毁了。 首先需要区分一下关闭socket和关闭TCP连接的区别，关闭TCP连接是指TCP协议层的东西，就是两个TCP端之间交换了一些协议包（FIN，RST等），具体的交换过程可以看TCP协议，这里不详细描述了。而关闭socket是指关闭用户应用程序中的socket句柄，释放相关资源。但是当用户关闭socket句柄时会隐含的触发TCP连接的关闭过程。 TCP连接的关闭过程有两种，一种是优雅关闭（graceful close），一种是强制关闭（hard close或abortive close）。所谓优雅关闭是指，如果发送缓存中还有数据未发出则其发出去，并且收到所有数据的ACK之后，发送FIN包，开始关闭过程。而强制关闭是指如果缓存中还有数据，则这些数据都将被丢弃，然后发送RST包，直接重置TCP连接。 close方法可以释放一个连接的资源，但是不是立即释放，如果想立即释放，那么在close之前使用shutdown方法 SHUT_RD：断开输入流。套接字无法接收数据（即使输入缓冲区收到数据也被抹去），无法调用输入相关函数。 SHUT_WR：断开输出流。套接字无法发送数据，但如果输出缓冲区中还有未传输的数据，则将传递到目标主机。 SHUT_RDWR：同时断开 I/O 流。相当于分两次调用 shutdown()，其中一次以 SHUT_RD 为参数，另一次以 SHUT_WR 为参数。 使用：在close()之前加上shutdown(num)即可 [shut_rd(), shut_wr(), shut_rdwr()分别代表num 为0 1 2 ] 也就是说，想要关闭一个连接，首先把通道全部关闭，然后在release连接，以上三个静态变量分别对应数字常量：0,1,2 close()和shutdown()的区别 确切地说，close() 用来关闭套接字，将套接字描述符（或句柄）从内存清除，之后再也不能使用该套接字，与C语言中的 fclose() 类似。应用程序关闭套接字后，与该套接字相关的连接和缓存也失去了意义，TCP协议会自动触发关闭连接的操作。 shutdown() 用来关闭连接，而不是套接字，不管调用多少次 shutdown()，套接字依然存在，直到调用 close() 将套接字从内存清除。 调用 close() 关闭套接字时，或调用 shutdown() 关闭输出流时，都会向对方发送 FIN 包。FIN 包表示数据传输完毕，计算机收到 FIN 包就知道不会再有数据传送过来了。 默认情况下，close() 会立即向网络中发送FIN包，不管输出缓冲区中是否还有数据，而shutdown() 会等输出缓冲区中的数据传输完毕再发送FIN包。也就意味着，调用 close() 将丢失输出缓冲区中的数据，而调用 shutdown() 不会。 shutdown用于以下几种情况: 通常来说，socket是双向的，即数据是双向通信的。但有些时候，你会想在socket上实现单向的socket，即数据往一个方向传输。 单向的socket便称为半开放Socket。要实现半开放式，需要用到shutdown()函数。 一般来说，半开放socket适用于以下场合: （1）当你想要确保所有写好的数据已经发送成功时。如果在发送数据的过程中，网络意外断开或者出现异常，系统不一定会返回异常，这是你可能以为对端已经接收到数据了。这时需要用shutdown()来确定数据是否发送成功，因为调用shutdown()时只有在缓存中的数据全部发送成功后才会返回。 （2）想用一种方法来捕获程序潜在的错误，这错误可能是因为往一个不能写的socket上写数据，也有可能是在一个不该读操作的socket上读数据。当程序尝试这样做时，将会捕获到一个异常，捕获异常对于程序排错来说是相对简单和省劲的。 （3）当您的程序使用了fork()或者使用多线程时，你想防止其他线程或进程访问到该资源，又或者你想立刻关闭这个socket，那么可以用shutdown()来实现。 另外说一下，如果调用了Close()函数，程序中只是确保了对于某个特定的进程或线程来说，该连接是关闭的；但socket只有在所有的进程调用了Close()或者socket超出了工作范围时，才会真正的被关闭或删除。而如果想立刻关闭socket，那么可以通过shutdown()来实现。 shutdown()的调用是需要一个参数：0代表禁止下次的数据读取；1代表禁止下次的数据写入；2代表禁止下次的数据读取和写入。 同时，shutdown()的效果是累计的，不可逆转的。既如果关闭了一个方向数据传输，那么这个方向将会被关闭直至完全被关闭或删除，而不能重新被打开。如果第一次调用了shutdown(0),第二次调用了shutdown(1),那么这时的效果就相当于shutdown(2)，也就是双向关闭socket。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566服务端from socket import *import threading,os,time class Server(): def __init__(self,host='127.0.0.1',port=9990): try: addr=(host,port) self.tcpSerSock=socket(AF_INET,SOCK_STREAM) self.tcpSerSock.setsockopt(SOL_SOCKET,SO_REUSEADDR,1) self.tcpSerSock.bind(addr) self.tcpSerSock.listen(5) except Exception,e : print 'ip or port error :',str(e) self.tcpSerSock.close() def main(self): while 1 : try : print 'wait for connecting ...' tcpCliSock,addr = self.tcpSerSock.accept() addrStr = addr[0]+':'+str(addr[1]) print 'connect from',addrStr except KeyboardInterrupt: self.close=True tcpCliSock.close() self.tcpSerSock.close() print 'KeyboardInterrupt' break ct = ClientThread(tcpCliSock,addrStr) ct.start() class ClientThread(threading.Thread): def __init__(self,tcpClient,addr): super(ClientThread,self).__init__() self.tcpClient = tcpClient self.addr = addr self.timeout = 60 tcpClient.settimeout(self.timeout) self.cf = tcpClient.makefile('rw',0) def run(self): while 1: try: data = self.cf.readline().strip() if data: if data.find("set time")&gt;=0: self.timeout = int(data.replace("set time ","")) self.tcpClient.settimeout(self.timeout) print self.addr,"client say:",data self.cf.write(str(self.addr)+" recevied ok!"+"\n") else: break except Exception,e: self.tcpClient.close() self.cf.write("time out !"+"\n") print self.addr,"send message error,",str(e) #此处将break注释掉# break if __name__ == "__main__" : ser = Server() ser.main() 12345678910111213141516171819202122232425262728客户端from socket import *class Client(): def __init__(self): pass def main(self): tcpCliSock=socket(AF_INET,SOCK_STREAM) tcpCliSock.connect(('127.0.0.1',9990)) print 'connect server 9999 successfully !' cf = tcpCliSock.makefile('rw', 0) while 1: data=raw_input('&gt;') try: if data: cf.write(data+"\n") data = cf.readline().strip() if data: print "server say:",data else: break else: break except Exception,e: print "send error,",str(e)if __name__ == "__main__": cl = Client() cl.main() 在代码中可以看出，如果timeout后，except肯定能够捕获到timeout异常，这样就会进入到except代码中，在上面我们故意将break注释掉，也就是不让其跳出循环，经过试验，可以得知，虽然在server端已经将连接close掉了，但是client端仍然可以顺利的接收到消息，而且，如果client端发送数据的间隔小于超时时间的话，此连接可以顺利的一直使用，这样，我们close貌似就一点儿效果都没有了要实现我们的功能，只需要改变server端的代码 123456except Exception,e:self.tcpClient.shutdown(2) self.tcpClient.close() self.cf.write(&quot;time out !&quot;+&quot;\n&quot;) print self.addr,&quot;send message error,&quot;,str(e) break]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详细分析TCP数据的传输过程]]></title>
    <url>%2F2019%2F09%2F30%2F%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90TCP%E6%95%B0%E6%8D%AE%E7%9A%84%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[详细分析TCP数据的传输过程建立连接后，两台主机就可以相互传输数据了。如下图所示： ​ TCP 套接字的数据交换过程 上图给出了主机A分2次（分2个数据包）向主机B传递200字节的过程。首先，主机A通过1个数据包发送100个字节的数据，数据包的 Seq 号设置为 1200。主机B为了确认这一点，向主机A发送 ACK 包，并将 Ack 号设置为 1301。 为了保证数据准确到达，目标机器在收到数据包（包括SYN包、FIN包、普通数据包等）包后必须立即回传ACK包，这样发送方才能确认数据传输成功。 此时 Ack 号为 1301 而不是 1201，原因在于 Ack 号的增量为传输的数据字节数。假设每次 Ack 号不加传输的字节数，这样虽然可以确认数据包的传输，但无法明确100字节全部正确传递还是丢失了一部分，比如只传递了80字节。因此按如下的公式确认 Ack 号： Ack号 = Seq号 + 传递的字节数 + 1 与三次握手协议相同，最后加 1 是为了告诉对方要传递的 Seq 号。下面分析传输过程中数据包丢失的情况，如下图所示： ​ TCP套接字数据传输过程中发生错误 上图表示通过 Seq 1301 数据包向主机B传递100字节的数据，但中间发生了错误，主机B未收到。经过一段时间后，主机A仍未收到对于 Seq 1301 的ACK确认，因此尝试重传数据。 为了完成数据包的重传，TCP套接字每次发送数据包时都会启动定时器，如果在一定时间内没有收到目标机器传回的 ACK 包，那么定时器超时，数据包会重传。 上图演示的是数据包丢失的情况，也会有 ACK 包丢失的情况，一样会重传。 重传超时时间（RTO, Retransmission Time Out）这个值太大了会导致不必要的等待，太小会导致不必要的重传，理论上最好是网络 RTT 时间，但又受制于网络距离与瞬态时延变化，所以实际上使用自适应的动态算法（例如 Jacobson 算法和 Karn 算法等）来确定超时时间。 往返时间（RTT，Round-Trip Time）表示从发送端发送数据开始，到发送端收到来自接收端的 ACK 确认包（接收端收到数据后便立即确认），总共经历的时延。 重传次数TCP数据包重传次数根据系统设置的不同而有所区别。有些系统，一个数据包只会被重传3次，如果重传3次后还未收到该数据包的 ACK 确认，就不再尝试重传。但有些要求很高的业务系统，会不断地重传丢失的数据包，以尽最大可能保证业务数据的正常交互。最后需要说明的是，发送端只有在收到对方的 ACK 确认包后，才会清空输出缓冲区中的数据。]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图解TCP四次握手断开连接]]></title>
    <url>%2F2019%2F09%2F30%2F%E5%9B%BE%E8%A7%A3TCP%E5%9B%9B%E6%AC%A1%E6%8F%A1%E6%89%8B%E6%96%AD%E5%BC%80%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[图解TCP四次握手断开连接建立连接非常重要，它是数据正确传输的前提；断开连接同样重要，它让计算机释放不再使用的资源。如果连接不能正常断开，不仅会造成数据传输错误，还会导致套接字不能关闭，持续占用资源，如果并发量高，服务器压力堪忧。建立连接需要三次握手，断开连接需要四次握手，可以形象的比喻为下面的对话：[Shake 1] 套接字A：“任务处理完毕，我希望断开连接。” [Shake 2] 套接字B：“哦，是吗？请稍等，我准备一下。”等待片刻后…… [Shake 3] 套接字B：“我准备好了，可以断开连接了。” [Shake 4] 套接字A：“好的，谢谢合作。” 下图演示了客户端主动断开连接的场景： 建立连接后，客户端和服务器都处于ESTABLISED状态。这时，客户端发起断开连接的请求：1) 客户端调用 close() 函数后，向服务器发送 FIN 数据包，进入FIN_WAIT_1状态。FIN 是 Finish 的缩写，表示完成任务需要断开连接。 2) 服务器收到数据包后，检测到设置了 FIN 标志位，知道要断开连接，于是向客户端发送“确认包”，进入CLOSE_WAIT状态。 注意：服务器收到请求后并不是立即断开连接，而是先向客户端发送“确认包”，告诉它我知道了，我需要准备一下才能断开连接。 3) 客户端收到“确认包”后进入FIN_WAIT_2状态，等待服务器准备完毕后再次发送数据包。 4) 等待片刻后，服务器准备完毕，可以断开连接，于是再主动向客户端发送 FIN 包，告诉它我准备好了，断开连接吧。然后进入LAST_ACK状态。 5) 客户端收到服务器的 FIN 包后，再向服务器发送 ACK 包，告诉它你断开连接吧。然后进入TIME_WAIT状态。 6) 服务器收到客户端的 ACK 包后，就断开连接，关闭套接字，进入CLOSED状态。 关于 TIME_WAIT 状态的说明客户端最后一次发送 ACK包后进入 TIME_WAIT 状态，而不是直接进入 CLOSED 状态关闭连接，这是为什么呢？TCP 是面向连接的传输方式，必须保证数据能够正确到达目标机器，不能丢失或出错，而网络是不稳定的，随时可能会毁坏数据，所以机器A每次向机器B发送数据包后，都要求机器B”确认“，回传ACK包，告诉机器A我收到了，这样机器A才能知道数据传送成功了。如果机器B没有回传ACK包，机器A会重新发送，直到机器B回传ACK包。 客户端最后一次向服务器回传ACK包时，有可能会因为网络问题导致服务器收不到，服务器会再次发送 FIN 包，如果这时客户端完全关闭了连接，那么服务器无论如何也收不到ACK包了，所以客户端需要等待片刻、确认对方收到ACK包后才能进入CLOSED状态。那么，要等待多久呢？ 数据包在网络中是有生存时间的，超过这个时间还未到达目标主机就会被丢弃，并通知源主机。这称为。TIME_WAIT 要等待 2MSL 才会进入 CLOSED 状态。ACK 包到达服务器需要 MSL 时间，服务器重传 FIN 包也需要 MSL 时间，2MSL 是数据包往返的最大时间，如果 2MSL 后还未收到服务器重传的 FIN 包，就说明服务器已经收到了 ACK 包。]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图解TCP数据报结构以及三次握手]]></title>
    <url>%2F2019%2F09%2F30%2F%E5%9B%BE%E8%A7%A3TCP%E6%95%B0%E6%8D%AE%E6%8A%A5%E7%BB%93%E6%9E%84%E4%BB%A5%E5%8F%8A%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%2F</url>
    <content type="text"><![CDATA[图解TCP数据报结构以及三次握手TCP（Transmission Control Protocol，传输控制协议）是一种面向连接的、可靠的、基于字节流的通信协议，数据在传输前要建立连接，传输完毕后还要断开连接。客户端在收发数据前要使用 connect() 函数和服务器建立连接。建立连接的目的是保证IP地址、端口、物理链路等正确无误，为数据的传输开辟通道。三次握手（Three-way Handshaking） [Shake 1] 套接字A：“你好，套接字B，我这里有数据要传送给你，建立连接吧。” [Shake 2] 套接字B：“好的，我这边已准备就绪。” [Shake 3] 套接字A：“谢谢你受理我的请求。” TCP数据报结构 带阴影的几个字段需要重点说明一下： 2) 确认号：Ack（Acknowledge Number）确认号占32位，客户端和服务器端都可以发送，Ack = Seq + 1。 URG：紧急指针（urgent pointer）有效。 ACK：确认序号有效。 PSH：接收方应该尽快将这个报文交给应用层。 RST：重置连接。 SYN：建立一个新连接。 FIN：断开一个连接。 对英文字母缩写的总结：Seq 是 Sequence 的缩写，表示序列；Ack(ACK) 是 Acknowledge 的缩写，表示确认；SYN 是 Synchronous 的缩写，愿意是“同步的”，这里表示建立同步连接；FIN 是 Finish 的缩写，表示完成。 连接的建立（三次握手）使用 connect() 建立连接时，客户端和服务器端会相互发送三个数据包，请看下图： 客户端调用 socket() 函数创建套接字后，因为没有建立连接，所以套接字处于CLOSED(关闭)状态；服务器端调用 listen() 函数后，套接字进入LISTEN(监听)状态，开始监听客户端请求。 这个时候，客户端开始发起请求：1) 当客户端调用 connect() 函数后，TCP协议会组建一个数据包，并设置 SYN 标志位，表示该数据包是用来建立同步连接的。同时生成一个随机数字 1000，填充“序号（Seq）”字段，表示该数据包的序号。完成这些工作，开始向服务器端发送数据包，客户端就进入了SYN-SEND状态。2) 服务器端收到数据包，检测到已经设置了 SYN 标志位，就知道这是客户端发来的建立连接的“请求包”。服务器端也会组建一个数据包，并设置 SYN 和 ACK 标志位，SYN 表示该数据包用来建立连接，ACK 用来确认收到了刚才客户端发送的数据包。 服务器生成一个随机数 2000，填充“序号（Seq）”字段。2000 和客户端数据包没有关系。 服务器将客户端数据包序号（1000）加1，得到1001，并用这个数字填充“确认号（Ack）”字段。 服务器将数据包发出，进入SYN-RECV状态。 3) 客户端收到数据包，检测到已经设置了 SYN 和 ACK 标志位，就知道这是服务器发来的“确认包”。客户端会检测“确认号（Ack）”字段，看它的值是否为 1000+1，如果是就说明连接建立成功。 接下来，客户端会继续组建数据包，并设置 ACK 标志位，表示客户端正确接收了服务器发来的“确认包”。同时，将刚才服务器发来的数据包序号（2000）加1，得到 2001，并用这个数字来填充“确认号（Ack）”字段。 客户端将数据包发出，进入ESTABLISED状态，表示连接已经成功建立。 4) 服务器端收到数据包，检测到已经设置了 ACK 标志位，就知道这是客户端发来的“确认包”。服务器会检测“确认号（Ack）”字段，看它的值是否为 2000+1，如果是就说明连接建立成功，服务器进入ESTABLISED状态。 至此，客户端和服务器都进入了ESTABLISED状态，连接建立成功，接下来就可以收发数据了。 最后的说明三次握手的关键是要确认对方收到了自己的数据包，这个目标就是通过“确认号（Ack）”字段实现的。计算机会记录下自己发送的数据包序号 Seq，待收到对方的数据包后，检测“确认号（Ack）”字段，看Ack = Seq + 1]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP之“滑动窗口”协议]]></title>
    <url>%2F2019%2F09%2F27%2FTCP%E4%B9%8B%E2%80%9C%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E2%80%9D%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[TCP之“滑动窗口”协议网络传输的可靠性是通过TCP协议，它是如何解决网络传输不可靠的问题。其中关键部分，就是用的滑动窗口协议。 滑动窗口协议： TCP协议的使用 维持发送方/接收方缓冲区 缓冲区是 用来解决网络之间数据不可靠的问题，例如丢包，重复包，出错，乱序 在TCP协议中，发送方和接受方通过各自维护自己的缓冲区。通过商定包的重传机制等一系列操作，来解决不可靠的问题。 问题一：如何保证次序？ 提出问题：在我们滑动窗口协议之前，我们如何来保证发送方与接收方之间，每个包都能被收到。并且是按次序的呢？ 发送方发送一个包1，这时候接收方确认包1。发送包2，确认包2。就这样一直下去，知道把数据完全发送完毕，这样就结束了。那么就解决了丢包，出错，乱序等一些情况！同时也存在一些问题。问题：吞吐量非常的低。我们发完包1，一定要等确认包1.我们才能发送第二个包。 问题二：如何提高吞吐量？ 提出问题：那么我们就不能先连发几个包等他一起确认吗？这样的话，我们的速度会不会更快，吞吐量更高些呢？ 如图，这个就是我们把两个包一起发送，然后一起确认。可以看出我们改进的方案比之前的好很多，所花的时间只是一个来回的时间。接下来，我们还有一个问题：改善了吞吐量的问题 问题三：如何实现最优解？ 问题：我们每次需要发多少个包过去呢？发送多少包是最优解呢？ 我们能不能把第一个和第二个包发过去后，收到第一个确认包就把第三个包发过去呢？而不是去等到第二个包的确认包才去发第三个包。这样就很自然的产生了我们”滑动窗口”的实现。 在图中，我们可看出灰色1号2号3号包已经发送完毕，并且已经收到Ack。这些包就已经是过去式。4、5、6、7号包是黄色的，表示已经发送了。但是并没有收到对方的Ack，所以也不知道接收方有没有收到。8、9、10号包是绿色的。是我们还没有发送的。这些绿色也就是我们接下来马上要发送的包。 可以看出我们的窗口正好是11格。后面的11-16还没有被读进内存。要等4号-10号包有接下来的动作后，我们的包才会继续往下发送。 正常情况 可以看到4号包对方已经被接收到，所以被涂成了灰色。“窗口”就往右移一格，这里只要保证“窗口”是7格的。 我们就把11号包读进了我们的缓存。进入了“待发送”的状态。8、9号包已经变成了黄色，表示已经发送出去了。接下来的操作就是一样的了，确认包后，窗口往后移继续将未发送的包读进缓存，把“待发送“状态的包变为”已发送“。 丢包情况有可能我们包发过去，对方的Ack丢了。也有可能我们的包并没有发送过去。从发送方角度看就是我们没有收到Ack。 发生的情况：一直在等Ack。如果一直等不到的话，我们也会把读进缓存的待发送的包也一起发过去。但是，这个时候我们的窗口已经发满了。所以并不能把12号包读进来，而是始终在等待5号包的Ack。 如果我们这个Ack始终不来怎么办呢？ 超时重发这时候我们有个解决方法：超时重传 这里有一点要说明：这个Ack是要按顺序的。必须要等到5的Ack收到，才会把6-11的Ack发送过去。这样就保证了滑动窗口的一个顺序。 这时候可以看出5号包已经接受到Ack，后面的6、7、8号包也已经发送过去已Ack。窗口便继续向后移动。 文末从我们为了增加网络的吞吐量，想讲数据包一起发送过去，这时候便产生了“滑动窗口”这种协议。有了“滑动窗口”这个概念，我们又解决了其中出现的一些问题。例如丢包，我们又通过重发的机制去解决了。 作者：https://juejin.im/post/5c9f1dd651882567b4339bce来源]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Socket的粘包和分包的处理]]></title>
    <url>%2F2019%2F09%2F27%2FSocket%E7%9A%84%E7%B2%98%E5%8C%85%E5%92%8C%E5%88%86%E5%8C%85%E7%9A%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[TCP Socket的粘包和分包的处理概述在进行TCP socket开发时，都需要处理数据包粘包和分包的情况。解决方法在应用层下，定义一个协议：消息头部+消息长度+消息正文即可。 只有TCP有粘包问题，而UDP永远不会粘包。socket收发消息原理： 服务端可以1kb,1kb的向客户端发送数据，客户端的应用程序可以在缓存当中2kb,2kb地取走数据，当然也可以更多，或更少。也就是说，应用程序看到的数据是个整体。或者说是一个流。一条消息有多少个字节对应用程序是可见的，tcp协议是面向流的协议，这就是它容易粘包的问题原因。 所谓粘包问题主要还是因为接收方不知道消息之间的界限，不知道一个消息要提取多少字节的数据所造成的。 此外，发送方引起的粘包是由tcp协议本身造成的，tcp为提高传输效率，发送方往往收集到足够多的数据才发送一个tcp段。若连续几次需要发送的数据都很少，通常tcp会根据nagle优化算法 ，把这些数据合成一个tcp段后发送出去，这样接收方就收到了粘包数据。 粘包情况一：发送端要等缓冲区满才发送出去(即发送的数量少且时间间隔短，会合并到一起)，造成粘包 1、服务端 123456789101112131415from socket import * server=socket(AF_INET,SOCK_STREAM)server.bind(('127.0.0.1',8080))server.listen(5) conn,client_addr=server.accept() res1=conn.recv(1024)print('第一次:',res1)res2=conn.recv(1024)print('第二次:',res2) conn.close()server.close() 2、客户端 12345678from socket import * client=socket(AF_INET,SOCK_STREAM)client.connect(('127.0.0.1',8080)) client.send(b'hello')client.send(b'world')client.close() 先启动服务端，后再启动客户端，服务端得到的结果为： 123[root@seafile:~]# python3 server.py 第一次: b'helloworld'第二次: b'' 可以看出客户端是发了两次数据而服务端一次就接收完了。 粘包情况二：客户端发送了一段数据，服务端只收了一小部分，服务端下次再收的时候是从缓冲区拿上次遗留的数据，产生粘包。 情况一的，客户端不变，服务端略作修改，如下： 123456789101112131415from socket import * server=socket(AF_INET,SOCK_STREAM)server.bind(('127.0.0.1',8080))server.listen(5) conn,client_addr=server.accept() res1=conn.recv(2)print('第一次:',res1)res2=conn.recv(3)print('第二次:',res2) conn.close()server.close() 先启动服务端，后再启动客户端，服务端得到的结果为： 123[root@seafile:~]# python3 server.py 第一次: b'hel'第二次: b'lowor' 现在我们来想一下如何处理粘包的方法。粘包的问题根源是接收端不知发送端将要传送的字节流，所以我们要让发送端在发送数据前，把要发送的字节流总大小让接收端知晓，然后接收端来个循环将其全部接收。这种方法比较低级，存在的问题：程序的运行速度远快于网络传输速度，所以在发送一段字节前，先用send去发送该字节流长度，这种方式会放大网络延迟带来的性能损耗 程序的运行速度远快于网络传输速度，所以在发送一段字节前，先用send去发送该字节流长度，这种方式会放大网络延迟带来的性能损耗 刚才上面 在发送消息之前需先发送消息长度给对端，还必须要等对端返回一个ready收消息的确认，不等到对端确认就直接发消息的话，还是会产生粘包问题(承载消息长度的那条消息和消息本身粘在一起)。 有没有优化的好办法么？ 思考一个问题，为什么不能在发送了消息长度(称为消息头head吧)给对端后，立刻发消息内容(称为body吧)，是因为怕head 和body 粘在一起，所以通过等对端返回确认来把两条消息中断开。 可不可以直接发head + body,但又能让对端区分出哪个是head，哪个是body呢？我靠、我靠，感觉智商要涨了。 想到了，把head设置成定长的呀，这样对端只要收消息时，先固定收定长的数据，head里写好，后面还有多少是属于这条消息的数据，然后直接写个循环收下来不就完了嘛！唉呀妈呀，我真机智。 可是、可是如何制作定长的消息头呢？假设你有2条消息要发送，第一条消息长度是 3000个字节，第2条消息是200字节。如果消息头只包含消息长度的话，那两个消息的消息头分别是 1len(msg1) = 4000 = 4字节 len(msg2) = 200 = 3字节 你的服务端如何完整的收到这个消息头呢？是recv(3)还是recv(4)服务器端怎么知道？ 目前比较合理的处理方法是：为字节流加上一个报头，将这个报头做成字典，字典里包含将要发送的真实数据详细信息。将这个字典JSON序列化，然后用struck将序列化后的数据长度打包成4个字节（4个字节完全够用） 发送时：先发报头长度，再编码报头内容后发送，最后发真实数据。 接收时：用struct取出报头长度，然后根据长度取出报头，解码，反序列化。最后从反序列化的结果中取出待取数据的详细信息，然后取真实的数据内容。 来看实现的代码： 1、服务端 1234567891011121314151617181920212223242526272829303132333435363738394041424344from socket import * import subprocessimport structimport json server=socket(AF_INET,SOCK_STREAM)server.bind(('127.0.0.1',8080))server.listen(5)while True: conn,client_addr=server.accept() print(conn,client_addr)#(连接对象，客户端的ip和端口) while True: try: cmd=conn.recv(1024) obj=subprocess.Popen( cmd.decode('utf-8'), shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE ) stdout=obj.stdout.read() stderr=obj.stderr.read() #1、制作报头 header_dic=&#123; 'total_size':len(stdout)+len(stderr), 'md5':'dgdsfsdfdsdfsfewrewge', 'file_name':'a.txt' &#125; header_json=json.dumps(header_dic) header_bytes=header_json.encode('utf-8') #2、先发送报头的长度 header_size=len(header_bytes) conn.send(struct.pack('i',header_size)) #3、发送报头 conn.send(header_bytes) #4、发送真实的数据 conn.send(stdout) conn.send(stderr) except ConnectionResetError: break conn.close()server.close() 2、客户端 1234567891011121314151617181920212223242526272829303132333435from socket import *import jsonimport struct client=socket(AF_INET,SOCK_STREAM)client.connect(('127.0.0.1',8080)) while True: cmd=input("&gt;&gt;:").strip() if not cmd:continue client.send(cmd.encode('utf-8')) #1、接收报文头的长度 header_size=struct.unpack('i',client.recv(4))[0] #2、接收报文 header_bytes=client.recv(header_size) #3、解析报文 header_json=header_bytes.decode('utf-8') header_dic=json.loads(header_json) print(header_dic) #4、获取真实数据的长度 totol_size=header_dic['total_size'] #5、获取数据 recv_size=0 res=b'' while recv_size&lt;totol_size: recv_date=client.recv(1024) res+=recv_date recv_size+=len(recv_date) print(res.decode('gbk'))client.close() 补充struct模块： 123456789101112131415161718192021222324import struct#struct是用来将整型的数字转成固定长度的bytes.import json header_dic=&#123; 'total_size':32322, 'md5':'gdssfsfsdfsf', 'filename':'a.txt' &#125;#1、将报头字典序列化。header_json=json.dumps(header_dic)#2、将序列后的字典转成字节header_bytes=header_json.encode('utf-8')#3、获取序列的字字典转成字节的个数header_size=len(header_bytes)print(header_size)#4、将这个个数转成固字长度的字节表示obj=struct.pack('i',header_size)print(obj,len(obj))#、这个固定长度的字节经过反转后是一个元组。res=struct.unpack('i',obj)#、通过按索取值就可等到报头字典长度。header_size=res[0] 原文链接：https://blog.csdn.net/miaoqinian/article/details/80020291]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[socket的发送和接收缓冲区]]></title>
    <url>%2F2019%2F09%2F27%2Fsocket%E7%9A%84%E5%8F%91%E9%80%81%E5%92%8C%E6%8E%A5%E6%94%B6%E7%BC%93%E5%86%B2%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[socket的发送和接收缓冲区 对于每一个TCP的SOCKET来说，都有一个发送缓冲区和接受缓冲区与之对应。 首先要理解的是，缓冲区有两种。第一种是用户定义的缓冲区，就是send里的缓冲区参数。另一种就是套接字缓冲区，是由协议自定的。可以用getsockopt获取其大小，并用setsockopt重置其大小。发送的时候使用send实际上是从用户缓冲区发送到socket缓冲区中，至于数据真正何时发送给另一端则是由协议规定的。Recv也类似。 按照Unix下的send实现的话应该是先检查用户缓冲区是否大于socket发送缓冲区，如果大于则分批次发送，每次拷贝socket缓冲区大小之后发送，发送的时候socket缓冲区锁定，发送之后在从用户缓冲区接受直至发送完毕为止。如果小于则直接发送。在socket缓冲区发送的过程中这个缓冲区是锁定的。 一、recv端 在监听套接字上准备accept,在accept结束以后不做什么操作，直接sleep很久，也就是在recv端并不做接收数据的操作，在sleep结束之后再recv数据 二、send端 ​ 通过查看本系统内核支持的发送缓冲区大小，cat/proc/sys/net/ipv4/tcp_wmem，三个参数分别最小值、默认值和最大值。接收缓冲区的配置文件在tcp_rmen中。 ​ 将套接字设置为阻塞，一次发送的buffer大于发送缓冲区所能容纳的数据量，一次send结束，在发送返回后接着打印发送的数据长度。 测试结果： ​ 阶段一： ​ recv端表现：在刚开始发送数据时，接收端处于慢启动状态，滑动窗口值越来越大，但是由于接收端不处理接收缓冲区内的数据，其滑动窗口越来越小（因为接收端回应发送端中的win大小表示接受端还能够接受多少数据，发送端下次发送的数据大小不能超过回应中win的大小），最后发送端回应给接受端的ACK中显示的win大小为0，表示接收端不能够再接受数据。 ​ send端表现：发送端一直不能返回，如果接收端一直回应win为0的情况下，发送端的send就会一直不能返回，这种僵局一直持续到接收端的sleep结束。 ​ 原因分析：首先需要明白几个事实，阻塞式I/O会一直等待，直达这个操作完成；发送端接受到接收端的回应后才能将发送缓冲区中的数据进行清空。 ​ 在接收端不recv，那么接收端的接收缓冲区内会一直有数据，接收缓冲区满，导致滑动窗口为0，发送端不能发送数据。但是send操作为何不能返回呢？send操作只是将应用缓冲区的数据拷贝到发送缓冲区，但是发送缓冲区的数据并没有完全得到接收端的ACK回应，所以暂时不能将发送缓冲区中的数据丢弃，导致发送缓冲区的被填满，这样应用层中的数据也就不能拷贝到内核发送缓冲区内，也就会一直阻塞在这里，直到可以继续将应用层的数据拷贝到发送缓冲区中，何时触发这个操作呢？等到发送端回应win大于0时才有这样的操作。 ​ 阶段二： ​ recv端：在sleep结束以后，开始调用recv系统调用。这个时候接收端的滑动窗口又开始大于零，这样就唤醒了发送端继续发送数据。 ​ send端：发送端接收到接收端win大于0的回应，这个时候发送端又可以将应用层buffer中的数据拷贝到内核的发送缓冲区中。 ​ 由于接收端调用recv将内核接收缓冲区的数据拷贝到应用层中，这样滑动窗口又大于0了，所以激发了发送端继续发送数据。由于发送端可以发送数据了，内核协议栈便将发送缓冲区中的数据发送给接收端，这样发送缓冲区又有空间了，那么send操作就可以将应用层的数据拷贝到发送缓冲区了！这样的操作一直保持到send操作返回，这样代表着将应用层的数据全部拷贝到发送缓冲区内，但不代表将数据发送给对端。发送给对端成功的标志是接收到对端的ACK回应，这个时候发送端才可以将发送缓冲区的数据丢弃。不丢弃的原因是时刻准备重发丢失/出错的数据！ ​ Ps: TCP通信为了保证可靠性，每次发送的数据都需要得到对方的ACK才确认对方收到了(仅保证对方TCP接收缓冲区收到数据了，但不保证对方应用程序取到数据了)，这时如果每次发送一次就要停下来等着对方的ACK消息，显然是一种极大的资源浪费和低下的效率，这时就有了滑动窗口的出现。 ​ 发送方的滑动窗口维持着当前发送的帧序号，已发出去帧的计时器，接收方当前的窗口大小(由接收方ACK通知，大体等于接收缓冲大小-未处理的数据包)，接收方滑动窗口保存的有已接收的帧信息、期待的下一帧的帧号等，至于滑动窗口的具体工作原理这里就不说了。 ​ 一 个socket有两个滑动窗口(一个sendbuf、一个recvbuf)，两个窗口的大小是通过setsockopt函数设置的，现在问题就出在这里， 通过抓包显示，设置的窗口大小没有生效，最后排查发现setsockopt函数是后来加上的，写到了listen函数的后面，这样每次accept出的 socket并没有继承得到主socket设置的窗口大小，无语啊…… ​ 解决办法：setsockopt函数提前到listen函数之前，这样在服务器程序启动监听前recvbuf就已经有了，accept后的链接得到的就是recvbuf了，启动程序运行，抓包显示窗口已经是指定的大小了。 结论： ​ 1.TCP的滑动窗口大小实际上就是socket的接收缓冲区（SO_RCVBUF）大小的字节数 ​ 2.对于server端的socket一定要在listen之前设置缓冲区大小，因为，accept时新产生的socket会继承监听socket的缓冲区大 小。对于client端的socket一定要在connet之前设置缓冲区大小，因为connet时需要进行三次握手过程，会通知对方自己的窗口大小。在 connet之后再设置缓冲区，已经没有什么意义。 ​ 3.由于缓冲区大小在TCP头部只有16位来表示，所以它的最大值是65536，但是对于一些情况来说需要使用更大的滑动窗口，这时候就要使用扩展的滑动窗口，如光纤高速通信网络，或者是卫星长连接网络，需要窗口尽可能的大。这时会使用扩展的32位的滑动窗口大小。 ​ 这就是发送缓冲区、接受缓冲区、滑动窗口协议之间的关系，不知道大家明白了没有。 【个人补充】socket的发送缓冲区与滑动窗口关系 socket/send成功与否与滑动窗口没有任何关系，但是，存在逻辑上的先后作用关系，即 socket/send向缓冲区发送数据包时，要看发送缓冲区大小（SO_SNDBUF）–&gt; tcp/send发送缓冲区中的数据包时，看滑动窗口win的值。]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中TCP协议中的粘包问题]]></title>
    <url>%2F2019%2F09%2F27%2Fpython%E4%B8%ADTCP%E5%8D%8F%E8%AE%AE%E4%B8%AD%E7%9A%84%E7%B2%98%E5%8C%85%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[python中TCP协议中的粘包问题TCP协议中的粘包问题1.粘包现象 基于TCP实现一个简易远程cmd功能 123456789101112131415161718192021222324252627282930313233343536#服务端import socketimport subprocesssever = socket.socket()sever.bind(('127.0.0.1', 33521))sever.listen()while True: client, address = sever.accept() while True: try: cmd = client.recv(1024).decode('utf-8') p1 = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr= subprocess.PIPE) data = p1.stdout.read() err_data = p1.stderr.read() client.send(data) client.send(err_data) except ConnectionResetError: print('connect broken') client.close() breaksever.close()​​​#客户端import socketclient = socket.socket()client.connect(('127.0.0.1', 33521))while True: cmd = input('请输入指令(Q\q退出)&gt;&gt;:').strip().lower() if cmd == 'q': break client.send(cmd.encode('utf-8')) data = client.recv(1024) print(data.decode('gbk'))client.close() 上述是基于TCP协议的远程cmd简单功能，在运行时会发生粘包。 2、什么是粘包？ 只有TCP会发生粘包现象，UDP协议永远不会发生粘包； TCP：（transport control protocol，传输控制协议）流式协议。在socket中TCP协议是按照字节数进行数据的收发，数据的发送方发出的数据往往接收方不知道数据到底长度是多长，而TCP协议由于本身为了提高传输的效率，发送方往往需要收集到足够的数据才会进行发送。使用了优化方法（Nagle算法），将多次间隔较小且数据量小的数据，合并成一个大的数据块，然后进行封包。这样，接收端，就难于分辨出来了，必须提供科学的拆包机制。 即面向流的通信是无消息保护边界的。 UDP：（user datagram protocol，用户数据报协议）数据报协议。在socket中udp协议收发数据是以数据报为单位，服务端和客户端收发数据是以一个单位，所以不会使用块的合并优化算法，, 由于UDP支持的是一对多的模式，所以接收端的skbuff(套接字缓冲区）采用了链式结构来记录每一个到达的UDP包，在每个UDP包中就有了消息头（消息来源地址，端口等信息），这样，对于接收端来说，就容易进行区分处理了。 即面向消息的通信是有消息保护边界的。 TCP协议不会丢失数据，UDP协议会丢失数据。 udp的recvfrom是阻塞的，一个recvfrom(x)必须对唯一一个sendinto(y),收完了x个字节的数据就算完成,若是y&gt;x数据就丢失，这意味着udp根本不会粘包，但是会丢数据，不可靠。 tcp的协议数据不会丢，没有收完包，下次接收，会继续上次继续接收，己端总是在收到ack时才会清除缓冲区内容。数据是可靠的，但是会粘包。 3、什么情况下会发生粘包？ 1.由于TCP协议的优化算法，当单个数据包较小的时候，会等到缓冲区满才会发生数据包前后数据叠加在一起的情况。然后取的时候就分不清了到底是哪段数据，这是第一种粘包。 2.当发送的单个数据包较大超过缓冲区时，收数据方一次就只能取一部分的数据，下次再收数据方再收数据将会延续上次为接收数据。这是第二种粘包。 粘包的本质问题就是接收方不知道发送数据方一次到底发送了多少数据，解决问题的方向也是从控制数据长度着手，也就是如何设置缓冲区的问题 4、如何解决粘包问题？ 解决问题思路：上述已经明确粘包的产生是因为接收数据时不知道数据的具体长度。所以我们应该先发送一段数据表明我们发送的数据长度，那么就不会产生数据没有发送或者没有收取完全的情况。 1.struct 模块（结构体） struct模块的功能可以将python中的数据类型转换成C语言中的结构体（bytes类型） 1234567import structs = 123456789res = struct.pack('i', s)print(res)res2 = struct.unpack('i', res)print(res2)print(res2[0]) 2.粘包的解决方案基本版 既然我们拿到了一个可以固定长度的办法，那么应用struct模块，可以固定长度了。 为字节流加上自定义固定长度报头，报头中包含字节流长度，然后一次send到对端，对端在接收时，先从缓存中取出定长的报头，然后再取真实数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#服务器端import socketimport subprocessimport structsever = socket.socket()sever.bind(('127.0.0.1', 33520))sever.listen()while True: client, address = sever.accept() while True: try: cmd = client.recv(1024).decode('utf-8') #利用子进程模块启动程序 p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) #管道输出的信息有正确和错误的 data = p.stdout.read() err_data = p.stderr.read() #先将数据的长度发送给客户端 length = len(data)+len(err_data) #利用struct模块将数据的长度信息转化成固定的字节 len_data = struct.pack('i', length) #以下将信息传输给客户端 #1.数据的长度 client.send(len_data) #2.正确的数据 client.send(data) #2.错误管道的数据 client.send(err_data) except Exception as e: client.close() print('连接中断。。。。') break #客户端 import socketimport structclient = socket.socket()client.connect(('127.0.0.1', 33520))while True: cmd = input('请输入指令&gt;&gt;:').strip().encode('utf-8') client.send(cmd) #1.先接收传过来数据的长度是多少，我们通过struct模块固定了字节长度为4 length = client.recv(4) #将struct的字节再转回去整型数字 len_data = struct.unpack('i', length) print(len_data) len_data = len_data[0] print('数据长度为%s:' % len_data) all_data = b'' recv_size = 0 #2.接收真实的数据 #循环接收直到接收到数据的长度等于数据的真实长度（总长度） while recv_size &lt; len_data: data = client.recv(1024) recv_size += len(data) all_data += data print('接收长度%s' % recv_size) print(all_data.decode('gbk')) #总结： 1服务器端： 1.在服务器端先收到命令，打开子进程，然后计算返回的数据的长度 2.先利用struct模块将数据长度转成固定4个字节传给客户端 3.再向客户端发送真实的数据。 客户端（两次接收）： 1.第一次只接受4个字节，因为长度数据就是4个字节。这样防止了数据粘包。解码得到长度数据 2.第二次循环接收真实数据，拼接真实数据完成解码读取数据。 很显然，如果仅仅只是这样肯定无法满足在实际生产中一些需求。那么该怎么修改？ 我们可以把报头做成字典，字典里包含将要发送的真实数据的详细信息，然后json序列化，然后用struck将序列化后的数据长度打包成4个字节（4个字节足够用了） 我们可以将自定义的报头设置成这种这种格式。 发送时： 1先发报头长度 2再编码报头内容然后发送 3最后发真实内容 接收时： 1先收报头长度，用struct取出来 2根据取出的长度收取报头内容，然后解码，反序列化 3从反序列化的结果中取出待取数据的详细信息，然后去取真实的数据内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687#服务器端import socketimport subprocessimport datetimeimport jsonimport structsever = socket.socket()sever.bind(('127.0.0.1', 33520))sever.listen()while True: client, address = sever.accept() while True: try: cmd = client.recv(1024).decode('utf-8') #启动子进程 p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) #得到子进程运行的数据 data = p.stdout.read() #子进程运行正确的输出管道数据，数据读出来后是字节 err_data = p.stderr.read() #子进程运行错误的输出管道数据 #计算数据的总长度 length = len(data) + len(err_data) print('数据总长度：%s' % length) #先需要发送报头信息，以下为创建报头信息（至第一次发送） #需要添加时间信息 time_info = datetime.datetime.now() #设置一个字典将一些额外的信息和长度信息放进去然后json序列化，报头字典 masthead = &#123;&#125; #将时间数据放入报头字典中 masthead['time'] = str(time_info) #时间格式不能被json序列化，所以将其转化为字符串形式 masthead['length'] = length #将报头字典json序列化 json_masthead = json.dumps(masthead) #得到json格式的报头 # 将json格式的报头编码成字节形式 masthead_data = json_masthead.encode('utf-8') #利用struct将报头编码的字节的长度转成固定的字节(4个字节） masthead_length = struct.pack('i', len(masthead_data)) #1.发送报头的长度（第一次发送） client.send(masthead_length) #2.发送报头信息(第二次发送） client.send(masthead_data) #3.发送真实数据（第三次发送） client.send(data) client.send(err_data) except ConnectionResetError: print('客户端断开连接。。。') client.close() break #客户端import socketimport structimport jsonclient = socket.socket()client.connect(('127.0.0.1', 33520))while True: cmd = input('请输入cmd指令(Q\q退出)&gt;&gt;:').strip() if cmd == 'q': break #发送CMD指令至服务器 client.send(cmd.encode('utf-8')) #1.第一次接收，接收报头信息的长度，由于struct模块固定长度为4字节，括号内直接填4 len_masthead = client.recv(4) #利用struct反解报头长度，由于是元组形式，取值得到整型数字masthead_length masthead_length = struct.unpack('i', len_masthead)[0] #2.第二次接收，接收报头信息，接收长度为报头长度masthead_length 被编码成字节形式的json格式的字典, # 解字符编码得到json格式的字典masthead_data masthead_data = client.recv(masthead_length).decode('utf-8') #得到报头字典masthead masthead = json.loads(masthead_data) print('执行时间%s' % masthead['time']) #通过报头字典得到数据长度 data_length = masthead['length'] #3.第三次接收，接收真实数据，真实数据长度为data_length # data = client.recv(data_length) #有可能真实数据长度太大会撑爆内存。 #所以循环读取数据 all_data = b'' length = 0 #循环直到长度大于等于数据长度 while length &lt; data_length: data = client.recv(1024) length += len(data) all_data += data print('数据的总长度：%s' % data_length) #我的电脑是Windows系统，所以用gbk解码系统发出的信息 print(all_data.decode('gbk')) 总结： 1.TCP协议中，会产生粘包现象。粘包现象产生本质就是读取数据长度未知。 2.解决粘包现象本质就是处理读取数据长度。 3.报头的作用就是解决数据传输过程中数据长度怎么计算传达和传输其他额外信息的。]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自定义TCP应用层传输协议]]></title>
    <url>%2F2019%2F09%2F27%2F%E8%87%AA%E5%AE%9A%E4%B9%89TCP%E5%BA%94%E7%94%A8%E5%B1%82%E4%BC%A0%E8%BE%93%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[自定义TCP应用层传输协议什么是分包与黏包？分包：指接受方没有接受到一个完整的包，只接受了部分。黏包：指发送方发送的若干包数据到接收方接收时粘成一包，从接收缓冲区看，后一包数据的头紧接着前一包数据的尾。 PS:因为TCP是面向字节流的，是没有边界的概念的，严格意义上来说，是没有分包和黏包的概念的，但是为了更好理解，也更好来描述现象，我在这里就接着采用这两个名词来解释现象了。我觉得大家知道这个概念就行了，不必细扣，能解决问题就行。 产生分包与黏包现象的原因是什么？ 产生分包原因：可能是IP分片传输导致的，也可能是传输过程中丢失部分包导致出现的半包，还有可能就是一个包可能被分成了两次传输，在取数据的时候，先取到了一部分（还可能与接收的缓冲区大小有关系），总之就是一个数据包被分成了多次接收。 产生黏包的原因：由于TCP协议本身的机制（面向连接的可靠地协议-三次握手机制）客户端与服务器会维持一个连接（Channel），数据在连接不断开的情况下，可以持续不断地将多个数据包发往服务器，但是如果发送的网络数据包太小，那么他本身会启用Nagle算法（可配置是否启用）对较小的数据包进行合并（基于此，TCP的网络延迟要UDP的高些）然后再发送（超时或者包大小足够）。那么这样的话，服务器在接收到消息（数据流）的时候就无法区分哪些数据包是客户端自己分开发送的，这样产生了粘包；服务器在接收到数据后，放到缓冲区中，如果消息没有被及时从缓存区取走，下次在取数据的时候可能就会出现一次取出多个数据包的情况，造成粘包现象 什么是封包与解包？TCP/IP 网络数据以流的方式传输，数据流是由包组成，如何判定接收方收到的包是否是一个完整的包就要在发送时对包进行处理，这就是封包技术，将包处理成包头，包体。包头是包的开始标记，整个包的大小就是包的结束标。 如何自定义协议？发送时数据包是由包头+数据 组成的：其中包头内容分为包类型+包长度。 接收时，只需要先保证将数据包的包头读完整，通过收到的数据包包头里的数据长度和数据包类型，判断出我们将要收到一个带有什么样类型的多少长度的数据。然后循环接收直到接收的数据大小等于数据长度停止，此时我们完成接收一个完整数据包。 消息头部（包含消息长度）消息头部不一定只能是一个字节比如0xAA什么的，也可以包含协议版本号，指令等，当然也可以把消息长度合并到消息消息头部（包含消息长度） 消息头部不一定只能是一个字节比如0xAA什么的，也可以包含协议版本号，指令等，当然也可以把消息长度合并到消息头部里，唯一的要求是包头长度要固定的，包体则可变长。头部里，唯一的要求是包头长度要固定的，包体则可变长。自定义包头：版本号、消息长度、指令 版本号、消息长度、指令数据类型都是无符号32位整数变量，于是这个消息长度固定4*3=12字节。python没有类型定义，所以一般是使用struct模块生成包头。 12345678910import structimport jsonver = 1body = json.dumps(dict(hello="world"))print(body) # &#123;"hello": "world"&#125;cmd = 101header = [ver, body.__len__(), cmd]headPack = struct.pack("!3I", *header)print(headPack) # b'\x00\x00\x00\x01\x00\x00\x00\x12\x00\x00\x00e' 关于不用自定义结束符分割数据包有的人会想用自定义的结束符分割每一个数据包，这样传输数据包时就不需要指定长度甚至也不需要包头了。但是如果这样做，网络传输性能损失非常大，因为每一读取一个字节都要做一次if判断是否是结束符。所以建议还是选择消息头部+消息长度+消息正文这种方式。 而且，使用自定义结束符的时候，如果消息正文中出现这个符号，就会把后面的数据截止，这个时候还需要处理符号转义，类比于\r\n的反斜杠。所以非常不建议使用结束符分割数据包。消息正文 消息正文消息正文的数据格式可以使用Json格式，这里一般是用来存放独特信息的数据。在下面代码中，我使用{&quot;hello&quot;,&quot;world&quot;}数据来测试。在Python使用json模块来生成json数据]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[socket缓冲区以及阻塞模式详解]]></title>
    <url>%2F2019%2F09%2F27%2Fsocket%E7%BC%93%E5%86%B2%E5%8C%BA%E4%BB%A5%E5%8F%8A%E9%98%BB%E5%A1%9E%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[socket缓冲区以及阻塞模式详解每个socket被创建后，都会分配两个缓冲区，输入缓冲区和输出缓冲区。 send()/recv()并不立即向网络中传输数据，而是先将数据写入缓冲区中，再由TCP协议将数据从缓冲区发送到目标机器。一旦将数据写入到缓冲区，函数就可以成功返回，不管它们有没有到达目标机器，也不管它们何时被发送到网络，这些都是TCP协议负责的事情。 TCP协议独立于send()/recv()函数，数据有可能刚被写入缓冲区就发送到网络，也可能在缓冲区不断积压，多次写入的数据被一次性发送到网络，这取决于当时的网络情况、当前线程是否空闲等诸多因素，不由程序员控制。 send()/recv()函数也是如此，也从输入缓冲区中读取数据，而不是直接从网络中读取。 ​ TCP套接字的I/O缓冲区示意图 这些I/O缓冲区特性可整理如下： I/O缓冲区在每个TCP套接字中单独存在； I/O缓冲区在创建套接字时自动生成； 即使关闭套接字也会继续传送输出缓冲区中遗留的数据； 关闭套接字将丢失输入缓冲区中的数据。 默认的套节字缓冲区大小可能不够用。因此我们可以想办法去修改默认缓存区大小，改成一个合适的值。 我们使用的方法是利用套节字对象的setsocketopt( )的方法修改默认的套节字缓冲区大小。 首先我们定义两个常量：SEND_BUF_SIZE和RECV_BUF_SIZE。然后在一个函数中调用套节字的实例setsocketopt( )方法。以下是具体代码： 12345678910111213141516171819202122232425262728SEND_BUF_SIZE = 4096RECV_BUF_SIZE = 4096def modify_buff_size(): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) bufsize = sock.getsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF) print "Buffer size [Before] :%d" %bufsize sock.setsockopt(socket.SOL_TCP, socket.TCP_NODELAY, 1) sock.setsockopt( socket.SOL_SOCKET, socket.SO_SNDBUF, SEND_BUF_SIZE ) sock.setsockopt( socket.SOL_SOCKET, socket.SO_RCVBUF, RECV_BUF_SIZE ) bufsize = sock.getsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF) print "Buffer size [After] :%d" %bufsizeif __name__ == '__main__': modify_buff_size() 在套节字对象可调用方法getsocketopt( )和setsocketopt( )分别获取和修改套节字对象属性。setsocketopt( )方法接收三个参数：level、optname和value。其中optname是选项名、value是该选项的值、level代表选项所在的协议层，以下列出了level常用的符号常量（so_*等）的意义。 在套节字对象可调用方法getsocketopt( )和setsocketopt( )分别获取和修改套节字对象属性。setsocketopt( )方法接收三个参数：level、optname和value。其中optname是选项名、value是该选项的值、level代表选项所在的协议层，以下列出了level常用的符号常量（so_*等）的意义。 选项 意义SO_BROADCAST 允许套接口传送广播信息SO_DEBUG 记录调试信息SO_DONTLINER 不要因为数据未发送就阻塞关闭操作SO_DONTROUTE 禁止选径；直接传送SO_KEEPALⅣE 发送“保持活动”包SO_LINGER struct linger FAR* 如关闭时有未发送数据，则逗留SO_OOBINLINE 在常规数据流中接收带外数据SO_RCVBUF 为接收确定缓冲区大小SO_REUSEADDR 允许套接口和一个已在使用中的地址捆绑（参见bind（）)SO_SNDBUF 指定发送缓冲区大小TCP_NODELAY 禁止发送合并的Nagle算法 阻塞模式对于TCP套接字默认情况下，当使用send()发送数据时。 1) 首先会检查缓冲区，如果缓冲区的可用空间长度小于要发送的数据，那么 send() 会被阻塞（暂停执行），直到缓冲区中的数据被发送到目标机器，腾出足够的空间，才唤醒 send() 函数继续写入数据。 2) 如果TCP协议正在向网络发送数据，那么输出缓冲区会被锁定，不允许写入，send() 也会被阻塞，直到数据发送完毕缓冲区解锁，send() 才会被唤醒。 3) 如果要写入的数据大于缓冲区的最大长度，那么将分批写入。 4) 直到所有数据被写入缓冲区send() 才能返回。 当使用recv() 读取数据时：1) 首先会检查缓冲区，如果缓冲区中有数据，那么就读取，否则函数会被阻塞，直到网络上有数据到来。 2) 如果要读取的数据长度小于缓冲区中的数据长度，那么就不能一次性将缓冲区中的所有数据读出，剩余数据将不断积压，直到有 recv() 函数再次读取。 3) 直到读取到数据后recv() 函数才会返回，否则就一直被阻塞。 这就是TCP套接字的阻塞模式。所谓阻塞，就是上一步动作没有完成，下一步动作将暂停，直到上一步动作完成后才能继续，以保持同步性。]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下的socket演示程序]]></title>
    <url>%2F2019%2F09%2F27%2FLinux%E4%B8%8B%E7%9A%84socket%E6%BC%94%E7%A4%BA%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[一个Web服务之所以不会执行一段时间就结束，本质上是因为它在代码里启动了一个while死循环。另外一个重要特征是，它能够跟客户端打交道，需要进行网络通信，因此需要用到socket这个东西。 socket是一个4元组，标识TCP连接的两个终端：本地IP地址、本地端口、远程IP地址、远程端口。一个socket对唯一地标识着网络上的TCP连接。每个终端的IP地址和端口号，称为socket。服务器创建socket并开始接受客户端连接的流程如下： 1. 创建一个TCP/IP socket，socket.socket() 2. 设置一些socket选项，setsockopt()函数 3. 绑定指定地址，bind()函数 4. 将创建的socket变为监听socket，listen()函数 做完这些以后，服务器开始循环地一次接受一个客户端连接。当有连接到达时，accept调用返回已连接的客户端socket。然后服务器从这个socket读取请求数据，处理后发送一个响应给客户端。然后服务器关闭客户端连接，准备好接受新的客户端连接。 服务端：TCP服务端一般需要下面几个操作：建立，绑定IP地址和端口，监听端口，等待连接,接收数据,传输数据 ,关闭连接 建立：server=socket.socket(socket.AF_INET, socket.SOCK_STREAM) 【参数默认就是socket.AF_INET, socket.SOCK_STREAM】 绑定端口：server.bind((‘IP地址’,端口))，【地址和端口号是一个 tuple 】 监听：server.listen() 接受连接: conn,addr=server.accept()，返回值是一个连接实例和一个地址，地址是连接过来的客户端地址，而数据操作要利用这个连接实例 传输数据：conn.send(data)，【传输的数据必须是字节流，所以对字符串数据需要使用encode() 】 接收数据read：conn.recv(size)，【传输的数据必须是字节流,size是接收的字节数,如果需要转成Unicode，需要使用decode() 】 关闭连接close:close() 1234567891011121314151617import socketserver=socket.socket()#建立socketserver.bind(('localhost',1234))#绑定server.listen()#监听print("开始等待。。。")conn,addr=server.accept()#接收连接print("连接成功")data=conn.recv(1024)#接收数据print(data.decode())conn.send(data)#发送数据server.close()#关闭连接print("--------------------")server 只接受一次 client 请求，当 server 向 client 传回数据后，程序就运行结束了。如果想再次接收到服务器的数据，必须再次运行 server，所以这是一个非常简陋的 socket 程序，不能够一直接受客户端的请求。 上述代码存在一个问题：只能接受一次连接，连接结束后，服务端socket将关闭，更改成不立即关闭能等待下一个连接的： 12345678910111213141516171819202122#服务器端import socketserver = socket.socket()server.bind(('localhost',1234)) #绑定ip和端口server.listen(5) #监听while True: print("开始等待") conn, addr = server.accept() print(conn, addr) print("客户端连接") while True: data = conn.recv(1024) print("recv:",data) if not data: #当data=0时为真 print("连接断开...") break conn.send(data)server.close() 注：上述代码中在linux中正常运行，在windows中会报错！ 如果要在windows中运行，需要捕获异常： 1234567891011121314151617181920212223242526#服务器端import socketserver = socket.socket()server.bind(('localhost',1234)) #绑定ip和端口server.listen(5) #监听while True: print("开始等待") conn, addr = server.accept() print(conn, addr) print("客户端连接") while True: try: data = conn.recv(1024) print("recv:",data) if not data: #当data=0时为真 print("连接断开...") break conn.send(data) except ConnectionResetError as e: print(e) breakserver.close() 客户端：TCP客户端一般需要下面几个操作：建立socket,连接远程socket，传输数据 ，接收数据,关闭连接 建立：client=socket.socket() 连接：client.connect((‘IP地址’,端口))，其中地址和端口号是一个 tuple 传输数据：client.send(data)，传输的数据必须是字节流,所以对字符串数据需要使用encode() 接收数据recv：client.recv(size)，传输的数据是字节流,如果需要转成Unicode，需要使用decode() 关闭连接close:close() 12345678import socketclient=socket.socket()#建立socketclient.connect(('localhost',1234))#连接client.send("你好".encode())#发送数据data=client.recv(1024)#接收数据print(data.decode())client.close()#关闭连接 上述代码存在一个问题：只能发送一次数据，发生完数据就会断开连接，改成可以多次发送数据，不自动断开的【前提是服务端能接收多次】： 1234567891011121314import socketclient=socket.socket()client.connect(('localhost',1234))while True: cmd=input("&gt;&gt;") if len(cmd)==0: continue client.send(cmd.encode()) cmd_res=client.recv(1024) print(cmd_res.decode())client.close() 正常情况下，程序运行到accept()函数就会被阻塞，等待客户端发起请求。 接下来再启动一个终端，运行client.py client接收从server发送过来的字符串。然后继续运行。 client运行后，通过connect()函数向server发起请求，处于监听状态的server被激活，执行accept()函数，接受客户端的请求，然后执行send()函数向client传回数据。client接收到传回的数据后，通过recv()将数据读取出来。 通过socket()函数创建一个套接字，参数AF_INET表示使用IPv4地址，SOCK_STREAM 表示使用面向连接的套接字，在 Linux 中，socket 也是一种文件，有文件描述符，可以使用 send() / recv() 函数进行 I/O 操作。 通过 bind() 函数将套接字 serv_sock 与特定的 IP 地址和端口绑定。 socket() 函数确定了套接字的各种属性，bind() 函数让套接字与特定的IP地址和端口对应起来，这样客户端才能连接到该套接字。 让套接字处于被动监听状态。所谓被动监听，是指套接字一直处于“睡眠”中，直到客户端发起请求才会被“唤醒”。 accept() 函数用来接收客户端的请求。程序一旦执行到 accept() 就会被阻塞（暂停运行），直到客户端发起请求。 send() 函数用来向套接字文件中写入数据，也就是向客户端发送数据。 和普通文件一样，socket 在使用完毕后也要用 close() 关闭。 client与server区别 ： 通过 connect() 向服务器发起请求，服务器的IP地址和端口号保存在client 结构体中。直到服务器传回数据后，直到退出循环，connect() 才运行结束。 通过 recv() 从套接字文件中读取数据。 UDP:服务端：UDP服务端通常有以下几个操作：创建socket,绑定端口，传输数据，接收数据 创建socket:server=socket.socket(socket.AF_INET,socket.SOCK_DGRAM) 绑定端口：server.bind(addr),【addr是一个元组，内容为(地址,端口)】 接收数据：data,client_addr=server.recvfrom(1024) 传输数据：server.sendto(data,client_addr) 12345678910111213141516import socketimport timeserver=socket.socket(socket.AF_INET,socket.SOCK_DGRAM)server.bind(("localhost",1234))start_time=time.time()while True: data,addr=server.recvfrom(1024) print(data,addr) server.sendto("hello".encode(),addr) time.sleep(1) if time.time()-start_time&gt;30: breakserver.close() 客户端：UDP客户端通常有以下几个操作：创建socket,传输数据，接收数据 创建socket:client=socket.socket(socket.AF_INET,socket.SOCK_DGRAM) 传输数据：server.sendto(data,addr),【addr是一个元组，内容为(地址,端口)】 接收数据：data,server_addr=client.recvfrom(1024) 12345678910111213141516import socket,timeclient=socket.socket(socket.AF_INET,socket.SOCK_DGRAM)addr=("localhost",1234)start_time=time.time()while True: client.sendto(time.ctime().encode(),addr) data,addr= client.recvfrom(1024) print(data) time.sleep(1) if time.time()-start_time&gt;30: breakclient.close()]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IP、MAC和端口号——网络通信中确认身份信息的三要素]]></title>
    <url>%2F2019%2F09%2F27%2FIP%E3%80%81MAC%E5%92%8C%E7%AB%AF%E5%8F%A3%E5%8F%B7%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E4%B8%AD%E7%A1%AE%E8%AE%A4%E8%BA%AB%E4%BB%BD%E4%BF%A1%E6%81%AF%E7%9A%84%E4%B8%89%E8%A6%81%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[在茫茫的互联网海洋中，要找到一台计算机非常不容易，有三个要素必须具备，它们分别是 IP 地址、MAC 地址和端口号。 IP地址IP地址是 Internet Protocol Address 的缩写，译为“网际协议地址”。目前大部分软件使用 IPv4 地址，但 IPv6 也正在被人们接受，尤其是在教育网中，已经大量使用。 一台计算机可以拥有一个独立的 IP 地址，一个局域网也可以拥有一个独立的 IP 地址（对外就好像只有一台计算机）。对于目前广泛使用 IPv4 地址，它的资源是非常有限的，一台计算机一个 IP 地址是不现实的，往往是一个局域网才拥有一个 IP 地址。 在因特网上进行通信时，必须要知道对方的 IP 地址。实际上数据包中已经附带了 IP 地址，把数据包发送给路由器以后，路由器会根据 IP 地址找到对方的地里位置，完成一次数据的传递。路由器有非常高效和智能的算法，很快就会找到目标计算机。 MAC地址现实的情况是，一个局域网往往才能拥有一个独立的 IP；换句话说，IP 地址只能定位到一个局域网，无法定位到具体的一台计算机。这可怎么办呀？这样也没法通信啊。 其实，真正能唯一标识一台计算机的是 MAC 地址，每个网卡的 MAC 地址在全世界都是独一无二的。计算机出厂时，MAC 地址已经被写死到网卡里面了（当然通过某些“奇巧淫技”也是可以修改的）。局域网中的路由器/交换机会记录每台计算机的 MAC 地址。 MAC 地址是 Media Access Control Address 的缩写，直译为“媒体访问控制地址”，也称为局域网地址（LAN Address），以太网地址（Ethernet Address）或物理地址（Physical Address）。 数据包中除了会附带对方的 IP 地址，还会附带对方的 MAC 地址，当数据包达到局域网以后，路由器/交换机会根据数据包中的 MAC 地址找到对应的计算机，然后把数据包转交给它，这样就完成了数据的传递。 端口号有了 IP 地址和 MAC 地址，虽然可以找到目标计算机，但仍然不能进行通信。一台计算机可以同时提供多种网络服务，例如 Web 服务（网站）、FTP 服务（文件传输服务）、SMTP 服务（邮箱服务）等，仅有 IP 地址和 MAC 地址，计算机虽然可以正确接收到数据包，但是却不知道要将数据包交给哪个网络程序来处理，所以通信失败。 为了区分不同的网络程序，计算机会为每个网络程序分配一个独一无二的端口号（Port Number），例如，Web 服务的端口号是 80，FTP 服务的端口号是 21，SMTP 服务的端口号是 25。 端口（Port）是一个虚拟的、逻辑上的概念。可以将端口理解为一道门，数据通过这道门流入流出，每道门有不同的编号，就是端口号。如下图所示：]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP/IP协议族]]></title>
    <url>%2F2019%2F09%2F27%2FTCP%2FIP%E5%8D%8F%E8%AE%AE%E6%97%8F%2F</url>
    <content type="text"><![CDATA[目前实际使用的网络模型是 TCP/IP 模型，它对 OSI 模型进行了简化，只包含了四层，从上到下分别是应用层、传输层、网络层和链路层（网络接口层），每一层都包含了若干协议。 协议（Protocol）就是网络通信过程中的约定或者合同，通信的双方必须都遵守才能正常收发数据。协议有很多种，例如 TCP、UDP、IP 等，通信的双方必须使用同一协议才能通信。协议是一种规范，由计算机组织制定，规定了很多细节，例如，如何建立连接，如何相互识别等。 协议仅仅是一种规范，必须由计算机软件来实现。例如 IP 协议规定了如何找到目标计算机，那么各个开发商在开发自己的软件时就必须遵守该协议，不能另起炉灶。 TCP/IP 模型包含了 TCP、IP、UDP、Telnet、FTP、SMTP 等上百个互为关联的协议，其中 TCP 和 IP 是最常用的两种底层协议，所以把它们统称为“TCP/IP 协议族”。 也就是说，“TCP/IP模型”中所涉及到的协议称为“TCP/IP协议族”，你可以区分这两个概念，也可以认为它们是等价的，随便你怎么想。 socket编程是基于 TCP 和 UDP 协议的，它们的层级关系如下图所示： 【扩展阅读】开放式系统（Open System）把协议分成多个层次有哪些优点？协议设计更容易？当然这也足以成为优点之一。但是还有更重要的原因，就是为了通过标准化操作设计成开放式系统。标准本身就是对外公开的，会引导更多的人遵守规范。以多个标准为依据设计的系统称为开放式系统（Open System），我们现在学习的 TCP/IP 协议族也属于其中之一。 接下来了解一下开放式系统具有哪些优点。 路由器用来完成 IP 层的交互任务。某个网络原来使用 A 公司的路由器，现要将其替换成 B 公司的，是否可行？这并非难事，并不一定要换成同一公司的同一型号路由器，因为所有生产商都会按照 IP 层标准制造。 再举个例子。大家的计算机是否装有网络接口卡，也就是所谓的网卡？尚未安装也无妨，其实很容易买到，因为所有网卡制造商都会遵守链路层的协议标准。这就是开放式系统的优点。 标准的存在意味着高速的技术发展，这也是开放式系统设计最大的原因所在。实际上，软件工程中的“面向对象（Object Oriented）”的诞生背景中也有标准化的影子。也就是说，标准对于技术发展起着举足轻重的作用。]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OSI网络七层模型简明教程]]></title>
    <url>%2F2019%2F09%2F27%2FOSI%E7%BD%91%E7%BB%9C%E4%B8%83%E5%B1%82%E6%A8%A1%E5%9E%8B%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[OSI 是 Open System Interconnection 的缩写，译为“开放式系统互联”。 OSI 模型把网络通信的工作分为 7 层，从下到上分别是物理层、数据链路层、网络层、传输层、会话层、表示层和应用层。 OSI 只是存在于概念和理论上的一种模型，它的缺点是分层太多，增加了网络工作的复杂性，所以没有大规模应用。后来人们对 OSI 进行了简化，合并了一些层，最终只保留了 4 层，从下到上分别是接口层、网络层、传输层和应用层，这就是大名鼎鼎的 TCP/IP 模型。 这个网络模型究竟是干什么呢？简而言之就是进行数据封装的。 我们平常使用的程序（或者说软件）一般都是通过应用层来访问网络的，程序产生的数据会一层一层地往下传输，直到最后的网络接口层，就通过网线发送到互联网上去了。数据每往下走一层，就会被这一层的协议增加一层包装，等到发送到互联网上时，已经比原始数据多了四层包装。整个数据封装的过程就像俄罗斯套娃。 当另一台计算机接收到数据包时，会从网络接口层再一层一层往上传输，每传输一层就拆开一层包装，直到最后的应用层，就得到了最原始的数据，这才是程序要使用的数据。 给数据加包装的过程，实际上就是在数据的头部增加一个标志（一个数据块），表示数据经过了这一层，我已经处理过了。给数据拆包装的过程正好相反，就是去掉数据头部的标志，让它逐渐现出原形。 你看，在互联网上传输一份数据是多么地复杂啊，而我们却感受不到，这就是网络模型的厉害之处。我们只需要在代码中调用一个函数，就能让下面的所有网络层为我们工作。 我们所说的 socket 编程，是站在传输层的基础上，所以可以使用 TCP/UDP 协议，但是不能干「访问网页」这样的事情，因为访问网页所需要的 http 协议位于应用层。 两台计算机进行通信时，必须遵守以下原则： 必须是同一层次进行通信，比如，A 计算机的应用层和 B 计算机的传输层就不能通信，因为它们不在一个层次，数据的拆包会遇到问题。 每一层的功能都必须相同，也就是拥有完全相同的网络模型。如果网络模型都不同，那不就乱套了，谁都不认识谁。 数据只能逐层传输，不能跃层。 每一层可以使用下层提供的服务，并向上层提供服务。]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面向连接和无连接的套接字到底有什么区别？]]></title>
    <url>%2F2019%2F09%2F27%2F%E9%9D%A2%E5%90%91%E8%BF%9E%E6%8E%A5%E5%92%8C%E6%97%A0%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%A5%97%E6%8E%A5%E5%AD%97%E5%88%B0%E5%BA%95%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[流格式套接字（Stream Sockets）就是“面向连接的套接字”，它基于 TCP 协议；数据报格式套接字（Datagram Sockets）就是“无连接的套接字”，它基于 UDP 协议。 这给大家造成一种印象，面向连接就是可靠的通信，无连接就是不可靠的通信，实际情况是这样吗？ 另外，不管是哪种数据传输方式，都得通过整个 Internet 网络的物理线路将数据传输过去，从这个层面理解，所有的 socket 都是有物理连接的呀，为什么还有无连接的 socket 呢？ 从字面上理解，面向连接好像有一条管道，它连接发送端和接收端，数据包都通过这条管道来传输。当然，两台计算机在通信之前必须先搭建好管道。 无连接好像没头苍蝇乱撞，数据包从发送端到接收端并没有固定的线路，爱怎么走就怎么走，只要能到达就行。每个数据包都比较自私，不和别人分享自己的线路，但是，大家最终都能殊途同归，到达接收端。 这样理解没错，但是我相信这还不够深入，大家还是感觉云里雾里，没有看到本质。好，接下来就是见证奇迹的时刻，我会用实例给大家演示！ 上图是一个简化的互联网模型，H1 ~ H6 表示计算机，A~E 表示路由器，发送端发送的数据必须经过路由器的转发才能到达接收端。假设 H1 要发送若干个数据包给 H6，那么有多条路径可以选择，比如： 路径①：H1 –&gt; A –&gt; C –&gt; E –&gt; H6 路径②：H1 –&gt; A –&gt; B –&gt; E –&gt; H6 路径③：H1 –&gt; A –&gt; B –&gt; D –&gt; E –&gt; H6 路径④：H1 –&gt; A –&gt; B –&gt; C –&gt; E –&gt; H6 路径⑤：H1 –&gt; A –&gt; C –&gt; B –&gt; D –&gt; E –&gt; H6 数据包的传输路径是路由器根据算法来计算出来的，算法会考虑很多因素，比如网络的拥堵状况、下一个路由器是否忙碌等。 无连接的套接字对于无连接的套接字，每个数据包可以选择不同的路径，比如第一个数据包选择路径④，第二个数据包选择路径①，第三个数据包选择路径②……当然，它们也可以选择相同的路径，那也只不过是巧合而已。每个数据包之间都是独立的，各走各的路，谁也不影响谁，除了迷路的或者发生意外的数据包，最后都能到达 H6。但是，到达的顺序是不确定的，比如： 第一个数据包选择了一条比较长的路径（比如路径⑤），第三个数据包选择了一条比较短的路径（比如路径①），虽然第一个数据包很早就出发了，但是走的路比较远，最终还是晚于第三个数据包达到。 第一个数据包选择了一条比较短的路径（比如路径①），第三个数据包选择了一条比较长的路径（比如路径⑤），按理说第一个数据包应该先到达，但是非常不幸，第一个数据包走的路比较拥堵，这条路上的数据量非常大，路由器处理得很慢，所以它还是晚于第三个数据包达到了。 还有一些意外情况会发生，比如： 第一个数据包选择了路径①，但是路由器C突然断电了，那它就到不了 H6 了。 第三个数据包选择了路径②，虽然路不远，但是太拥堵，以至于它等待的时间太长，路由器把它丢弃了。 总之，对于无连接的套接字，数据包在传输过程中会发生各种不测，也会发生各种奇迹。H1 只负责把数据包发出，至于它什么时候到达，先到达还是后到达，有没有成功到达，H1 都不管了；H6 也没有选择的权利，只能被动接收，收到什么算什么，爱用不用。 面向连接的套接字面向连接的套接字在正式通信之前要先确定一条路径，没有特殊情况的话，以后就固定地使用这条路径来传递数据包了。当然，路径被破坏的话，比如某个路由器断电了，那么会重新建立路径。 这条路径是由路由器维护的，路径上的所有路由器都要存储该路径的信息（实际上只需要存储上游和下游的两个路由器的位置就行），所以路由器是有开销的。H1 和 H6 通信完毕后，要断开连接，销毁路径，这个时候路由器也会把之前存储的路径信息擦除。 在很多网络通信教程中，这条预先建立好的路径被称为“虚电路”，就是一条虚拟的通信电路。 为了保证数据包准确、顺序地到达，发送端在发送数据包以后，必须得到接收端的确认才发送下一个数据包；如果数据包发出去了，一段时间以后仍然没有得到接收端的回应，那么发送端会重新再发送一次，直到得到接收端的回应。这样一来，发送端发送的所有数据包都能到达接收端，并且是按照顺序到达的。 发送端发送一个数据包，如何得到接收端的确认呢？很简单，为每一个数据包分配一个 ID，接收端接收到数据包以后，再给发送端返回一个数据包，告诉发送端我接收到了 ID 为 xxx 的数据包。 面向连接的套接字会比无连接的套接字多出很多数据包，因为发送端每发送一个数据包，接收端就会返回一个数据包。此外，建立连接和断开连接的过程也会传递很多数据包。 不但是数量多了，每个数据包也变大了：除了源端口和目的端口，面向连接的套接字还包括序号、确认信号、数据偏移、控制标志（通常说的 URG、ACK、PSH、RST、SYN、FIN）、窗口、校验和、紧急指针、选项等信息；而无连接的套接字则只包含长度和校验和信息。 有连接的数据包比无连接大很多，这意味着更大的负载和更大的带宽。许多即时聊天软件采用 UDP 协议（无连接套接字），与此有莫大的关系。 总结两种套接字各有优缺点： 无连接套接字传输效率高，但是不可靠，有丢失数据包、捣乱数据的风险； 有连接套接字非常可靠，万无一失，但是传输效率低，耗费资源多。 两种套接字的特点决定了它们的应用场景，有些服务对可靠性要求比较高，必须数据包能够完整无误地送达，那就得选择有连接的套接字（TCP 服务），比如 HTTP、FTP 等；而另一些服务，并不需要那么高的可靠性，效率和实时才是它们所关心的，那就可以选择无连接的套接字（UDP 服务），比如 DNS、即时聊天工具等。]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[套接字有哪些类型？socket有哪些类型？]]></title>
    <url>%2F2019%2F09%2F27%2F%E5%A5%97%E6%8E%A5%E5%AD%97%E6%9C%89%E5%93%AA%E4%BA%9B%E7%B1%BB%E5%9E%8B%EF%BC%9Fsocket%E6%9C%89%E5%93%AA%E4%BA%9B%E7%B1%BB%E5%9E%8B%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[套接字有哪些类型？socket有哪些类型？这个世界上有很多种套接字（socket），比如 DARPA Internet 地址（Internet 套接字）、本地节点的路径名（Unix套接字）、CCITT X.25地址（X.25 套接字）等。——Internet 套接字，它是最具代表性的，也是最经典最常用的。以后我们提及套接字，指的都是 Internet 套接字。 根据数据的传输方式，可以将Internet套接字分成两种类型。通过socket() 函数创建连接时，必须告诉它使用哪种数据传输方式。 Internet套接字其实还有很多其它数据传输方式。 流格式套接字（SOCK_STREAM）流格式套接字（Stream Sockets）也叫“面向连接的套接字”，在代码中使用 SOCK_STREAM 表示。 SOCK_STREAM是一种可靠的、双向的通信数据流，数据可以准确无误地到达另一台计算机，如果损坏或丢失，可以重新发送。 流格式套接字有自己的纠错机制。 SOCK_STREAM 有以下几个特征： 数据在传输过程中不会消失； 数据是按照顺序传输的； 数据的发送和接收不是同步的（有的教程也称“不存在数据边界”）。 可以将 SOCK_STREAM 比喻成一条传送带，只要传送带本身没有问题（不会断网），就能保证数据不丢失；同时，较晚传送的数据不会先到达，较早传送的数据不会晚到达，这就保证了数据是按照顺序传递的。 为什么流格式套接字可以达到高质量的数据传输呢？这是因为它使用了 TCP 协议（The Transmission Control Protocol，传输控制协议），TCP 协议会控制你的数据按照顺序到达并且没有错误。 那么，“数据的发送和接收不同步”该如何理解呢？ 流格式套接字的内部有一个缓冲区（也就是字符数组），通过 socket 传输的数据将保存到这个缓冲区。接收端在收到数据后并不一定立即读取，只要数据不超过缓冲区的容量，接收端有可能在缓冲区被填满以后一次性地读取，也可能分成好几次读取。 流格式套接字有什么实际的应用场景吗？浏览器所使用的 http 协议就基于面向连接的套接字，因为必须要确保数据准确无误，否则加载的 HTML 将无法解析。数据报格式套接字（Datagram Sockets）也叫“无连接的套接字”，在代码中使用 SOCK_DGRAM 表示。 因为数据报套接字所做的校验工作少，所以在传输效率方面比流格式套接字要高。 强调快速传输而非传输顺序； 传输的数据可能丢失也可能损毁； 限制每次传输的数据大小； 数据的发送和接收是同步的（有的教程也称“存在数据边界”）。 众所周知，速度是快递行业的生命。用摩托车发往同一地点的两件包裹无需保证顺序，只要以最快的速度交给客户就行。这种方式存在损坏或丢失的风险，而且包裹大小有一定限制。因此，想要传递大量包裹，就得分配发送。 另外，用两辆摩托车分别发送两件包裹，那么接收者也需要分两次接收，所以“数据的发送和接收是同步的”；换句话说，接收次数应该和发送次数相同。总之，数据报套接字是一种不可靠的、不按顺序传递的、以追求速度为目的的套接字。 QQ 视频聊天和语音聊天就使用 SOCK_DGRAM 来传输数据，因为首先要保证通信的效率，尽量减小延迟，而数据的正确性是次要的，即使丢失很小的一部分数据，视频和音频也可以正常解析，最多出现噪点或杂音，不会对通信质量有实质的影响。]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[socket是什么？套接字是什么？]]></title>
    <url>%2F2019%2F09%2F27%2Fsocket%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E5%A5%97%E6%8E%A5%E5%AD%97%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[socket是什么？套接字是什么？网络编程就是编写程序使两台联网的计算机相互交换数据。 那么，这两台计算机之间用什么传输数据呢？首先需要物理连接。如今大部分计算机都已经连接到互联网，因此不用担心这一点。 在此基础上，只需要考虑如何编写数据传输程序。但实际上这点也不用愁，因为操作系统已经提供了socket。即使对网络数据传输的原理不太熟悉，我们也能通过socket来编程。 什么是 socket？socket的原意是”插座”，在计算机通信领域，socket被翻译为”套接字”，它是计算之间进行通信的一种约定或一种方式。通过socket这种约定，一台计算机可以接收其他计算机的数据，也可以向其他计算机发送数据。 我们把插头插到插座上就能从电网获得电力供应，同样，为了与远程计算机进行数据传输，需要连接到因特网，而socket就是用来连接到因特网的工具。 socket的典型应用就是web服务器和浏览器:浏览器获取用户输入的URL，向服务器发送请求，服务器分析接收到的URL，将对应的网页内容返回给浏览器，浏览器再经过解析和渲染，就将文字、图片、视频等元素呈现给用户。 学习socket，也就是学习计算机之间如何通信，并编写出实用的程序。 UNIX/Linux 中的 socket 是什么？在 UNIX/Linux 系统中，为了统一对各种硬件的操作，简化接口，不同的硬件设备也都被看成一个文件。对这些文件的操作，等同于对磁盘上普通文件的操作。 你也许听很多高手说过:UNIX/Linux中的一切都是文件！ 为了表示和区分已经打开的文件，UNIX/Linux 会给每个文件分配一个 ID，这个 ID 就是一个整数，被称为文件描述符（File Descriptor） 通常用 0 来表示标准输入文件（stdin），它对应的硬件设备就是键盘； 通常用 1 来表示标准输出文件（stdout），它对应的硬件设备就是显示器。 UNIX/Linux程序在执行任何形式的I/O操作时，都是在读取或者写入一个文件描述符。一个文件描述符只是一个和打开的文件想在关联的整数，它的背后可能是一个硬盘上的普通文件、FIFO、管道、终端、键盘、显示器、甚至是一个网络连接。 请注意，网络连接也是一个文件，它也有文件描述符! 我们可以通过socket()函数来创建一个网络连接，或者说打开一个网络文件，socket()的返回值就是文件描述符。有了文件描述符，我们就可以使用普通的文件操作函数来传输数据了，例如: 用 read() 读取从远程计算机传来的数据； 用 write() 向远程计算机写入数据。 你看，只要用 socket() 创建了连接，剩下的就是文件操作了，网络编程原来就是如此简单！ WIndow 系统中的 socket 是什么？Windows 也有类似“文件描述符”的概念，但通常被称为“文件句柄”。因此，本教程如果涉及 Windows 平台将使用“句柄”，如果涉及 Linux 平台则使用“描述符”。与 UNIX/Linux 不同的是，Windows 会区分 socket 和文件，Windows 就把 socket 当做一个网络连接来对待，因此需要调用专门针对 socket 而设计的数据传输函数，针对普通文件的输入输出函数就无效了。]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之pymysql详解]]></title>
    <url>%2F2019%2F09%2F21%2FPython%E4%B9%8Bpymysql%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Python之pymysql详解PyMySQLPyMySQL 是在 Python3.x 版本中用于连接 MySQL 服务器的一个库，Python2中则使用mysqldb。 安装pymysqlPyMySQL ： 这是一个使Python连接到MySQL的库，它是一个纯Python库。 PyMySQL是一个开源项目 ：https://github.com/PyMySQL/PyMySQL 通过下面的命令来进行安装pymysql模块： 1pip3 install PyMySQL 连接数据库安装好pymysql后，在python程序中可以用 import pymysql 来导入模块。 在连接数据库前，首先要确定数据库服务端已启动。在数据库中有已经创建好的数据库，这里连接数据库‘mydb’，使用的用户名为‘root’，密码为‘mysql’，你可以可以自己设定。 比较常用的参数包括: host:数据库主机名.默认是用本地主机 user:数据库登陆名.默认是当前用户 passwd:数据库登陆的秘密.默认为空 db:要使用的数据库名.没有默认值 port:MySQL服务使用的TCP端口.默认是3306 charset:数据库编码 123456789101112131415161718导入pymysqlimport pymysql 打开数据库连接db = pymysql.connect( host = ''127.0.0.1'', 服务端地址 port = 3306, 服务端端口 user = ''root'', 用户名 password = ''mysql'', 密码 database = ''mydb'', 要连接的数据库 charset = ''utf8'' 设置数据库编码)使用 cursor() 方法创建一个游标对象 cursorcursor = db.cursor()关闭数据库连接db.close() 插入数据这个连接对象也提供了对事务操作的支持,标准的方法: commit() 提交数据 rollback() 发生错误后回滚 commit()方法游标的所有更新操作，rollback（）方法回滚当前游标的所有操作 123456789101112131415161718192021222324import pymysqldb = pymysql.connect( host = ''127.0.0.1'', port = 3306, user = ''root'', password = ''mysql'', database = ''mydb'', charset = ''utf8'' )cursor = db.cursor()sql = """INSERT INTO EMPLOYEE(NAME, PASSWORD, AGE, SEX) VALUES (''LinWoW'', ''mysql'', 20, ''M'')"""try： cursor.execute(sql) 确认数据无误之后 commit之后才会将数据真正修改到数据库，如果不执行，程序运行正常，但是数据实际未插入成功 db.commit()except: print(''插入数据失败！'') 发生错误时回滚 db.rollback()db.close() 为了操作方便，可以在打开数据库连接的参数中添加下面的语句，修改完数据后就不需要手动确认数据执行。 1autocommit = True 查询数据Python查询Mysql使用 fetchone() 方法获取单条数据, 使用fetchall() 方法获取多条数据。 fetchone(): 该方法获取下一个查询结果集。结果集是一个对象 etchall(): 接收全部的返回结果行. rowcount: 这是一个只读属性，并返回执行execute()方法后影响的行数。 12345678910111213141516171819202122232425262728293031323334353637383940import pymysqldb = pymysql.connect( host = ''127.0.0.1'', port = 3306, user = ''root'', password = ''mysql'', database = ''mydb'', charset = ''utf8'', autocommit = True)cursor = db.cursor(pymysql.cursors.DictCursor) 设置为字典的形式返回数据（默认是元组）使用 execute() 方法执行 SQL 查询 ，这里返回的是查询到数据的总条数cursor.execute("SELECT * from employee") 使用 fetchone() 方法获取查询到的单条数据，可以利用该函数实现一条一条数据的读取data1 = cursor.fetchone()print (data1)''''''不设置数据返回类型打印的结果(2, ''LinWoW'', ''mysql'', 20, ''M'')设置数据返回类型为字典后打印的结果&#123;''ID'': 2, ''NAME'': ''LinWoW'', ''PASSWORD'': ''mysql'', ''AGE'': 20, ''SEX'': ''M''&#125;''''''使用 fetchall() 方法获取所有查询到的所有数据data2 = cursor.fetchall()print (data2)''''''[&#123;''ID'': 3, ''NAME'': ''LinWoW'', ''PASSWORD'': ''mysql'', ''AGE'': 20, ''SEX'': ''M''&#125;, &#123;''ID'': 4, ''NAME'': ''LinWoW'', ''PASSWORD'': ''mysql'', ''AGE'': 20, ''SEX'': ''M''&#125;]''''''scroll()类似文件操作中的seek(),第一个参数表示偏移量，第二个参数表示偏移的起始位置绝对移动，参照开始的位置cursor.scroll(1,''absolute'')相对移动，参照当前光标的位置cursor.scroll(1,''relative'') db.close() 更新数据1234567891011121314151617181920import pymysqldb = pymysql.connect( host = ''127.0.0.1'', port = 3306, user = ''root'', password = ''mysql'', database = ''mydb'', charset = ''utf8'' )cursor = db.cursor()sql = "UPDATE EMPLOYEE SET AGE = 18 WHERE ID = %s" % (2)try： cursor.execute(sql) db.commit()except: print(''修改数据失败！'') db.rollback()db.close() 删除数据1234567891011121314151617181920import pymysqldb = pymysql.connect( host = ''127.0.0.1'', port = 3306, user = ''root'', password = ''mysql'', database = ''mydb'', charset = ''utf8'' )cursor = db.cursor()sql = "DELETE FROM EMPLOYEE WHERE AGE &gt; %s" % (20)try： cursor.execute(sql) db.commit()except: print(''删除数据失败！'') db.rollback()db.close() 查询优化为了防止SQL注入，千万不要手动拼接(关键参数)查询条件，通过下面的示例来实现对查询数据的优化。 12345678910111213141516171819202122232425262728293031import pymysqlcone = pymysql.connect( host = ''127.0.0.1'', port = 3306, user = ''root'', password = ''mysql'', database = ''day41'', charset = ''utf8'', autocommit = True)ursor = cone.cursor(pymysql.cursors.DictCursor)username = input(''username&gt;&gt;&gt;:'')password = input(''password&gt;&gt;&gt;:'')sql = "select * from employee where name=%s and password=%s"res = cursor.execute(sql,(username,password)) 利用方法实现字符的自动拼接if res: print(''输入正确！'')else: print(''用户名或密码错误！'')cone.close()''''''username&gt;&gt;&gt;:LinWoWpassword&gt;&gt;&gt;:mysql输入正确！username&gt;&gt;&gt;:"qwwqw" or 1=1 --password&gt;&gt;&gt;:用户名或密码错误！'''''']]></content>
      <categories>
        <category>python模块</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx之Location配置详解Location匹配顺序]]></title>
    <url>%2F2019%2F09%2F20%2FNginx%E4%B9%8BLocation%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3Location%E5%8C%B9%E9%85%8D%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[Nginx之Location配置详解(Location匹配顺序)location有”定位”的意思，主要是根据uri来进行不同的定位。在虚假主机的配置中，是必不可少的。 location可以把网站的不同部分，定位到不同的处理方式上。 一、location的基础语法 12345678910111213location [=|~|~*|^~] /uri/ &#123; … &#125;= 开头表示精确匹配^~ 开头表示uri以某个常规字符串开头，理解为匹配 url路径即可。nginx不对url做编码，因此请求为/static/20%/aa，可以被规则^~ /static/ /aa匹配到（注意是空格）。~ 开头表示区分大小写的正则匹配~* 开头表示不区分大小写的正则匹配!~和!~*分别为区分大小写不匹配及不区分大小写不匹配 的正则/ 通用匹配，任何请求都会匹配到。 二、location如何发挥作用 本章以192.168.60.106为例 1、精准匹配和一般匹配 123456789location = /zg/ &#123;root html;index index.html;&#125;location /zg/ &#123;root html;index index2.html&#125; 请求URL：http://192.168.60.106/zg/ 访问时匹配的是：=/zg/ 2、精准匹配和一般匹配，uri后面不带“/”匹配 123456789location = /zg &#123;root html;index index.html;&#125;location /zg &#123;root html;index index2.html;&#125; 请求URL：http://192.168.60.106/zg/ 访问时匹配的是：/zg 3、精准匹配和一般匹配，uri前面和后面都不带“/” 123456789location = zg &#123;root html;index index.html;&#125;location zg &#123;root html/yb;index index1.html&#125; 请求URL：http://192.168.60.106/zg/ 访问时匹配的是：= zg 4、精准匹配和一般匹配，uri带”/“和不带”/“匹配 1234567891011121314location = zg &#123;root html;index index.html;&#125;location /zg/ &#123;root html;index index2.html;&#125;location zg &#123;root html/yb;index index1.html;&#125; 请求URL：http://192.168.60.106/zg/ 访问时匹配的是：/zg/ 顺序换也是一样 综上所述:路径相同时的精准匹配优先，必须是满足/uri/或者uri,要么uri两加都加/,要么uri两边都不加斜杆的情况 5、一般匹配时的匹配规则 1234567location /images/ &#123;root html/file;&#125;location /images/aa &#123;root html/lfile;&#125; 在html下创建file,lfile文件夹，然后在file下创建images文件夹，在images下创建aa文件夹，在lfile下创建images文件夹，接着在images下创建aa文件夹，然后同时在两个aa文件夹下导入test.jpg图片，这样file和lfile下都有images/aa路径 请求url：http://192.168.60.106/images/aa/test.jpg，既能匹配/images/,又能匹配/images/aa，这时以最长uri匹配优先，匹配的是：/images/aa 6、^~开头的非正则匹配和一般匹配 ^~代表非正则匹配，非正则，不需要继续正则匹配。 1234567location ^~ /images/ &#123;root html/file&#125;location /images/aa &#123;root html/lfile;&#125; ^~:如果这个匹配使用^〜前缀，搜索停止。这个前缀官网和网上都说得很含糊，加上这个前缀，是会停止搜索正则匹配，但是想对一般匹配是不会停止的，也就是说还是可以匹配到一般匹配的。 请求url： http://192.168.60.106/images/aa/test.jpg，匹配结果：/images/aa/ 7、^~开头的非正则匹配和正则匹配 ~ 开头表示区分大小写的正则匹配 1234567location ^~ /images/ &#123;root html/file;&#125;location ~ /images/aa &#123;root html/lfile;&#125; 请求url： http://192.168.60.106/images/aa/test.jpg，匹配结果：^~/images/ 8、严格精准匹配和正则匹配 123456789location / &#123;root html;index index.html index.htm;denv all;&#125;location ~ \.html$&#123;allow all;&#125; 严格精准匹配，如果被严格精准匹配到了，则不会继续搜索正则匹配 如果http://192.168.60.106,这个就严格精准匹配到了 /,则不会继续匹配 ~ .html$ 如果：http://192.168.60.106/index.html，则会被/ 匹配到，但是不是严格精准匹配，则会继续搜索正则匹配 9、正则匹配规则 都是正则uri的情况下，匹配是按照编辑顺序的 1234567location ~ \.html$ &#123;allow all;&#125;location ~ ^/prefix/.*\.html$&#123;deny all;&#125; 请求URL：http://192.168.60.106/prefix/index.html，会优先匹配前面定义的location。 10、@开头的uri 1234error_page 404 = @fallback;location @fallback &#123;proxy_pass http://www.baidu.com;&#125; @开头的，如果请求的 URI 存在，则本 nginx 返回对应的页面；如果不存在，则把请求代理到baidu.com 上去做个弥补，其实就是做了一个容错，把找不到的url全部转发到fallback的反向代理服务器去。 最后总结： 先判断精准命中，如果命中，立即返回结果并结束解析过程 判断普通命中，如果有多个命中，记录下来最长的命中结果 3、如果是^~开头的命中，则不会继续搜索正则命中，但是会继续搜索一般命中 继续判断正则表达式的解析结果，按配置里的正则表达式顺序为准，由上到下开始匹配，一旦匹配成功立刻返回结果，并结束解析过程。 延伸分析：a. 普通命中：顺序无所谓，是因为按命中长短来确定的 b. 正则命中：顺序有所谓，因为是从前往后命中的 原文链接：https://blog.csdn.net/luoyang_java/article/details/83507193]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ集群部署方式总结]]></title>
    <url>%2F2019%2F09%2F10%2FRocketMQ%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[RocketMQ集群部署方式总结1 RocketMQ网络部署图RocketMQ网络部署图如下图所示： 1.1 RocketMQ网络部署特点： NameServer是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。 Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave的对应关系通过指定相同的BrokerName，不同的BrokerId来定义，BrokerId为0表示Master，非0表示Slave。Master也可以部署多个。每个Broker与NameServer集群中的所有节点建立长连接，定时注册Topic信息到所有 NameServer。 Producer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。 Consumer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，订阅规则由Broker配置决定。 2 NameServer集群NameServer集群如下： NameServer集群 IP地址 NameServer-1 192.168.1.101 NameServer-2 192.168.1.102 分别启动 12nohup sh mqnamesrv &amp;tail -f -n 500 $ROCKETMQ_HOME/logs/rocketmqlogs/namesrv.log 3 RocketMQ配置文件众所周知，RocketMQ有多种集群部署方式，它们的配置文件也是分开的，如下： 12345678910[root@rocketmq01 conf]# ls -rlt /usr/local/rocketmq/conftotal 32-rw-r--r-- 1 root root 949 Sep 19 2017 broker.confdrwxr-xr-x 2 root root 60 Sep 19 2017 2m-noslavedrwxr-xr-x 2 root root 118 Sep 19 2017 2m-2s-syncdrwxr-xr-x 2 root root 118 Sep 19 2017 2m-2s-async-rw-r--r-- 1 root root 3720 Apr 21 07:56 logback_namesrv.xml-rw-r--r-- 1 root root 3718 Apr 21 07:56 logback_filtersrv.xml-rw-r--r-- 1 root root 15146 Apr 21 07:56 logback_broker.xml-rw-r--r-- 1 root root 3789 Apr 21 07:56 logback_tools.xml 说明： 2m-noslave： 多Master模式 2m-2s-sync： 多Master多Slave模式，同步双写 2m-2s-async：多Master多Slave模式，异步复制 RocketMQ默认提供的配置文件都是最基本的，很多配置都是默认值，在生产环境中我们需要根据实际情况进行修改。样例配置如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#所属集群名字brokerClusterName=rocketmq-cluster#broker名字，注意此处不同的配置文件填写的不一样brokerName=broker-a|broker-b#0表示Master，&gt;0表示SlavebrokerId=0#nameServer地址，分号分割namesrvAddr=192.168.1.101:9876;192.168.1.102:9876#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker 对外服务的监听端口listenPort=10911#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径storePathRootDir=/usr/local/alibaba-rocketmq/store#commitLog 存储路径storePathCommitLog=/usr/local/alibaba-rocketmq/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/alibaba-rocketmq/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/alibaba-rocketmq/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/alibaba-rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/alibaba-rocketmq/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=ASYNC_MASTER#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=ASYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128 4 Broker集群部署Broker集群部署有几种不同的方式。这里的Slave不可写，但可读，类似于MySQL的主备方式。 4.1 单个Master这种方式风险较大，一旦Broker重启或者宕机时，会导致整个服务不可用，不建议线上环境使用 4.2 多Master模式一个集群无Slave，全是Master，例如2个Master或者3个Master。 brokerName brokerId brokerRole IP地址 broker-a 0 ASYNC_MASTER 192.168.1.101 broker-b 0 ASYNC_MASTER 192.168.1.102 优点：配置简单，单个Master宕机或重启维护对应用无影响，在磁盘配置为RAID10时，即使机器宕机不可恢复情况下，由于RAID10磁盘非常可靠，消息也不会丢失（异步刷盘丢失少量消息，同步刷盘一条不丢）。性能最高。 缺点：单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受到影响。 启动步骤：第一步：先启动NameServer集群 第二步：在192.168.1.101，启动第一个Master 12nohup sh mqbroker -c $ROCKETMQ_HOME/conf/2m-noslave/broker-a.properties &gt;/dev/null 2&gt;&amp;1 &amp;tail -f -n 500 $ROCKETMQ_HOME/logs/rocketmqlogs/broker.log 第三步：在192.168.1.102，启动第二个Master 12nohup sh mqbroker -c $ROCKETMQ_HOME/conf/2m-noslave/broker-b.properties &gt;/dev/null 2&gt;&amp;1 &amp;tail -f -n 500 $ROCKETMQ_HOME/logs/rocketmqlogs/broker.log 4.3 多Master多Slave模式，异步复制每个Master配置一个Slave，有多对Master-Slave，HA采用异步复制方式，主备有短暂消息延迟，毫秒级。 brokerName brokerId brokerRole IP地址 broker-a 0 ASYNC_MASTER 192.168.1.101 broker-a 1 SLAVE 192.168.1.102 broker-b 0 ASYNC_MASTER 192.168.1.103 broker-b 1 SLAVE 192.168.1.104 优点：即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，因为Master 宕机后，消费者仍然可以从Slave消费，此过程对应用透明。不需要人工干预。性能同多 Master 模式几乎一样。 缺点：Master宕机，磁盘损坏情况，会丢失少量消息。 启动步骤第一步：先启动NameServer集群 第二步：在192.168.1.101，启动第一个Master 12nohup sh mqbroker -c $ROCKETMQ_HOME/conf/2m-2s-async/broker-a.properties &gt;$ROCKETMQ_HOME/log/mq.log &gt;/dev/null 2&gt;&amp;1 &amp;tail -f -n 500 $ROCKETMQ_HOME/logs/rocketmqlogs/broker.log 第三步：在192.168.1.102，启动第一个Slave 12nohup sh mqbroker -c $ROCKETMQ_HOME/conf/2m-2s-async/broker-a-s.properties &gt;$ROCKETMQ_HOME/log/mq.log &gt;/dev/null 2&gt;&amp;1 &amp;tail -f -n 500 $ROCKETMQ_HOME/logs/rocketmqlogs/broker.log 第四步：在192.168.1.103，启动第二个Master 12nohup sh mqbroker -c $ROCKETMQ_HOME/conf/2m-2s-async/broker-b.properties &gt;$ROCKETMQ_HOME/log/mq.log &gt;/dev/null 2&gt;&amp;1 &amp;tail -f -n 500 $ROCKETMQ_HOME/logs/rocketmqlogs/broker.log 第五步：在机器 192.168.1.104，启动第二个Slave 1nohup sh mqbroker -c $ROCKETMQ_HOME/conf/2m-2s-async/broker-b-s.properties &gt;$ROCKETMQ_HOME/log/mq.log &gt;/dev/null 2&gt;&amp;1 &amp; 4.4 多Master多Slave模式，同步双写每个Master配置一个Slave，有多对Master-Slave，HA采用同步双写方式，主备都写成功，向应用才返回成功。 brokerName brokerId brokerRole IP地址 broker-a 0 SYNC_MASTER 192.168.1.101 broker-a 1 SLAVE 192.168.1.102 broker-b 0 SYNC_MASTER 192.168.1.103 broker-b 1 SLAVE 192.168.1.104 优点：数据与服务都无单点，Master宕机情况下，消息无延迟，服务可用性与数据可用性都非常高。 缺点：性能比异步复制模式略低，大约低10%左右，发送单个消息的RT会略高。目前主宕机后，备机不能自动切换为主机，后续会支持自动切换功能。 启动步骤：第一步：先启动NameServer集群 第二步：在192.168.1.101，启动第一个Master 12nohup sh mqbroker -c $ROCKETMQ_HOME/conf/2m-2s-sync/broker-a.properties &gt;$ROCKETMQ_HOME/log/mq.log &gt;/dev/null 2&gt;&amp;1 &amp;tail -f -n 500 $ROCKETMQ_HOME/logs/rocketmqlogs/broker.log 第三步：在192.168.1.102，启动第一个Slave 12nohup sh mqbroker -c $ROCKETMQ_HOME/conf/2m-2s-sync/broker-a-s.properties &gt;$ROCKETMQ_HOME/log/mq.log &gt;/dev/null 2&gt;&amp;1 &amp;tail -f -n 500 $ROCKETMQ_HOME/logs/rocketmqlogs/broker.log 第四步：在192.168.1.103，启动第二个Master 12nohup sh mqbroker -c $ROCKETMQ_HOME/conf/2m-2s-sync/broker-b.properties &gt;$ROCKETMQ_HOME/log/mq.log &gt;/dev/null 2&gt;&amp;1 &amp;tail -f -n 500 $ROCKETMQ_HOME/logs/rocketmqlogs/broker.log 第五步：在192.168.1.104，启动第二个Slave 12nohup sh mqbroker -c $ROCKETMQ_HOME/conf/2m-2s-sync/broker-b-s.properties &gt;$ROCKETMQ_HOME/log/mq.log &gt;/dev/null 2&gt;&amp;1 &amp;tail -f -n 500 $ROCKETMQ_HOME/logs/rocketmqlogs/broker.log 注意事项：以上Broker与Slave配对是通过指定相同的brokerName参数来配对，Master的 BrokerId必须是0，Slave的BrokerId必须是大于0的数。另外一个Master下面可以挂载多个Slave，同一Master下的多个Slave 通过指定不同的BrokerId来区分。 4.5 总结1、异步复制和同步双写总结 2、集群方式对比 集群方式 运维特点 消息可靠性(master宕机情况) 服务可用性(master宕机情况) 其他特点 备注 单Master 结构简单，扩容方便，机器要求低 同步刷盘消息一条都不会丢 整体可用，未被消费的消息无法取得，影响实时性 性能最高 多Master 异步有毫秒级丢失，同步双写不丢失 差评，主备不能自动切换，且备机只能读不能写，会造成服务整体不可写 不考虑，除非自己提供主从切换的方案 Master-Slave(异步复制) 结构复杂，扩容方便 故障时会丢失消息 整体可用，实时性影响毫秒级别 该组服务只能读不能写 性能很高 适合消息可靠性中等，实时性中等的要求 Master-Slave(同步双写) 结构复杂，扩容方便 不丢消息 整体可用，不影响实时性，该组服务只能读不能写 性能比异步低10%，所以实时性也并不比异步方式太高 适合消息可靠性略高，实时性中等、性能要求不高的需求 5 高可用演练场景RocketMQ高可用演练场景 项目 发送消息 发送消息过程中 接收消费消息 停用一个namesrv 不影响通信 不影响通信 不影响通信 停用全部namesrv 影响通信 不影响通信 影响通信，启动任意的namesrv可恢复 停用单个master broker 不影响通信 不影响通信 不影响通信 停用全部master broker 影响通信 影响通信，无法恢复 影响通信 停用一个slave broker 不影响通信 不影响通信 不影响通信 停用全部slave broker 不影响通信 影响通信，数秒恢复 不影响通信，数秒恢复]]></content>
      <categories>
        <category>运维基础</category>
      </categories>
      <tags>
        <tag>运维基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ多Master集群模式部署]]></title>
    <url>%2F2019%2F09%2F10%2FRocketMQ%E5%A4%9AMaster%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[RocketMQ多Master集群模式部署1 概述多Master集群模式指的是一个集群无Slave，全是Maste。 优点：配置简单，单个Master宕机或重启维护对应用无影响，在磁盘配置为RAID10 时，即使机器宕机不可恢复情由于RAID10 磁盘非常可靠，消息也不会丢（异步刷盘丢失少量消息，同步刷盘一条不丢）。性能最高。 缺点：单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受到受到影响。 1 环境信息 序号 IP地址 主机名 用户 角色 模式 1 192.168.3.41 rocketmq01 root nameServer1,brokerServer1 Master1 2 192.168.3.42 rocketmq02 root nameServer2,brokerServer2 Master2 2 安装前准备工作2.1 /etc/hosts配置123456cat &gt;&gt; /etc/hosts &lt;&lt; EOF192.168.3.41 rocketmq-nameserver1192.168.3.41 rocketmq-master1192.168.3.42 rocketmq-nameserver2192.168.3.42 rocketmq-master2EOF 2.2 Java环境部署在/usr/目录下创建java目录 12mkdir /usr/local/java cd /usr/local/java 下载jdk,然后解压 12curl -O http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/jdk-8u171-linux-x64.tar.gztar -zxvf jdk-8u171-linux-x64.tar.gz -C /usr/local/java 设置环境变量 12345678cat &gt;&gt; /etc/profile &lt;&lt; EOF#set java environment JAVA_HOME=/usr/local/java/jdk1.8.0_171JRE_HOME=/usr/java/jdk1.8.0_171/jre CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin export JAVA_HOME JRE_HOME CLASS_PATH PATHEOF 让修改生效 1source /etc/profile 验证JDK有效性 1234java -version java version &quot;1.8.0_171&quot;Java(TM) SE Runtime Environment (build 1.8.0_171-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode) 3 安装rocketmq3.1 上传解压【两台机器操作】12345678mkdir /usr/local/rocketmq4.2cd /usr/local/rocketmq4.2curl -O https://www.apache.org/dyn/closer.cgi?path=rocketmq/4.2.0/rocketmq-all-4.2.0-bin-release.zipunzip rocketmq-all-4.2.0-bin-release.ziprm -f rocketmq-all-4.2.0-bin-release.zipcd /usr/localln -s rocketmq4.2 rocketmqll /usr/local 3.2 创建存储路径【两台机器操作】1234mkdir /usr/local/rocketmq/storemkdir /usr/local/rocketmq/store/commitlogmkdir /usr/local/rocketmq/store/consumequeuemkdir /usr/local/rocketmq/store/index 3.3 RocketMQ配置文件配置【两台机器】12vim /usr/local/rocketmq/conf/2m-noslave/broker-a.propertiesvim /usr/local/rocketmq/conf/2m-noslave/broker-a.properties broker-a.properties配置文件内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#所属集群名字brokerClusterName=rocketmq-cluster#broker名字，注意此处不同的配置文件填写的不一样brokerName=broker-a#0 表示 Master，&gt;0 表示 SlavebrokerId=0#nameServer地址，分号分割namesrvAddr=192.168.3.41:9876;192.168.3.42:9876#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker 对外服务的监听端口listenPort=10911#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径storePathRootDir=/usr/local/alibaba-rocketmq/store#commitLog 存储路径storePathCommitLog=/usr/local/alibaba-rocketmq/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/alibaba-rocketmq/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/alibaba-rocketmq/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/alibaba-rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/alibaba-rocketmq/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=ASYNC_MASTER#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=ASYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128 broker-b.properties配置文件内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#所属集群名字brokerClusterName=rocketmq-cluster#broker名字，注意此处不同的配置文件填写的不一样brokerName=broker-b#0 表示 Master，&gt;0 表示 SlavebrokerId=0#nameServer地址，分号分割namesrvAddr=192.168.3.41:9876;192.168.3.42:9876#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker 对外服务的监听端口listenPort=10911#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径storePathRootDir=/usr/local/alibaba-rocketmq/store#commitLog 存储路径storePathCommitLog=/usr/local/alibaba-rocketmq/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/alibaba-rocketmq/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/alibaba-rocketmq/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/alibaba-rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/alibaba-rocketmq/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=ASYNC_MASTER#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=ASYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128 3.4 修改日志配置文件【两台机器】12mkdir -p /usr/local/rocketmq/logscd /usr/local/rocketmq/conf &amp;&amp; sed -i &apos;s#$&#123;user.home&#125;#/usr/local/rocketmq#g&apos; *.xml 3.5 修改启动脚本参数【两台机器】1234567cd /usr/local/rocketmq/binvim runbroker.sh#将JAVA_OPT修改为JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms1g -Xmx1g -Xmn512m -XX:PermSize=128m -XX:MaxPermSize=320m&quot;vim runserver.sh#将JAVA_OPT修改为JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms1g -Xmx1g -Xmn512m -XX:PermSize=128m -XX:MaxPermSize=320m&quot; 3.6 启动NameServer【两台机器】12cd /usr/local/rocketmq/binnohup sh mqnamesrv &amp; 3.7 启动BrokerServer A【192.168.3.41】123456cd /usr/local/rocketmq/binnohup sh mqbroker -c /usr/local/rocketmq/conf/2m-noslave/broker-a.properties &gt; /dev/null 2&gt;&amp;1 &amp;netstat -ntlpjpstail -500f /usr/local/rocketmq/logs/rocketmqlogs/broker.logtail -500f /usr/local/rocketmq/logs/rocketmqlogs/namesrv.log 3.8 启动BrokerServer B【192.168.3.42】123456cd /usr/local/rocketmq/binnohup sh mqbroker -c /usr/local/rocketmq/conf/2m-noslave/broker-b.properties &gt; /dev/null 2&gt;&amp;1 &amp;netstat -ntlpjpstail -500f /usr/local/rocketmq/logs/rocketmqlogs/broker.logtail -500f /usr/local/rocketmq/logs/rocketmqlogs/namesrv.log]]></content>
      <categories>
        <category>运维基础</category>
      </categories>
      <tags>
        <tag>运维基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件MQ概述]]></title>
    <url>%2F2019%2F09%2F10%2F%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6MQ%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[消息中间件MQ概述1 消息中间件介绍一般，我们认为消息中间件是指支持与保障分布式应用程序之间同步/异步收发消息的中间件。消息是分布应用之间进行数据交换的基本信息单位，分布式应用程序之间的通信接口由消息中间件提供。其中，异步方式指消息发送方在发送消息时不必知道接收方的状态，更无需等待接收方的回复，而接收方在收到消息时不必知道发送方的目前状态，更无需进行同步的消息处理，它们之间的连接完全是松耦合的，通信是非阻塞的，这种异步通信方式是由消息中间件中的消息队列及其服务机制保障的。一般地，实时性要求较高的业务采用同步方式处理，实时性要求不高的业务采用异步方式进行处理。 消息中间件已广泛应用于各分布式应用系统，消息中间件开始向发布/订阅架构转变，并成为企业应用集成中间件的一种核心机制，而基于发布/订阅架构的消息中间件通常称为发布/订阅消息中间件或消息代理。目前比较典型的消息中间件包括IBM WebSphere MQSeries、阿里的RocketMQ和Kafka等。 消息中间件是在消息的传输过程中保存信息的容器。消息中间件再将消息从它的源中继到它的目标时充当中间人的作用。队列的主要目的是提供路由并保证消息的传递；如果发送消息时接收者不可用，消息队列会保留消息，直到可以成功地传递它为止，当然，消息队列保存消息也是有期限的。 2 消息中间件的特点2.1 采用异步处理模式消息发送者可以发送一个消息而无须等待响应。消息发送者将消息发送到一条虚拟的通道（主题或队列）上，消息接收者则订阅或是监听该通道。一条信息可能最终转发给一个或多个消息接收者，这些接收者都无需对消息发送者做出同步回应。整个过程都是异步的。 2.2 应用程序和应用程序调用关系为松耦合关系主要体现在如下两点：（1）发送者和接受者不必了解对方、只需要确认消息（2）发送者和接受者不必同时在线比如在线交易系统为了保证数据的最终一致，在支付系统处理完成后会把支付结果放到消息中间件里通知订单系统修改订单支付状态。两个系统通过消息中间件解耦。 3 消息传递服务模型消息传递服务模型如下图所示 4 消息中间件的传输模式4.1 点对点模型点对点模型用于消息生产者和消息消费者之间点到点的通信。消息生产者将消息发送到由某个名字标识的特定消费者。这个名字实际上对于消费服务中的一个队列（Queue），在消息传递给消费者之前它被存储在这个队列中。队列消息可以放在内存中也可以是持久的，以保证在消息服务出现故障时仍然能够传递消息。 传统的点对点消息中间件通常由消息队列服务、消息传递服务、消息队列和消息应用程序接口API组成，其典型的结构如下图所示。 特点：（1）每个消息只用一个消费者（2）发送者和接受者没有时间依赖（3）接受者确认消息接受和处理成功示意图如下所示： 4.2 发布-订阅模型（Pub/Sub）发布者/订阅者模型支持向一个特定的消息主题生产消息。0或多个订阅者可能对接收来自特定消息主题的消息感兴趣。在这种模型下，发布者和订阅者彼此不知道对方。这种模式就好比是匿名公告板。这种模式被概况为：多个消费者可以获得消息，在发布者和订阅者之间存在时间依赖性。发布者需要建立一个订阅（subscription），以便能够消费者订阅。订阅者必须保持持续的活动状态及接收消息，除非订阅者建立了持久的订阅。在这种情况下，在订阅者未连接时发布的消息将在订阅者重新连接时重新发布。如下图所示： 发布/订阅模型特性（1）每个消息可以有多个订阅者（2）客户端只有订阅后才能接收到消息（3）持久订阅和非持久订阅 注意：（1）发布者和订阅者有时间依赖接受者和发布者只有建立订阅关系才能收到消息（2）持久订阅订阅关系建立后，消息就不会消失，不管订阅者是否都在线（3）非持久订阅订阅者为了接受消息，必须一直在线。当只有一个订阅者时约等于点对点模式 5 消息中间件应用场景5.1 用户注册异步处理网站用户注册，注册成功后会过一会发送邮件确认或者短息 5.2 日志分析使用把日志进行集中收集，用于计算PV、用户行为分析 5.3 数据复制（1）将数据从源头复制到多个目的地，一般是要求顺序或者保证因果序列（2）用于跨机房数据传输、搜索、离线数据计算等。 5.4 延迟消息发送和暂存（1）把消息中间件当成可靠的消息暂存地（2）定时进行消息投递，比如模拟用户秒杀访问，进行系统性能压测 5.5 消息广播（1）缓存数据同步更新（2）往应用推送数据比如更新本地缓存： 6 消息中间件消息模型分类6.1 push推消息模型消息生产者将消息发送给消息传递服务，消息传递服务又将消息推给消息消费者。 6.2 pull拉消息模型消费者请求消息服务接受消息，消息生产者从消息中间件拉该消息。 6.3 两种类型的区别push推消息和pull拉消息的区别]]></content>
      <categories>
        <category>python高级篇</category>
      </categories>
      <tags>
        <tag>python高级篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件MQ详解]]></title>
    <url>%2F2019%2F09%2F10%2F%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6MQ%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[消息中间件详解 1 消息中间件概述消息队列已经逐渐成为企业IT系统内部通信的核心手段。它具有低耦合、可靠投递、广播、流量控制、最终一致性等一系列功能，成为异步RPC的主要手段之一。当今天市面上很多主流的消息中间件，如老牌的ActiveMQ、RabbitMQ，炙手可热的Kafka，阿里巴巴自主开发RocketMQ等。 2 消息中间件的组成2.1 Broker消息服务器，作为server提供消息核心服务 2.2 Producer消息生产者，业务的发起方，负责生产消息传输给broker， 2.3 Consumer消息消费者，业务的处理方，负责从broker获取消息并进行业务逻辑处理 2.4 Topic主题，发布订阅模式下的消息统一汇集地，不同生产者向topic发送消息，由MQ服务器分发到不同的订阅者，实现消息的广播 2.5 Queue队列，PTP模式下，特定生产者向特定queue发送消息，消费者订阅特定的queue完成指定消息的接收 2.6 Message消息体，根据不同通信协议定义的固定格式进行编码的数据包，来封装业务数据，实现消息的传输 3 消息中间件模式分类3.1 点对点PTP点对点:使用queue作为通信载体 说明:消息生产者生产消息发送到queue中，然后消息消费从queue中取出并且消费消息。 消息被消费以后，queue中不再存储，所以消息消费者不可能消费到已经被消费的消息。queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 3.2 发布/订阅Pub/Sub发布订阅（广播）：使用topic作为通信载体 说明： 消息生产者(发布)将消息发布到topic中，同时有多个消息消费者(订阅)消费该消息。和点对点方式 不同，发布到topic的消息会被所有订阅者消费。 queue实现了负载均衡，将producer生产的消息发送到消息队列中，由多个消费者消费。但一个消息只能被一个消费者接受，当没有消费者可用时，这个消息会被保存直到有一个可用的消费者。 topic实现了发布和订阅，当你发布一个消息，所有订阅这个topic的服务都能得到这个消息，所以人1到N个订阅者都能得到一个消息的拷贝。 4 消息中间件的优势4.1 系统解耦交互系统之间没有直接的调用关系，只是通过消息传输，故系统侵入性不强，耦合度低。 4.2 提高系统响应时间例如原来的一套逻辑，完成支付可能涉及先修改订单状态、计算会员积分、通知物流配送几个逻辑才能完成;通过MQ架构设计，就可将紧急重要(需要立刻响应)的业务放到该调用方法中，响应要求不高的使用消息 4.3 为大数据处理架构提供服务通过消息作为整合，大数据的背景下，消息队列还与实时处理架构整合，为数据处理提供性能支持。 4.4 Java消息服务——JMSjava消息服务(java message service，JMS)应用程序接口是一个java平台中关于面向消息中间件(MOM)的API,用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。 JMS中的P2P和pub/sub消息模式:点对点(point to point,queue)与发布订阅(publish/subscribe,topic)最初是由JMS定义的。这两种模式主要区别或解决的问题就是发送队列的消息能否重复消息(多订阅)。 5 消息中间件应用场景5.1 异步通信有些业务不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列放入多个消息就放多少，然后在需要的时候再去处理它们。 5.2 解耦降低工程间的强依赖程度，针对异构系统进行适配。在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。通过消息系统在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口，当应用发生变化时，可以独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 5.3 冗余有些情况下，处理数据的过程会失败，除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入—获取—–删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 5.4 扩展性因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。不需要改变代码、不需要调节参数。便于分布式扩容。 5.5 过载保护在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量无法提取预知；如果以为了能处理这类瞬间峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 5.6 可恢复性系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 5.7 顺序保证在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。 5.8 缓冲在任何重要的系统中，都会有需要不同的处理时间的元素。消息队列通过一个缓冲层来帮助任务最高效率的执行，该缓冲有助于控制和优化数据流经过系统的速度。以调节系统响应时间。 5.9 数据流处理分布式系统产生的海量数据流，如：业务日志、监控数据、用户行为等，针对这些数据流进行实时或批量采集汇总，然后进行大数据分析是当前互联网的必备技术，通过消息队列完成此类数据收集是最好的选择。 6 消息中间件常用协议6.1 AMQP协议AMQP即Advanced Message Queuing Protocol,一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同开发语言等条件的限制。 优点:可靠、通用 6.2 MQTT协议MQTT（Message Queuing Telemetry Transport，消息队列遥测传输）是IBM开发的一个即时通讯协议，有可能成为物联网的重要组成部分。该协议支持所有平台，几乎可以把所有联网物品和外部连接起来，被用来当做传感器和致动器（比如通过Twitter让房屋联网）的通信协议。优点：格式简洁、占用带宽小、移动端通信、PUSH、嵌入式系统 6.3 STOMP协议STOMP（Streaming Text Orientated Message Protocol）是流文本定向消息协议，是一种为MOM(Message Oriented Middleware，面向消息的中间件)设计的简单文本协议。STOMP提供一个可互操作的连接格式，允许客户端与任意STOMP消息代理（Broker）进行交互。优点：命令模式（非topic\queue模式） 6.4 XMPP协议XMPP（可扩展消息处理现场协议，Extensible Messaging and Presence Protocol）是基于可扩展标记语言（XML）的协议，多用于即时消息（IM）以及在线现场探测。适用于服务器之间的准即时操作。核心是基于XML流传输，这个协议可能最终允许因特网用户向因特网上的其他任何人发送即时消息，即使其操作系统和浏览器不同。优点：通用公开、兼容性强、可扩展、安全性高，但XML编码格式占用带宽大 6.5 其他基于TCP/IP自定义的协议有些特殊框架（如：redis、kafka、zeroMq等）根据自身需要未严格遵循MQ规范，而是基于TCP\IP自行封装了一套协议，通过网络socket接口进行传输，实现了MQ的功能。 7 常见消息中间件MQ介绍7.1 RocketMQ阿里系下开源的一款分布式、队列模式的消息中间件，原名Metaq，3.0版本名称改为RocketMQ，是阿里参照kafka设计思想使用java实现的一套mq。同时将阿里系内部多款mq产品(Notify、metaq)进行整合，只维护核心功能，去除了所有其他运行时依赖，保证核心功能最简化，在此基础上配合阿里上述其他开源产品实现不同场景下mq的架构，目前主要多用于订单交易系统。 具有以下特点： 能够保证严格的消息顺序 提供针对消息的过滤功能 提供丰富的消息拉取模式 高效的订阅者水平扩展能力 实时的消息订阅机制 亿级消息堆积能力 官方提供了一些不同于kafka的对比差异：https://rocketmq.apache.org/docs/motivation/ 7.2 RabbitMQ使用Erlang编写的一个开源的消息队列，本身支持很多的协议：AMQP，XMPP, SMTP,STOMP，也正是如此，使的它变的非常重量级，更适合于企业级的开发。同时实现了Broker架构，核心思想是生产者不会将消息直接发送给队列，消息在发送给客户端时先在中心队列排队。对路由(Routing)，负载均衡(Load balance)、数据持久化都有很好的支持。多用于进行企业级的ESB整合。 7.3 ActiveMQApache下的一个子项目。使用Java完全支持JMS1.1和J2EE 1.4规范的 JMS Provider实现，少量代码就可以高效地实现高级应用场景。可插拔的传输协议支持，比如：in-VM, TCP, SSL, NIO, UDP, multicast, JGroups and JXTA transports。RabbitMQ、ZeroMQ、ActiveMQ均支持常用的多种语言客户端 C++、Java、.Net,、Python、 Php、 Ruby等。 7.4 Redis使用C语言开发的一个Key-Value的NoSQL数据库，开发维护很活跃，虽然它是一个Key-Value数据库存储系统，但它本身支持MQ功能，所以完全可以当做一个轻量级的队列服务来使用。对于RabbitMQ和Redis的入队和出队操作，各执行100万次，每10万次记录一次执行时间。测试数据分为128Bytes、512Bytes、1K和10K四个不同大小的数据。实验表明：入队时，当数据比较小时Redis的性能要高于RabbitMQ，而如果数据大小超过了10K，Redis则慢的无法忍受；出队时，无论数据大小，Redis都表现出非常好的性能，而RabbitMQ的出队性能则远低于Redis。 7.5 KafkaApache下的一个子项目，使用scala实现的一个高性能分布式Publish/Subscribe消息队列系统，具有以下特性： 快速持久化：通过磁盘顺序读写与零拷贝机制，可以在O(1)的系统开销下进行消息持久化；高吞吐：在一台普通的服务器上既可以达到10W/s的吞吐速率；高堆积：支持topic下消费者较长时间离线，消息堆积量大；完全的分布式系统：Broker、Producer、Consumer都原生自动支持分布式，依赖zookeeper自动实现复杂均衡；支持Hadoop数据并行加载：对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。 7.6 ZeroMQ号称最快的消息队列系统，专门为高吞吐量/低延迟的场景开发，在金融界的应用中经常使用，偏重于实时数据通信场景。ZMQ能够实现RabbitMQ不擅长的高级/复杂的队列，但是开发人员需要自己组合多种技术框架，开发成本高。因此ZeroMQ具有一个独特的非中间件的模式，更像一个socket library，你不需要安装和运行一个消息服务器或中间件，因为你的应用程序本身就是使用ZeroMQ API完成逻辑服务的角色。但是ZeroMQ仅提供非持久性的队列，如果down机，数据将会丢失。如：Twitter的Storm中使用ZeroMQ作为数据流的传输。 ZeroMQ套接字是与传输层无关的：ZeroMQ套接字对所有传输层协议定义了统一的API接口。默认支持 进程内(inproc) ，进程间(IPC) ，多播，TCP协议，在不同的协议之间切换只要简单的改变连接字符串的前缀。可以在任何时候以最小的代价从进程间的本地通信切换到分布式下的TCP通信。ZeroMQ在背后处理连接建立，断开和重连逻辑。 特性： 无锁的队列模型：对于跨线程间的交互（用户端和session）之间的数据交换通道pipe，采用无锁的队列算法CAS；在pipe的两端注册有异步事件，在读或者写消息到pipe的时，会自动触发读写事件。批量处理的算法：对于批量的消息，进行了适应性的优化，可以批量的接收和发送消息。多核下的线程绑定，无须CPU切换：区别于传统的多线程并发模式，信号量或者临界区，zeroMQ充分利用多核的优势，每个核绑定运行一个工作者线程，避免多线程之间的CPU切换开销。 文章转载请注明出处www.leexide.com]]></content>
      <categories>
        <category>python高级篇</category>
      </categories>
      <tags>
        <tag>python高级篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程与线程二]]></title>
    <url>%2F2019%2F09%2F08%2F%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[进程、线程、协程本节内容 操作系统发展史介绍 进程、与线程区别 python GIL全局解释器锁 线程 语法 join 线程锁之Lock\Rlock\信号量 将线程变为守护进程 Event事件 queue队列 生产者消费者模型 Queue队列 开发一个线程池 进程 语法 进程间通讯 进程池 操作系统发展史手工操作（无操作系统）1946年第一台计算机诞生–20世纪50年代中期，还未出现操作系统，计算机工作采用手工操作方式。 手工操作程序员将对应于程序和数据的已穿孔的纸带（或卡片）装入输入机，然后启动输入机把程序和数据输入计算机内存，接着通过控制台开关启动程序针对数据运行；计算完毕，打印机输出计算结果；用户取走结果并卸下纸带（或卡片）后，才让下一个用户上机。 手工操作方式两个特点：（1）用户独占全机。不会出现因资源已被其他用户占用而等待的现象，但资源的利用率低。（2）CPU 等待手工操作。CPU的利用不充分。  20世纪50年代后期，出现人机矛盾：手工操作的慢速度和计算机的高速度之间形成了尖锐矛盾，手工操作方式已严重损害了系统资源的利用率（使资源利用率降为百分之几，甚至更低），不能容忍。唯一的解决办法：只有摆脱人的手工操作，实现作业的自动过渡。这样就出现了成批处理。 批处理系统批处理系统：加载在计算机上的一个系统软件，在它的控制下，计算机能够自动地、成批地处理一个或多个用户的作业（这作业包括程序、数据和命令）。 联机批处理系统首先出现的是联机批处理系统，即作业的输入/输出由CPU来处理。主机与输入机之间增加一个存储设备——磁带，在运行于主机上的监督程序的自动控制下，计算机可自动完成：成批地把输入机上的用户作业读入磁带，依次把磁带上的用户作业读入主机内存并执行并把计算结果向输出机输出。完成了上一批作业后，监督程序又从输入机上输入另一批作业，保存在磁带上，并按上述步骤重复处理。 监督程序不停地处理各个作业，从而实现了作业到作业的自动转接，减少了作业建立时间和手工操作时间，有效克服了人机矛盾，提高了计算机的利用率。 但是，在作业输入和结果输出时，主机的高速CPU仍处于空闲状态，等待慢速的输入/输出设备完成工作： 主机处于“忙等”状态。 脱机批处理系统为克服与缓解高速主机与慢速外设的矛盾，提高CPU的利用率，又引入了脱机批处理系统，即输入/输出脱离主机控制。这种方式的显著特征是：增加一台不与主机直接相连而专门用于与输入/输出设备打交道的卫星机。其功能是：（1）从输入机上读取用户作业并放到输入磁带上。（2）从输出磁带上读取执行结果并传给输出机。 这样，主机不是直接与慢速的输入/输出设备打交道，而是与速度相对较快的磁带机发生关系，有效缓解了主机与设备的矛盾。主机与卫星机可并行工作，二者分工明确，可以充分发挥主机的高速计算能力。 脱机批处理系统:20世纪60年代应用十分广泛，它极大缓解了人机矛盾及主机与外设的矛盾。IBM-7090/7094：配备的监督程序就是脱机批处理系统，是现代操作系统的原型。 不足：每次主机内存中仅存放一道作业，每当它运行期间发出输入/输出（I/O）请求后，高速的CPU便处于等待低速的I/O完成状态，致使CPU空闲。 为改善CPU的利用率，又引入了多道程序系统。 多道程序系统多道程序设计技术 所谓多道程序设计技术，就是指允许多个程序同时进入内存并运行。即同时把多个程序放入内存，并允许它们交替在CPU中运行，它们共享系统中的各种硬、软件资源。当一道程序因I/O请求而暂停运行时，CPU便立即转去运行另一道程序。 单道程序的运行过程：在A程序计算时，I/O空闲， A程序I/O操作时，CPU空闲（B程序也是同样）；必须A工作完成后，B才能进入内存中开始工作，两者是串行的，全部完成共需时间=T1+T2。 多道程序的运行过程：将A、B两道程序同时存放在内存中，它们在系统的控制下，可相互穿插、交替地在CPU上运行：当A程序因请求I/O操作而放弃CPU时，B程序就可占用CPU运行，这样 CPU不再空闲，而正进行A I/O操作的I/O设备也不空闲，显然，CPU和I/O设备都处于“忙”状态，大大提高了资源的利用率，从而也提高了系统的效率，A、B全部完成所需时间&lt;&lt;T1+T2。 多道程序设计技术不仅使CPU得到充分利用，同时改善I/O设备和内存的利用率，从而提高了整个系统的资源利用率和系统吞吐量（单位时间内处理作业（程序）的个数），最终提高了整个系统的效率。 单处理机系统中多道程序运行时的特点：（1）多道：计算机内存中同时存放几道相互独立的程序；（2）宏观上并行：同时进入系统的几道程序都处于运行过程中，即它们先后开始了各自的运行，但都未运行完毕；（3）微观上串行：实际上，各道程序轮流地用CPU，并交替运行。 多道程序系统的出现，标志着操作系统渐趋成熟的阶段，先后出现了作业调度管理、处理机管理、存储器管理、外部设备管理、文件系统管理等功能。 多道批处理系统20世纪60年代中期，在前述的批处理系统中，引入多道程序设计技术后形成多道批处理系统（简称：批处理系统）。它有两个特点：（1）多道：系统内可同时容纳多个作业。这些作业放在外存中，组成一个后备队列，系统按一定的调度原则每次从后备作业队列中选取一个或多个作业进入内存运行，运行作业结束、退出运行和后备作业进入运行均由系统自动实现，从而在系统中形成一个自动转接的、连续的作业流。（2）成批：在系统运行过程中，不允许用户与其作业发生交互作用，即：作业一旦进入系统，用户就不能直接干预其作业的运行。 批处理系统的追求目标：提高系统资源利用率和系统吞吐量，以及作业流程的自动化。 批处理系统的一个重要缺点：不提供人机交互能力，给用户使用计算机带来不便。虽然用户独占全机资源，并且直接控制程序的运行，可以随时了解程序运行情况。但这种工作方式因独占全机造成资源效率极低。 一种新的追求目标：既能保证计算机效率，又能方便用户使用计算机。 20世纪60年代中期，计算机技术和软件技术的发展使这种追求成为可能。 分时系统由于CPU速度不断提高和采用分时技术，一台计算机可同时连接多个用户终端，而每个用户可在自己的终端上联机使用计算机，好象自己独占机器一样。 分时技术：把处理机的运行时间分成很短的时间片，按时间片轮流把处理机分配给各联机作业使用。 若某个作业在分配给它的时间片内不能完成其计算，则该作业暂时中断，把处理机让给另一作业使用，等待下一轮时再继续其运行。由于计算机速度很快，作业运行轮转得很快，给每个用户的印象是，好象他独占了一台计算机。而每个用户可以通过自己的终端向系统发出各种操作控制命令，在充分的人机交互情况下，完成作业的运行。 具有上述特征的计算机系统称为分时系统，它允许多个用户同时联机使用计算机。 特点：（1）多路性。若干个用户同时使用一台计算机。微观上看是各用户轮流使用计算机；宏观上看是各用户并行工作。（2）交互性。用户可根据系统对请求的响应结果，进一步向系统提出新的请求。这种能使用户与系统进行人机对话的工作方式，明显地有别于批处理系统，因而，分时系统又被称为交互式系统。（3）独立性。用户之间可以相互独立操作，互不干扰。系统保证各用户程序运行的完整性，不会发生相互混淆或破坏现象。（4）及时性。系统可对用户的输入及时作出响应。分时系统性能的主要指标之一是响应时间，它是指：从终端发出命令到系统予以应答所需的时间。 分时系统的主要目标：对用户响应的及时性，即不至于用户等待每一个命令的处理时间过长。 分时系统可以同时接纳数十个甚至上百个用户，由于内存空间有限，往往采用对换（又称交换）方式的存储方法。即将未“轮到”的作业放入磁盘，一旦“轮到”，再将其调入内存；而时间片用完后，又将作业存回磁盘（俗称“滚进”、“滚出“法），使同一存储区域轮流为多个用户服务。 多用户分时系统是当今计算机操作系统中最普遍使用的一类操作系统。 实时系统虽然多道批处理系统和分时系统能获得较令人满意的资源利用率和系统响应时间，但却不能满足实时控制与实时信息处理两个应用领域的需求。于是就产生了实时系统，即系统能够及时响应随机发生的外部事件，并在严格的时间范围内完成对该事件的处理。实时系统在一个特定的应用中常作为一种控制设备来使用。 实时系统可分成两类：（1）实时控制系统。当用于飞机飞行、导弹发射等的自动控制时，要求计算机能尽快处理测量系统测得的数据，及时地对飞机或导弹进行控制，或将有关信息通过显示终端提供给决策人员。当用于轧钢、石化等工业生产过程控制时，也要求计算机能及时处理由各类传感器送来的数据，然后控制相应的执行机构。（2）实时信息处理系统。当用于预定飞机票、查询有关航班、航线、票价等事宜时，或当用于银行系统、情报检索系统时，都要求计算机能对终端设备发来的服务请求及时予以正确的回答。此类对响应及时性的要求稍弱于第一类。 实时操作系统的主要特点：（1）及时响应。每一个信息接收、分析处理和发送的过程必须在严格的时间限制内完成。（2）高可靠性。需采取冗余措施，双机系统前后台工作，也包括必要的保密措施等。 进程与线程什么是进程(process)？程序的执行实例称为进程。 每个进程都提供执行程序所需的资源。进程具有虚拟地址空间，可执行代码，系统对象的打开句柄，安全上下文，唯一进程标识符，环境变量，优先级类，最小和最大工作集大小以及至少一个执行线程。每个进程都使用单个线程启动，通常称为主线程，但可以从其任何线程创建其他线程。 程序不能单独运行，只有将程序装载到内存中，系统为它分配资源才能运行，而这种执行的程序就称之进程。程序和进程的区别就在于：程序是指令的集合，它是进程运行的静态描述文本;进程是程序的一次执行活动，属于动态概念。 在多道编程中，我们允许多个程序同时加载到内存中，在操作系统的调试下，可以实现并发地执行。正是这样的设计，大大提高 了cpu的利用率。进程的出现让每个用户感觉到自己独享cpu，因此，进程就是为了在cpu上实现多道编程而提出的。 有了进程为什么还要线程？ 进程有很多优点，提供了多道编程，让我们感觉我们每个人都拥有自己的cpu和其他资源，可以提高计算机的利用率。很多人就不理解了，既然进程这么优秀，为什么还要线程呢？其实，仔细观察就会发现进程还是有很多缺陷的，主要体现在两点上： 进程只能在一个时间干一件事，如果想同时干两件事或多件事，进程就无能为力了。 进程在执行的过程中如果阻塞，例如等待输入，整个进程就会挂起，即使进程中有些工作不依赖于输入的数据，也将无法执行。 例如，我们在使用qq聊天， qq做为一个独立进程如果同一时间只能干一件事，那他如何实现在同一时刻 即能监听键盘输入、又能监听其它人给你发的消息、同时还能把别人发的消息显示在屏幕上呢？你会说，操作系统不是有分时么？但我的亲，分时是指在不同进程间的分时呀， 即操作系统处理一会你的qq任务，又切换到word文档任务上了，每个cpu时间片分给你的qq程序时，你的qq还是只能同时干一件事呀。 再直白一点， 一个操作系统就像是一个工厂，工厂里面有很多个生产车间，不同的车间生产不同的产品，每个车间就相当于一个进程，且你的工厂又穷，供电不足，同一时间只能给一个车间供电，为了能让所有车间都能同时生产，你的工厂的电工只能给不同的车间分时供电，但是轮到你的qq车间时，发现只有一个干活的工人，结果生产效率极低，为了解决这个问题，应该怎么办呢？。。。。没错，你肯定想到了，就是多加几个工人，让几个人工人并行工作，这每个工人，就是线程！ 什么是线程(thread)？线程是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务 线程是执行上下文，它是CPU执行指令流所需的所有信息。假设你正在读一本书，而你现在想休息一下，但是你希望能够从你停下来的确切位置回来并继续阅读。实现这一目标的一种方法是记下页码，行号和字号。因此，阅读书籍的执行环境就是这三个数字。如果你有一个室友，并且她使用相同的技术，她可以在你不使用时拿走这本书，并从她停下的地方继续阅读。然后你可以把它拿回来，并从你所在的地方恢复。线程以相同的方式工作。 CPU正在给你一种错觉，即它同时进行多次计算。它通过在每次计算上花费一点时间来做到这一点。它可以做到这一点，因为它有每个计算的执行上下文。就像您可以与朋友共享一本书一样，许多任务可以共享CPU。在技术层面上，执行上下文（因此是一个线程）由CPU寄存器的值组成。最后：线程与进程不同。线程是执行的上下文，而进程是与计算相关联的一堆资源。一个进程可以有一个或多个线程。澄清：与进程关联的资源包括内存页面（进程中的所有线程具有相同的内存视图），文件描述符（例如，打开套接字）和安全凭证（例如，启动该进程的用户的ID）处理）。 进程与线程的区别？123456线程共享创建它的进程的地址空间;进程有自己的地址空间。线程可以直接访问其进程的数据段;进程拥有自己父进程数据段的副本。线程可以直接与其进程的其他线程通信;进程必须使用进程间通信来与兄弟进程通信。新线程很容易创建;新流程需要重复父流程。线程可以对同一进程的线程进行相当大的控制;进程只能控制子进程。对主线程的更改（取消，优先级更改等）可能会影响进程的其他线程的行为;对父进程的更改不会影响子进程。 Python GIL(Global Interpreter Lock) In CPython, the global interpreter lock, or GIL, is a mutex that prevents multiple native threads from executing Python bytecodes at once. This lock is necessary mainly because CPython’s memory management is not thread-safe. (However, since the GIL exists, other features have grown to depend on the guarantees that it enforces.) 上面的核心意思就是，无论你启多少个线程，你有多少个cpu, Python在执行的时候会淡定的在同一时刻只允许一个线程运行，擦。。。，那这还叫什么多线程呀？莫如此早的下结结论，听我现场讲。 首先需要明确的一点是GIL并不是Python的特性，它是在实现Python解析器(CPython)时所引入的一个概念。就好比C++是一套语言（语法）标准，但是可以用不同的编译器来编译成可执行代码。有名的编译器例如GCC，INTEL C++，Visual C++等。Python也一样，同样一段代码可以通过CPython，PyPy，Psyco等不同的Python执行环境来执行。像其中的JPython就没有GIL。然而因为CPython是大部分环境下默认的Python执行环境。所以在很多人的概念里CPython就是Python，也就想当然的把GIL归结为Python语言的缺陷。所以这里要先明确一点：GIL并不是Python的特性，Python完全可以不依赖于GIL 这篇文章透彻的剖析了GIL对python多线程的影响，强烈推荐看一下：http://www.dabeaz.com/python/UnderstandingGIL.pdf Python threading模块线程有2种调用方式，如下： 直接调用 12345678910111213141516171819import threadingimport time def sayhi(num): #定义每个线程要运行的函数 print(&quot;running on number:%s&quot; %num) time.sleep(3) if __name__ == &apos;__main__&apos;: t1 = threading.Thread(target=sayhi,args=(1,)) #生成一个线程实例 t2 = threading.Thread(target=sayhi,args=(2,)) #生成另一个线程实例 t1.start() #启动线程 t2.start() #启动另一个线程 print(t1.getName()) #获取线程名 print(t2.getName()) 继承式调用 123456789101112131415161718192021import threadingimport time class MyThread(threading.Thread): def __init__(self,num): threading.Thread.__init__(self) self.num = num def run(self):#定义每个线程要运行的函数 print(&quot;running on number:%s&quot; %self.num) time.sleep(3) if __name__ == &apos;__main__&apos;: t1 = MyThread(1) t2 = MyThread(2) t1.start() t2.start() 加入＆守护进程 有些线程执行后台任务，例如发送keepalive数据包，或执行定期垃圾收集等等。这些仅在主程序运行时才有用，并且一旦其他非守护程序线程退出就可以将它们终止。 如果没有守护程序线程，您必须跟踪它们，并在程序完全退出之前告诉它们退出。通过将它们设置为守护程序线程，您可以让它们运行并忘记它们，当程序退出时，任何守护程序线程都会自动终止。 1234567891011121314151617181920212223242526#_*_coding:utf-8_*___author__ = &apos;Alex Li&apos; import timeimport threading def run(n): print(&apos;[%s]------running----\n&apos; % n) time.sleep(2) print(&apos;--done--&apos;) def main(): for i in range(5): t = threading.Thread(target=run,args=[i,]) t.start() t.join(1) print(&apos;starting thread&apos;, t.getName()) m = threading.Thread(target=main,args=[])m.setDaemon(True) #将main线程设置为Daemon线程,它做为程序主线程的守护线程,当主线程退出时,m线程也会退出,由m启动的其它子线程会同时退出,不管是否执行完任务m.start()m.join(timeout=2)print(&quot;---main thread done----&quot;) 注意：守护程序线程在关闭时突然停止。他们的资源（例如打开文件，数据库事务等）可能无法正确发布。如果您希望线程正常停止，请将它们设置为非守护进程并使用合适的信号机制（如Event）。 线程锁(互斥锁Mutex) 一个进程下可以启动多个线程，多个线程共享父进程的内存空间，也就意味着每个线程可以访问同一份数据，此时，如果2个线程同时要修改同一份数据，会出现什么状况？ 123456789101112131415161718192021import timeimport threading def addNum(): global num #在每个线程中都获取这个全局变量 print(&apos;--get num:&apos;,num ) time.sleep(1) num -=1 #对此公共变量进行-1操作 num = 100 #设定一个共享变量thread_list = []for i in range(100): t = threading.Thread(target=addNum) t.start() thread_list.append(t) for t in thread_list: #等待所有线程执行完毕 t.join() print(&apos;final num:&apos;, num ) 正常来讲，这个num结果应该是0， 但在python 2.7上多运行几次，会发现，最后打印出来的num结果不总是0，为什么每次运行的结果不一样呢？ 哈，很简单，假设你有A,B两个线程，此时都 要对num 进行减1操作， 由于2个线程是并发同时运行的，所以2个线程很有可能同时拿走了num=100这个初始变量交给cpu去运算，当A线程去处完的结果是99，但此时B线程运算完的结果也是99，两个线程同时CPU运算的结果再赋值给num变量后，结果就都是99。那怎么办呢？ 很简单，每个线程在要修改公共数据时，为了避免自己在还没改完的时候别人也来修改此数据，可以给这个数据加一把锁， 这样其它线程想修改此数据时就必须等待你修改完毕并把锁释放掉后才能再访问此数据。 *注：不要在3.x上运行，不知为什么，3.x上的结果总是正确的，可能是自动加了锁 加锁版本 1234567891011121314151617181920212223import timeimport threading def addNum(): global num #在每个线程中都获取这个全局变量 print(&apos;--get num:&apos;,num ) time.sleep(1) lock.acquire() #修改数据前加锁 num -=1 #对此公共变量进行-1操作 lock.release() #修改后释放 num = 100 #设定一个共享变量thread_list = []lock = threading.Lock() #生成全局锁for i in range(100): t = threading.Thread(target=addNum) t.start() thread_list.append(t) for t in thread_list: #等待所有线程执行完毕 t.join() print(&apos;final num:&apos;, num ) GIL VS Lock 机智的同学可能会问到这个问题，就是既然你之前说过了，Python已经有一个GIL来保证同一时间只能有一个线程来执行了，为什么这里还需要lock? 注意啦，这里的lock是用户级的lock,跟那个GIL没关系 ，具体我们通过下图来看一下+配合我现场讲给大家，就明白了。 那你又问了， 既然用户程序已经自己有锁了，那为什么C python还需要GIL呢？加入GIL主要的原因是为了降低程序的开发的复杂度，比如现在的你写python不需要关心内存回收的问题，因为Python解释器帮你自动定期进行内存回收，你可以理解为python解释器里有一个独立的线程，每过一段时间它起wake up做一次全局轮询看看哪些内存数据是可以被清空的，此时你自己的程序 里的线程和 py解释器自己的线程是并发运行的，假设你的线程删除了一个变量，py解释器的垃圾回收线程在清空这个变量的过程中的clearing时刻，可能一个其它线程正好又重新给这个还没来及得清空的内存空间赋值了，结果就有可能新赋值的数据被删除了，为了解决类似的问题，python解释器简单粗暴的加了锁，即当一个线程运行时，其它人都不能动，这样就解决了上述的问题， 这可以说是Python早期版本的遗留问题。 RLock（递归锁） 说白了就是在一个大锁中还要再包含子锁 1234567891011121314151617181920212223242526272829303132333435363738import threading,time def run1(): print(&quot;grab the first part data&quot;) lock.acquire() global num num +=1 lock.release() return numdef run2(): print(&quot;grab the second part data&quot;) lock.acquire() global num2 num2+=1 lock.release() return num2def run3(): lock.acquire() res = run1() print(&apos;--------between run1 and run2-----&apos;) res2 = run2() lock.release() print(res,res2) if __name__ == &apos;__main__&apos;: num,num2 = 0,0 lock = threading.RLock() for i in range(10): t = threading.Thread(target=run3) t.start() while threading.active_count() != 1: print(threading.active_count())else: print(&apos;----all threads done---&apos;) print(num,num2) Semaphore(信号量) 互斥锁 同时只允许一个线程更改数据，而Semaphore是同时允许一定数量的线程更改数据 ，比如厕所有3个坑，那最多只允许3个人上厕所，后面的人只能等里面有人出来了才能再进去。 123456789101112131415161718192021import threading,time def run(n): semaphore.acquire() time.sleep(1) print(&quot;run the thread: %s\n&quot; %n) semaphore.release() if __name__ == &apos;__main__&apos;: num= 0 semaphore = threading.BoundedSemaphore(5) #最多允许5个线程同时运行 for i in range(20): t = threading.Thread(target=run,args=(i,)) t.start() while threading.active_count() != 1: pass #print threading.active_count()else: print(&apos;----all threads done---&apos;) print(num) 计时器此类表示仅在经过一定时间后才应运行的操作与线程一样，通过调用start（）方法启动计时器。可以通过调用thecancel（）方法停止计时器（在其动作开始之前）。计时器在执行其操作之前等待的时间间隔可能与用户指定的时间间隔不完全相同。 12345def hello(): print(&quot;hello, world&quot;) t = Timer(30.0, hello)t.start() # after 30 seconds, &quot;hello, world&quot; will be printed 事件事件是一个简单的同步对象;该事件代表一个内部标志和线程可以等待设置标志，或者自己设置或清除标志。event = threading.Event（）＃客户端线程可以等待设置标志event.wait（） #a服务器线程可以设置或重置它event.set（）event.clear（）如果设置了标志，则wait方法不会执行任何操作。如果该标志被清除，则等待将被阻塞，直到它再次被设置为止。任意数量的线程都可以等待同一事件。 通过Event来实现两个或多个线程间的交互，下面是一个红绿灯的例子，即起动一个线程做交通指挥灯，生成几个线程做车辆，车辆行驶按红灯停，绿灯行的规则。 12345678910111213141516171819202122232425262728293031323334import threading,timeimport randomdef light(): if not event.isSet(): event.set() #wait就不阻塞 #绿灯状态 count = 0 while True: if count &lt; 10: print(&apos;\033[42;1m--green light on---\033[0m&apos;) elif count &lt;13: print(&apos;\033[43;1m--yellow light on---\033[0m&apos;) elif count &lt;20: if event.isSet(): event.clear() print(&apos;\033[41;1m--red light on---\033[0m&apos;) else: count = 0 event.set() #打开绿灯 time.sleep(1) count +=1def car(n): while 1: time.sleep(random.randrange(10)) if event.isSet(): #绿灯 print(&quot;car [%s] is running..&quot; % n) else: print(&quot;car [%s] is waiting for the red light..&quot; %n)if __name__ == &apos;__main__&apos;: event = threading.Event() Light = threading.Thread(target=light) Light.start() for i in range(3): t = threading.Thread(target=car,args=(i,)) t.start() 这里还有一个event使用的例子，员工进公司门要刷卡， 我们这里设置一个线程是“门”， 再设置几个线程为“员工”，员工看到门没打开，就刷卡，刷完卡，门开了，员工就可以通过。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 1 #_*_coding:utf-8_*_ 2 __author__ = &apos;Alex Li&apos; 3 import threading 4 import time 5 import random 6 7 def door(): 8 door_open_time_counter = 0 9 while True:10 if door_swiping_event.is_set():11 print(&quot;\033[32;1mdoor opening....\033[0m&quot;)12 door_open_time_counter +=113 14 else:15 print(&quot;\033[31;1mdoor closed...., swipe to open.\033[0m&quot;)16 door_open_time_counter = 0 #清空计时器17 door_swiping_event.wait()18 19 20 if door_open_time_counter &gt; 3:#门开了已经3s了,该关了21 door_swiping_event.clear()22 23 time.sleep(0.5)24 25 26 def staff(n):27 28 print(&quot;staff [%s] is comming...&quot; % n )29 while True:30 if door_swiping_event.is_set():31 print(&quot;\033[34;1mdoor is opened, passing.....\033[0m&quot;)32 break33 else:34 print(&quot;staff [%s] sees door got closed, swipping the card.....&quot; % n)35 print(door_swiping_event.set())36 door_swiping_event.set()37 print(&quot;after set &quot;,door_swiping_event.set())38 time.sleep(0.5)39 door_swiping_event = threading.Event() #设置事件40 41 42 door_thread = threading.Thread(target=door)43 door_thread.start()44 45 46 47 for i in range(5):48 p = threading.Thread(target=staff,args=(i,))49 time.sleep(random.randrange(3))50 p.start() queue队列当必须在多个线程之间安全地交换信息时，队列在线程编程中特别有用。 class queue.Queue(maxsize=0) #先入先出 class queue.LifoQueue(maxsize=0) #last in fisrt out 先入后出 class queue.PriorityQueue(maxsize=0) #存储数据时可设置优先级的队列 优先级队列的构造函数。 maxsize是一个整数，用于设置可以放入队列的项目数的上限。达到此大小后，插入将阻止，直到消耗队列项。如果maxsize小于或等于零，则队列大小为无限大。首先检索最低值的条目（最低值条目是由sorted（list（entries））[0]返回的条目。条目的典型模式是以下形式的元组：（priority_number，data）。 exception queue.Empty 在对空的Queue对象调用非阻塞get（）（或get_nowait（））时引发异常。 exception queue.Full 在已满的Queue对象上调用非阻塞put（）（或put_nowait（））时引发异常。 Queue.``qsize() Queue.``empty() #return True if empty Queue.``full() # return True if full Queue.``put(item, block=True, timeout=None) 将项目放入队列。如果可选的args块为true且timeout为None（默认值），则在必要时阻塞，直到有空闲插槽可用。如果timeout是一个正数，它会阻止最多超时秒，如果在该时间内没有可用的空闲槽，则会引发Full异常。否则（块为假），如果空闲插槽立即可用，则将项目放入队列，否则引发完全异常（在这种情况下忽略超时）。 Queue.``put_nowait(item) Equivalent to put(item, False). Queue.``get(block=True, timeout=None) 从队列中删除并返回一个项目。如果可选的args块为true且timeout为None（默认值），则在必要时阻止，直到某个项可用为止。如果timeout是一个正数，它会阻止最多超时秒，如果在该时间内没有可用的项，则会引发Empty异常。否则（块为假），如果一个项立即可用则返回一个项，否则引发Empty异常（在这种情况下忽略超时）。 Queue.``get_nowait() Equivalent to get(False). 提供了两种方法来支持跟踪守护进程消费者线程是否已完全处理入队任务。 Queue.task_done（）表示以前排队的任务已完成。由队列使用者线程使用。对于用于获取任务的每个get（），对task_done（）的后续调用会告知队列该任务的处理已完成。 如果join（）当前正在阻塞，则它将在所有项目都已处理后恢复（这意味着已为每个已放入队列的项目收到task_done（）调用）。 如果调用的次数超过队列中放置的项目，则引发ValueError。 Queue.``join() block直到queue被消费完毕 生产者消费者模型在并发编程中使用生产者和消费者模式能够解决绝大多数并发问题。该模式通过平衡生产线程和消费线程的工作能力来提高程序的整体处理数据的速度。 为什么要使用生产者和消费者模式 在线程世界里，生产者就是生产数据的线程，消费者就是消费数据的线程。在多线程开发当中，如果生产者处理速度很快，而消费者处理速度很慢，那么生产者就必须等待消费者处理完，才能继续生产数据。同样的道理，如果消费者的处理能力大于生产者，那么消费者就必须等待生产者。为了解决这个问题于是引入了生产者和消费者模式。 什么是生产者消费者模式 生产者消费者模式是通过一个容器来解决生产者和消费者的强耦合问题。生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。 下面来学习一个最基本的生产者消费者模型的例子 12345678910111213141516171819202122232425262728import threadingimport queue def producer(): for i in range(10): q.put(&quot;骨头 %s&quot; % i ) print(&quot;开始等待所有的骨头被取走...&quot;) q.join() print(&quot;所有的骨头被取完了...&quot;) def consumer(n): while q.qsize() &gt;0: print(&quot;%s 取到&quot; %n , q.get()) q.task_done() #告知这个任务执行完了 q = queue.Queue() p = threading.Thread(target=producer,)p.start() c1 = consumer(&quot;李闯&quot;) 12345678910111213141516171819202122232425import time,randomimport queue,threadingq = queue.Queue()def Producer(name): count = 0 while count &lt;20: time.sleep(random.randrange(3)) q.put(count) print(&apos;Producer %s has produced %s baozi..&apos; %(name, count)) count +=1def Consumer(name): count = 0 while count &lt;20: time.sleep(random.randrange(4)) if not q.empty(): data = q.get() print(data) print(&apos;\033[32;1mConsumer %s has eat %s baozi...\033[0m&apos; %(name, data)) else: print(&quot;-----no baozi anymore----&quot;) count +=1p1 = threading.Thread(target=Producer, args=(&apos;A&apos;,))c1 = threading.Thread(target=Consumer, args=(&apos;B&apos;,))p1.start()c1.start() 多进程multiprocessingmultiprocessing是一个使用类似于线程模块的API支持产生进程的包。多处理包提供本地和远程并发，通过使用子进程而不是线程有效地侧向执行全局解释器锁。因此，多处理模块允许程序员充分利用给定机器上的多个处理器。它可以在Unix和Windows上运行。 12345678910from multiprocessing import Processimport timedef f(name): time.sleep(2) print(&apos;hello&apos;, name) if __name__ == &apos;__main__&apos;: p = Process(target=f, args=(&apos;bob&apos;,)) p.start() p.join() 要显示所涉及的各个进程ID，以下是一个扩展示例： 12345678910111213141516171819from multiprocessing import Processimport os def info(title): print(title) print(&apos;module name:&apos;, __name__) print(&apos;parent process:&apos;, os.getppid()) print(&apos;process id:&apos;, os.getpid()) print(&quot;\n\n&quot;) def f(name): info(&apos;\033[31;1mfunction f\033[0m&apos;) print(&apos;hello&apos;, name) if __name__ == &apos;__main__&apos;: info(&apos;\033[32;1mmain process line\033[0m&apos;) p = Process(target=f, args=(&apos;bob&apos;,)) p.start() p.join() 进程间通讯 不同进程间内存是不共享的，要想实现两个进程间的数据交换，可以用以下方法： Queues 使用方法跟threading里的queue差不多 1234567891011from multiprocessing import Process, Queue def f(q): q.put([42, None, &apos;hello&apos;]) if __name__ == &apos;__main__&apos;: q = Queue() p = Process(target=f, args=(q,)) p.start() print(q.get()) # prints &quot;[42, None, &apos;hello&apos;]&quot; p.join() Pipes Pipe（）函数返回一个由管道连接的连接对象，默认情况下是双工（双向）。例如： 123456789101112from multiprocessing import Process, Pipe def f(conn): conn.send([42, None, &apos;hello&apos;]) conn.close() if __name__ == &apos;__main__&apos;: parent_conn, child_conn = Pipe() p = Process(target=f, args=(child_conn,)) p.start() print(parent_conn.recv()) # prints &quot;[42, None, &apos;hello&apos;]&quot; p.join() Pipe（）返回的两个连接对象代表管道的两端。每个连接对象都有send（）和recv（）方法（以及其他方法）。请注意，如果两个进程（或线程）同时尝试读取或写入管道的同一端，则管道中的数据可能会损坏。当然，同时使用管道的不同端的进程不存在损坏的风险。 ManagerManager（）返回的管理器对象控制一个服务器进程，该进程保存Python对象并允许其他进程使用代理操作它们。 Manager（）返回的管理器将支持类型列表，dict，Namespace，Lock，RLock，Semaphore，BoundedSemaphore，Condition，Event，Barrier，Queue，Value和Array。例如， 123456789101112131415161718192021222324from multiprocessing import Process, Manager def f(d, l): d[1] = &apos;1&apos; d[&apos;2&apos;] = 2 d[0.25] = None l.append(1) print(l) if __name__ == &apos;__main__&apos;: with Manager() as manager: d = manager.dict() l = manager.list(range(5)) p_list = [] for i in range(10): p = Process(target=f, args=(d, l)) p.start() p_list.append(p) for res in p_list: res.join() print(d) print(l) 进程同步 如果不使用不同进程的锁定输出，可能会混淆不清。 1234567891011121314from multiprocessing import Process, Lock def f(l, i): l.acquire() try: print(&apos;hello world&apos;, i) finally: l.release() if __name__ == &apos;__main__&apos;: lock = Lock() for num in range(10): Process(target=f, args=(lock, num)).start() 进程池 进程池内部维护一个进程序列，当使用时，则去进程池中获取一个进程，如果进程池序列中没有可供使用的进进程，那么程序就会等待，直到进程池中有可用进程为止。 进程池中有两个方法： apply apply_async 12345678910111213141516171819from multiprocessing import Process,Poolimport time def Foo(i): time.sleep(2) return i+100 def Bar(arg): print(&apos;--&gt;exec done:&apos;,arg) pool = Pool(5) for i in range(10): pool.apply_async(func=Foo, args=(i,),callback=Bar) #pool.apply(func=Foo, args=(i,)) print(&apos;end&apos;)pool.close()pool.join()#进程池中进程执行完毕后再关闭，如果注释，那么程序直接关闭。]]></content>
      <categories>
        <category>python高级篇</category>
      </categories>
      <tags>
        <tag>python高级篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程与线程间通信方式]]></title>
    <url>%2F2019%2F09%2F08%2F%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[进程间、线程间通信方式一、进程间的通信方式 管道(pipe):管道是一种半双式的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。 有名管道(namedpipe):有名管道也是半双式的通信方式，但是它允许无亲缘关系进程间的通信。 信号量(semophore):信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止 某种进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 消息队列(messagequeue)：消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。 信号(sinal):信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。 共享内存(shared memory):共享内存就是映射一段能其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的IPC方式，它是针对其他进程间通信方式运行效率低而专门设计 的。它往往与其他通信机制，如信号量，配合使用，来实现进程间的同步和通信。 套接节(socket)：套解口也是种进程间通信机制，与其他通信机制不同的是，它可用于不同及其间的进程通信。 二、线程间的通信方式 锁机制:包括互斥锁、条件变量、读写锁 互斥锁提供了以排他方式防止数据结构被并发修改的方法。 读写锁允许多个线程同时读共享数据，而对写操作是互斥的。 条件变量可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进程的。条件变量始终与互斥锁一起使用。 信号量机制(Semaphore):包括无名线程信号量和命名线程信号量 信号机制(Signal):类似进程间的信号处理。 线程间的通信目的主要用于线程同步，所以线程没有像进程通信中的用于数据交换的通信。]]></content>
      <categories>
        <category>python高级篇</category>
      </categories>
      <tags>
        <tag>python高级篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux进程与线程的区别]]></title>
    <url>%2F2019%2F09%2F08%2Flinux%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[Linux进程与线程的区别 首先，简要了解一下进程和线程。对于操作系统而言，进程是核心之核心，整个现代操作系统根本，就是以进程为单位在执行任务。系统的管理架构也是基于进程层面的。在按下电源键之后，计算机就开始了复杂的启动进程，此处有一个经典的问题:当按下电源键之后，计算机如何把自己由静止启动起来的？操作系统启动的过程简直可以描述为上帝创造万物的过程，期初没有世界，但是有上帝，是上帝创造世界，之后创造了万物，然后再创造了人，然后塑造了人的七情六欲，再然后人类社会开始遵循自然规律繁衍生息。。。操作系统启动进程的阶段就相当于上帝造人的阶段。第一个被创建出来的进程是0号进程，这个进程在操作系统层面是不可见的，但它存在着。0号进程完成了操作系统的功能加载与初期设定，然后它创造了1号进程(init),这个1号进程就是操作系统的“耶稣”。1号进程是上帝派来管理整个操作系统的，所以在用pstree查看进程树可知，1号进程位于树根。再之后，系统的很多管理程序都以进程 身份被1号进程创造出来，还创造了人类沟通的桥梁——shell。从那之后，人类可以跟操作系统进行交流，可以编写程序，可以执行任务。 而这一切，都是基于进程的，每一个任务（进程）被创建时，系统会为他分配存储空间等必要资源，然后在内核管理区为该进程创建管理节点，以便后来控制和调度该任务的执行。 进程真正进入执行阶段，还需要获得cpu的使用权，这一切都是操作系统掌管着，也就是所谓的调试，在各种条件满足(资源与cpu使用权均获得)的情况下，启动进程的执行过程。 除cpu而外，一个很重要的资源就是存储器了，系统会为每个进程分配独有的存储空间，当然包括它特别需要的别的资源，比如写入时外部设备是可使用状态等等，有了上面的引入，我们可以对进程做一个简要的总结： 进程，是计算机中的程序关于某数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，是操作系统结构的基础。它的执行需要系统分配资源创建实体之后，才能进行。 随着技术发展，在执行一些细小任务时，本身无需分配单独资源时(多个任务共享同一组资源即可，比如所有子进程共享父进程的资源)，进程的实现机制依然会繁琐的将资源分割，这样造成浪费，而且还消耗时间，后来就有了专门的多任务技术被创造出来——线程。 线程的特点就是在不需要独立资源的情况下就可以运行。如此一来会极节省资源开销，以有处理时间。 从下面几个方面阐述进程和线程的区别。 1).二者的相同点 2).实现方式的差异 3).多任务程序设计模式的区别 4).实体间(进程间，线程间，进线程间)通信方式的不同 5).控制方式的异同 6).资源管理方式的异同 7).个体间辈分关系的迥异 8).进程池与线程池的技术实现差别 1).二者的相同点 无论是进程还是线程，对于程序员而言，都是用来实现多任务并发的技术手段。二者都可以独立调试，因此在多任务环境下，功能上并无差异。并且 二者都具有各自的实体，是系统独立管理的对象个体。所以在系统层面，都可以通过技术手段实现二者的控制。而且二者所具有的状态都非常相似。而且，在多任务程序中，子进程(子线程)的调试一般与父进程(父线程)平等竞争。 其实在Linux内核2.4版以前，线程的实现和管理方式就是完全按照进程方式实现的。在2.6版内核以后才有了单独的线程实现。 2).实现方式的差异 进程是资源分配的基本单位，线程是调度的基本单位。 这句经典名言已流传数十年，各种操作系统教材都可见此描述。确实如此，这就是二者的显著区别。读者请注意“基本”二字。相信有读者看到前半句的时候就在心里思考，“进程岂不是不能调度？”，非也！进程和线程都可以被调度，否则多进程程序该如何运行呢！ 只是，线程是更小的可以调度的单位，也就是说，只要达到线程的水平就可以被调度了，进程自然可以被调度。它强调的是分配资源时的对象必须是进程，不会给一个线程单独分配系统管理的资源。若要运行一个任务，想要获得资源，最起码得有进程，其他子任务可以以线程身份运行，资源共享就行了。 ​ 简而言之，进程的个体间是完全独立的，而线程间是彼此依存的。多进程环境中，任何一个进程的终止，不会影响到其他进程。而多线程环境中，父线程终止，全部子线程被迫终止(没有了资源)。而任何一个子线程终止一般不会影响其他线程，除非子线程执行了exit()系统调用。任何一个子线程执行exit()，全部线程同时灭亡。 其实，也没有人写出只有线程而没有进程的程序。多线程程序中至少有一个主线程，而这个主线程其实就是有main函数的进程。它是整个程序的进程，所有线程都是它的子线程。我们通常把具有多线程的主进程称之为主线程。 从系统实现角度讲，进程的实现是调用fork系统调用： pid_t fork(void); 线程的实现是调用clone系统调用： int clone(int (*fn)(void *), void *child_stack, int flags, void *arg, … /* pid_t *ptid, struct user_desc *tls, pid_t *ctid */ ); 其中，fork()是将父进程的全部资源复制给了子进程。而线程的clone只是复制了一小部分必要的资源。在调用clone时可以通过参数控制要复制的对象。可以说，fork实现的是clone的加强完整版。当然，后来操作系统还进一步优化fork实现——写时复制技术。在子进程需要复制资源(比如子进程执行写入动作更改父进程内存空间)时才复制，否则创建子进程时先不复制。 实际中，编写多进程程序时采用fork创建子进程实体。而创建线程时并不采用clone系统调用，而是采用线程库函数。常用线程库有Linux-Native线程库和POSIX线程库。其中应用最为广泛的是POSIX线程库。因此读者在多线程程序中看到的是pthread_create而非clone。 我们知道，库是建立在操作系统层面上的功能集合，因而它的功能都是操作系统提供的。由此可知，线程库的内部很可能实现了clone的调用。不管是进程还是线程的实体，都是操作系统上运行的实体。 ​ 最后，我们说一下vfork() 。这也是一个系统调用，用来创建一个新的进程。它创建的进程并不复制父进程的资源空间，而是共享，也就说实际上vfork实现的是一个接近线程的实体，只是以进程方式来管理它。并且，vfork()的子进程与父进程的运行时间是确定的：子进程“结束”后父进程才运行。请读者注意“结束”二字。并非子进程完成退出之意，而是子进程返回时。一般采用vfork()的子进程，都会紧接着执行execv启动一个全新的进程，该进程的进程空间与父进程完全独立不相干，所以不需要复制父进程资源空间。此时，execv返回时父进程就认为子进程“结束”了，自己开始运行。实际上子进程继续在一个完全独立的空间运行着。举个例子，比如在一个聊天程序中，弹出了一个视频播放器。你说视频播放器要继承你的聊天程序的进程空间的资源干嘛？莫非视频播放器想要窥探你的聊天隐私不成？懂了吧！ 3).多任务程序设计模式的区别 由于进程间是独立的，所以在设计多进程程序时，需要做到资源独立管理时就有了天然优势，而线程就显得麻烦多了。比如多任务的TCP程序的服务端，父进程执行accept()一个客户端连接请求之后会返回一个新建立的连接的描述符DES，此时如果fork()一个子进程，将DES带入到子进程空间去处理该连接的请求，父进程继续accept等待别的客户端连接请求，这样设计非常简练，而且父进程可以用同一变量(val)保存accept()的返回值，因为子进程会复制val到自己空间，父进程再覆盖此前的值不影响子进程工作。但是如果换成多线程，父线程就不能复用一个变量val多次执行accept()了。因为子线程没有复制val的存储空间，而是使用父线程的，如果子线程在读取val时父线程接受了另一个客户端请求覆盖了该值，则子线程无法继续处理上一次的连接任务了。改进的办法是子线程立马复制val的值在自己的栈区，但父线程必须保证子线程复制动作完成之后再执行新的accept()。但这执行起来并不简单，因为子线程与父线程的调度是独立的，父线程无法知道子线程何时复制完毕。这又得发生线程间通信，子线程复制完成后主动通知父线程。这样一来父线程的处理动作必然不能连贯，比起多进程环境，父线程显得效率有所下降。 PS：这里引述一个知名的面试问题：多进程的TCP服务端，能否互换fork()与accept()的位置？请读者自行思考。 关于资源不独立，看似是个缺点，但在有的情况下就成了优点。多进程环境间完全独立，要实现通信的话就得采用进程间的通信方式，它们通常都是耗时间的。而线程则不用任何手段数据就是共享的。当然多个子线程在同时执行写入操作时需要实现互斥，否则数据就写“脏”了。 4).实体间(进程间，线程间，进线程间)通信方式的不同 进程间的通信方式有这样几种： A.共享内存 B.消息队列 C.信号量 D.有名管道 E.无名管道 F.信号 G.文件 H.socket 线程间的通信方式上述进程间的方式都可沿用，且还有自己独特的几种： A.互斥量 B.自旋锁 C.条件变量 D.读写锁 E.线程信号 G.全局变量 值得注意的是，线程间通信用的信号不能采用进程间的信号，因为信号是基于进程为单位的，而线程是共属于同一进程空间的。故而要采用线程信号。 综上，进程间通信手段有8种。线程间通信手段有13种。 而且，进程间采用的通信方式要么需要切换内核上下文，要么要与外设访问(有名管道，文件)。所以速度会比较慢。而线程采用自己特有的通信方式的话，基本都在自己的进程空间内完成，不存在切换，所以通信速度会较快。也就是说，进程间与线程间分别采用的通信方式，除了种类的区别外，还有速度上的区别。 另外，进程与线程之间穿插通信的方式，除信号以外其他进程间通信方式都可采用。 线程有内核态线程与用户级线程，相关知识请参看我的另一篇博文《Linux线程的实质》。 5).控制方式的异同 进程与线程的身份标示ID管理方式不一样，进程的ID为pid_t类型，实际为一个int型的变量(也就是说是有限的)： /usr/include/unistd.h:260:typedef __pid_t pid_t; /usr/include/bits/types.h:126:# define __STD_TYPE typedef /usr/include/bits/types.h:142:__STD_TYPE __PID_T_TYPE __pid_t; /usr/include/bits/typesizes.h:53:#define __PID_T_TYPE __S32_TYPE /usr/include/bits/types.h:100:#define __S32_TYPE int 在全系统中，进程ID是唯一标识，对于进程的管理都是通过PID来实现的。每创建一个进程，内核去中就会创建一个结构体来存储该进程的全部信息： 注：下述代码来自 Linux内核3.18.1 include/linux/sched.h:1235:struct task_struct { ​ volatile long state; /* -1 unrunnable, 0 runnable, &gt;0 stopped */ ​ void *stack; … ​ pid_t pid; ​ pid_t tgid; … }; 每一个存储进程信息的节点也都保存着自己的PID。需要管理该进程时就通过这个ID来实现(比如发送信号)。当子进程结束要回收时(子进程调用exit()退出或代码执行完)，需要通过wait()系统调用来进行，未回收的消亡进程会成为僵尸进程，其进程实体已经不复存在，但会虚占PID资源，因此回收是有必要的。 线程的ID是一个long型变量： /usr/include/bits/pthreadtypes.h:60:typedef unsigned long int pthread_t; 它的范围大得多，管理方式也不一样。线程ID一般在本进程空间内作用就可以了，当然系统在管理线程时也需要记录其信息。其方式是，在内核创建一个内核态线程与之对应，也就是说每一个用户创建的线程都有一个内核态线程对应。但这种对应关系不是一对一，而是多对一的关系，也就是一个内核态线程可以对应着多个用户级线程。还是请读者参看《Linux线程的实质》普及相关概念。此处贴出blog地址： http://my.oschina.net/cnyinlinux/blog/367910 对于线程而言，若要主动终止需要调用pthread_exit() ，主线程需要调用pthread_join()来回收(前提是该线程没有被detached，相关概念请查阅线程的“分离属性”)。像线发送线程信号也是通过线程ID实现的。 6).资源管理方式的异同 进程本身是资源分配的基本单位，因而它的资源都是独立的，如果有多进程间的共享资源，就要用到进程间的通信方式了，比如共享内存。共享数据就放在共享内存去，大家都可以访问，为保证数据写入的安全，加上信号量一同使用。一般而言，共享内存都是和信号量一起使用。消息队列则不同，由于消息的收发是原子操作，因而自动实现了互斥，单独使用就是安全的。 线程间要使用共享资源不需要用共享内存，直接使用全局变量即可，或者malloc()动态申请内存。显得方便直接。而且互斥使用的是同一进程空间内的互斥量，所以效率上也有优势。 实际中，为了使程序内资源充分规整，也都采用共享内存来存储核心数据。不管进程还是线程，都采用这种方式。原因之一就是，共享内存是脱离进程的资源，如果进程发生意外终止的话，共享内存可以独立存在不会被回收(是否回收由用户编程实现)。进程的空间在进程崩溃的那一刻也被系统回收了。虽然有coredump机制，但也只能是有限的弥补。共享内存在进程down之后还完整保存，这样可以拿来分析程序的故障原因。同时，运行的宝贵数据没有丢失，程序重启之后还能继续处理之前未完成的任务，这也是采用共享内存的又一大好处。 总结之，进程间的通信方式都是脱离于进程本身存在的，是全系统都可见的。这样一来，进程的单点故障并不会损毁数据，当然这不一定全是优点。比如，进程崩溃前对信号量加锁，崩溃后重启，然后再次进入运行状态，此时直接进行加锁，可能造成死锁，程序再也无法继续运转。再比如，共享内存是全系统可见的，如果你的进程资源被他人误读误写，后果肯定也是你不想要的。所以，各有利弊，关键在于程序设计时如何考量，技术上如何规避。这说起来又是编程技巧和经验的事情了。 7).个体间辈分关系的迥异 进程的备份关系森严，在父进程没有结束前，所有的子进程都尊从父子关系，也就是说A创建了B，则A与B是父子关系，B又创建了C，则B与C也是父子关系，A与C构成爷孙关系，也就是说C是A的孙子进程。在系统上使用pstree命令打印进程树，可以清晰看到备份关系。 多线程间的关系没有那么严格，不管是父线程还是子线程创建了新的线程，都是共享父线程的资源，所以，都可以说是父线程的子线程，也就是只存在一个父线程，其余线程都是父线程的子线程。 8).进程池与线程池的技术实现差别 我们都知道，进程和线程的创建时需要时间的，并且系统所能承受的进程和线程数也是有上限的，这样一来，如果业务在运行中需要动态创建子进程或线程时，系统无法承受不能立即创建的话，必然影响业务。综上，聪明的程序员发明了一种新方法——池。 在程序启动时，就预先创建一些子进程或线程，这样在需要用时直接使唤。这就是老人口中的“多生孩子多种树”。程序才开始运行，没有那么多的服务请求，必然大量的进程或线程空闲，这时候一般让他们“冬眠”，这样不耗资源，要不然一大堆孩子的口食也是个负担啊。对于进程和线程而言，方式是不一样的。另外，当你有了任务，要分配给那些孩子的时候，手段也不一样。下面就分别来解说。 进程池 首先创建了一批进程，就得管理，也就是你得分开保存进程ID，可以用数组，也可用链表。建议用数组，这样可以实现常数内找到某个线程，而且既然做了进程池，就预先估计好了生产多少进程合适，一般也不会再动态延展。就算要动态延展，也能预估范围，提前做一个足够大的数组。不为别的，就是为了快速响应。本来错进程池的目的也是为了效率。 接下来就要让闲置进程冬眠了，可以让他们pause()挂起，也可用信号量挂起，还可以用IPC阻塞，方法很多，分析各自优缺点根据实际情况采用就是了。 然后是分配任务了，当你有任务的时候就要让他干活了。唤醒了进程，让它从哪儿开始干呢？肯定得用到进程间通信了，比如信号唤醒它，然后让它在预先指定的地方去读取任务，可以用函数指针来实现，要让它干什么，就在约定的地方设置代码段指针。这也只是告诉了它怎么干，还没说干什么(数据条件)，再通过共享内存把要处理的数据设置好，这也子进程就知道怎么做了。干完之后再来一次进程间通信然后自己继续冬眠，父进程就知道孩子干完了，收割成果。 最后结束时回收子进程，向各进程发送信号唤醒，改变激活状态让其主动结束，然后逐个wait()就可以了。 线程池 线程池的思想与上述类似，只是它更为轻量级，所以调度起来不用等待额外的资源。 要让线程阻塞，用条件变量就是了，需要干活的时候父线程改变条件，子线程就被激活。 线程间通信方式就不用赘述了，不用繁琐的通信就能达成，比起进程间效率要高一些。 线程干完之后自己再改变条件，这样父线程也就知道该收割成果了。 整个程序结束时，逐个改变条件并改变激活状态让子线程结束，最后逐个回收即可。]]></content>
      <categories>
        <category>linux进阶篇</category>
      </categories>
      <tags>
        <tag>linux进阶篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程与线程]]></title>
    <url>%2F2019%2F09%2F08%2F%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[线程与进程一、线程介绍 线程是操作系统能够进行运算调试的最小单位，它被包含在进程之中，是进程中的实际运行单位，一条线程指的是一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。 在同一个进程内的线程的数据可以进行互相访问的。 线程的切换使用过上下文来实现的，比如有一本书，有a和b这两个人(两个线程)看，a看完之后记录当前看到那一页哪一行，然后交给b看，b看完之后记录当前看到了那一页哪一行，此时a又要看了，那么a就通过上次记录的值(上下文)直接找到上次看到了哪里，然后继续往下看。 线程中的5种状态： 各状态说明: 1.新建状态 (New) 使用threading.threading创建实例时候，线程还没有开始运行，此时线程处在新建状态。 当一个线程处于新生状态时，程序还没有开始运行线程中的代码。 2.就绪状态(Runnable) ​ 一个新创建的线程并不自动开始运行，要执行线程，必须调用线程的start()方法。当线程对象调用start()方法即启动了线程，start()方法创建线程运行的系统资源，并调度线程运行run()方法。当start()方法返回后，线程就处于就绪状态。 ​ 处于就绪状态的线程并不一定立即运行run()方法，线程还必须同其他线程竞争CPU时间，只有获得CPU时间才可以运行线程。因为在单CPU的计算机系统中，不可能同时运行多个线程，一个时刻仅有一个线程处于运行状态。因此此时可能有多个线程处于就绪状态。 3.运行状态(Running) ​ 当线程获得CPU时间后，它才进入运行状态，真正开始执行run()方法. 4.阻塞状态(Blocked) ​ 线程运行过程中，可能由于各种原因进入阻塞状态:​ 1&gt;线程通过调用sleep方法进入睡眠状态；​ 2&gt;线程调用一个在I/O上被阻塞的操作，即该操作在输入输出操作完成之前不会返回到它的调用者；​ 3&gt;线程试图得到一个锁，而该锁正被其他线程持有；​ 4&gt;线程在等待某个触发条件； ​ …… ​ 所谓阻塞状态是正在运行的线程没有运行结束，暂时让出CPU，这时其他处于就绪状态的线程就可以获得CPU时间，进入运行状态。 5.死亡状态(Dead) ​ 有两个原因会导致线程死亡：​ 1) run方法正常退出而自然死亡，​ 2) 一个未捕获的异常终止了run方法而使线程猝死。​ 为了确定线程在当前是否存活着（就是要么是可运行的，要么是被阻塞了），需要使用isAlive方法。如果是可运行或被阻塞，这个方法返回true，如果线程仍旧是new状态且不是可运行的， 或者线程死亡了，则返回false. python中的多线程： python通过两个标准库thread和threading提供对线程的支持。thread提供了低级别的、原始的线程以及一个简单的锁、threading则弥补了其缺陷，所以线程模块使用threading就可以了。 多线程在python内实则就是一个假象，为什么这么说呢，因为cpu的处理速度是很快的，所以我们看起来以一个线程在执行多个任务，每个任务的执行的速度是非常之快的，利用上下文切换来快速的切换任务，以至于我们根本感觉不到时。 但是频繁的使用上下文切换也是要耗费一定的资源，因为单线程在每次切换任务的时候需要保存当前任务的上下文。 什么时候用到多线程？ 首先IO操作是不占用cpu的，只有计算的时候才会占用cpu(譬如1+1=2),python中的多线程不适合cpu密集型的任务，适合IO密集型的任务(socket server)。 IO密集型(I/O bound):频繁网络传输、读取硬盘及其他IO设备称之为IO密集型，最简单的就是硬盘存取数据，IO操作并不会涉及到cpu。 计算密集型(cpu bound)：程序大部分在做计算、逻辑判断、循环导致cpu占用率很高的情况，称之为计算密集型，比如说python程序中执行了一段代码1+1,这就是在计算1+1的值。 线程创建方法： 1.普通方法 123456789101112#!/usr/bin/env python3#_*_ coding:utf-8 _*_#Author:wdimport threadingdef task(n): print('run task ',n)for i in range(50):#启动50个线程 t=threading.Thread(target=task,args=(i,))#args参数是一个tuple t.start()#启动线程 2.类继承方法 123456789101112131415161718192021222324#!/usr/bin/env python3#_*_ coding:utf-8 _*_#Author:wdimport threadingclass Mythreading(threading.Thread): def __init__(self,fun,args): self._fun=fun self._agrs=args super(Mythreading,self).__init__() def run(self):#启动线程时候会允许run方法，这里重写父类run方法,可以自定义需要运行的task print('start running ....') self._fun(self._agrs)def func(n): print(n)t=Mythreading(func,1)t.start()#启动运行线程结果：start running ....1 threading模块提供方法： start 线程准备就绪，等待CPU调度 setName 为线程设置名称 getName 获取线程名称 setDaemon 设置为守护线程(在start之前)，默认为前台线程，设置为守护线程以后，如果主线程退出，守护线程无论执行完毕都会退出 join 等待线程执行结果，逐个执行每个线程，执行完毕后继续往下执行，该方法使得多线程变得无意义 run 线程被cpu调度后自动执行线程对象的run方法 isAlive 判断线程是否活跃 threading.active_count 返回当前活跃的线程数量 threading.current_thread 获取当前线程对象 使用list主线程阻塞，子线程并行执行demo： 1234567891011121314151617#!/usr/bin/env python3#_*_ coding:utf-8 _*_#Author:wdimport threadingimport timedef fun(n): print('start running',n) time.sleep(2) print('end runing ',n)thread_list=[]for i in range(10): t=threading.Thread(target=fun,args=(i,)) t.start() thread_list.append(t)for r in thread_list:#循环每个线程，等待其结果，好处是，这样做所有线程启动之后一起join r.join() 这样的好处在于，在启动线程后统一join,缩短了程序运行时间，并且提高运行效率。 关于python的GIL（Global Interpreter Lock） 首先需要明确的一点是GIL并不是python的特性，它是在实现python解析器(cpython)时所引入的一个概念。就好比C++是一套语言(语法)标准，但是可以用不同的编译器来编译成可执行代码 ，有名的导航器例如GCC,INTEL C++，Visual C++等，python也一样，同样一段代码可以通过CPython，pypy,psyco等不同的python执行环境来执行。像其中的JPython就没有GIL。然而因为Cpython是大部分环境下默认的python执行环境，所以在很多人的概念里Cpython就是pypthon，也就想当然的把GIL归结为Python语言的缺陷。所以这里要先明确一点：GIL并不是python的特性，python完全可以不依赖于GIL。 Python GIL其实是功能和性能之间权衡后的产物，它尤其存在的合理性，也有较难改变的客观因素，无论你启多少个线程，你有多少个cpu，Python在执行的时候在同一时刻只允许一个线程运行。 线程锁(互斥锁Mutex) 一个进程下可以启动多个线程，多个线程共享父进程的内存空间，也就意味着每个线程可以访问同一份数据，此时，如果2个线程同时要修改同一份数据，会出现什么状况? 123456789101112131415161718import threadingimport timeNUM=5def fun(): global NUM NUM-=1 time.sleep(2) print(NUM)for i in range(5): t=threading.Thread(target=fun) t.start()结果：00000 上述结果并不是我们想要的，去掉了sleep结果才是我们想要的，若是不去掉sleep呢，该怎么办？，此时我们可以加锁实现。 123456789101112131415import threadingimport timeNUM=5#共享数据def fun(): global NUM lock.acquire()#获取锁 NUM-=1 time.sleep(2) print(NUM) lock.release()#释放锁lock=threading.Lock()for i in range(5): t=threading.Thread(target=fun) t.start() RLock（递归锁） 递归锁，通俗来讲就是大锁里面再加小锁，有人可能会问，那我使用Lock不就完了吗，其实不然，想象一下，现在有两道门，一把锁对应一把钥匙，如果使用Lock，进去第一个门获取一把锁，在进去第二个人门又获取一把锁，然后要出来开锁时候（释放锁）程序还是用第一个门进来的钥匙，此时就会一直阻塞，那么RLock就解决了这样的问题场景。 12345678910111213141516171819import threadingimport timel=threading.RLock()#实例化def indoor(name): print('%s across second door'%name) l.acquire() print('%s do somethind',name) time.sleep(2) l.release()def outdoor(name): print('%s across first door'%name) l.acquire() indoor(name) print('%s do somethind'% name) time.sleep(1) l.release()outdoor('wd')#上述代码，若将l=threading.RLock()改为l=threading.Lock()，程序将一直阻塞。 Semaphore&amp;BoundedSemaphore(信号量) 前面已经介绍过了互斥锁， 互斥锁同时只允许一个线程更改数据，而Semaphore是同时允许一定数量的线程更改数据 ，比如厕所有3个坑，那最多只允许3个人上厕所，后面的人只能等里面有人出来了才能再进去. 1.Semaphore和BoundedSemaphore使用方法一致 方法： acquire(blocking=True,timeout=None) release（） 123456789101112131415161718192021222324import threadingimport timedef door(n): sp.acquire()#获取一把锁，可设置超时时间 print(&apos;%d in the door&apos;% n) time.sleep(1) print(&apos;%d out the door&apos;% n) sp.release()#释放锁sp=threading.Semaphore(3)#最多允许3个线程同时运行（获取到信号量）for i in range(5): t=threading.Thread(target=door,args=(i,)) t.start()#启动线程结果：0 in the door1 in the door2 in the door1 out the door0 out the door3 in the door4 in the door2 out the door4 out the door3 out the door Events Event是线程间通信最间的机制之一：一个线程发送一个event信号，其他的线程则等待这个信号。用于主线程控制其他线程的执行。 Events 维护着一个flag，这个flag可以使用set()设置成True或者使用clear()重置为False，flag默认为False，而当flag为false时候，wait(timeout=s)则阻塞。 常用方法： Event.set():将标志设置为True Event.clear():清空标志位，设置为False Event.wait(timeout=s):等待（阻塞），直到标志位变成True Event.isSet():判断标志位是否被设置 123456789101112131415161718192021222324252627282930313233343536import threading,timeimport randomdef light(): if not event.is_Set(): event.set() #wait就不阻塞 #绿灯状态 count = 0 while True: if count &lt; 10: print('\033[42;1m--green light on---\033[0m') elif count &lt;13: print('\033[43;1m--yellow light on---\033[0m') elif count &lt;20: if event.isSet(): event.clear() print('\033[41;1m--red light on---\033[0m') else: count = 0 event.set() #打开绿灯 time.sleep(1) count +=1def car(n): while 1: time.sleep(random.randrange(10)) if event.isSet(): #绿灯 print("car [%s] is running.." % n) else: print("car [%s] is waiting for the red light.." %n) #event.wait()if __name__ == '__main__': event = threading.Event() Light = threading.Thread(target=light) Light.start() for i in range(3): t = threading.Thread(target=car,args=(i,)) t.start() Timer（定时器） Timer用来定时执行某个线程，若取消运行，则使用cancel方法。 12345678910import threadingdef show_name(name): print("my name is ",name)t=threading.Timer(5,show_name,args=('wd',))#设置5秒后运行该线程t.start()#启动线程print('要开始运行线程了')t.cancel()#取消运行的线程结果：要开始运行线程了 线程池： 启动一个线程消耗的资源非常少，所以对线程的使用官方并没有给出标准的线程池模块，第三方模块(Threadpool),下面我们自己定义简单线程池。 123456789101112131415161718192021import threadingimport queueimport timeclass MyThread: def __init__(self,max_num=10): self.queue = queue.Queue() for n in range(max_num): self.queue.put(threading.Thread) def get_thread(self): return self.queue.get() def put_thread(self): self.queue.put(threading.Thread)pool = MyThread(5)def RunThread(arg,pool): print(arg) time.sleep(2) pool.put_thread()for n in range(10): Thread=pool.get_thread() t=Thread(target=RunThread, args=(n,pool,)) t.start() 二、进程介绍 一个进程至少要包含一个线程，每个进程在启动的时候就会自动的启动一个线程，进程里面的第一个线程就是主线程，每次在进程内创建的子线程都是由主线程进程创建和销毁，子线程也可以由主线程创建出来的线程创建和销毁线程。 进程是对各种资源管理的集合，比如要调用内存、CPU、网卡、声卡等，进程要操作上述的硬件之前都必须要创建一个线程，进程里面可以包含多个线程，QQ就是一个进程。 继续拿QQ来说，比如我现在打卡了QQ的聊天窗口、个人信息窗口、设置窗口等，那么每一个打开的窗口都是一个线程，他们都在执行不同的任务，比如聊天窗口这个线程可以和好友进行互动，聊天，视频等，个人信息窗口我可以查看、修改自己的资料。 为了进程安全起见，所以两个进程之间的数据是不能够互相访问的(默认情况下)，比如自己写了一个应用程序，然后让别人运行起来，那么我的这个程序就可以访问用户启动的其他应用，我可以通过我自己的程序去访问QQ，然后拿到一些聊天记录等比较隐秘的信息，那么这个时候就不安全了，所以说进程与进程之间的数据是不可以互相访问的，而且每一个进程的内存是独立的。 多进程多进程的资源是独立的，不可以互相访问，如果想多个进程之间实现数据交互就必须通过中间件实现。 启动一个进程方法与启动一个线程类似 12345678910111213#!/usr/bin/env python3#_*_ coding:utf-8 _*_#Author:wdfrom multiprocessing import Processdef show(n): print(&apos;runing&apos;,n)if __name__ == &apos;__main__&apos;: p=Process(target=show,args=(1,)) p.start() p.join() 在进程中启动线程： 12345678910111213141516171819202122232425262728#!/usr/bin/env python3#_*_ coding:utf-8 _*_#Author:wdfrom multiprocessing import Processimport threadingimport osdef fun(n): print(&quot;run threading &quot;,n) print(&apos;当前进程id &apos;,os.getpid())def show(n): print(&quot;子进程id:&#123;&#125; 父进程id:&#123;&#125; &quot;.format(os.getpid(),os.getppid()))#ppid指父进程pid，pid当前进程pid print(&apos;runing&apos;,n) t=threading.Thread(target=fun,args=(2,))#启动一个线程 t.start() t.join()if __name__ == &apos;__main__&apos;: print(&quot;主进程id&quot;,os.getpid()) p=Process(target=show,args=(1,)) p.start() p.join()结果：主进程id 8092子进程id:9100父进程id:8092 runing 1run threading 2当前进程id 9100 进程间通信方法（Queue、Pipes、Mangers） 前面已经提到，进程间通信是需要中间件来实现的，下面介绍几个实现进程间通信的中间件。 1.进程Queue：建立一个共享的队列(其实并不是共享的，实际是克隆的，内部维护着数据的共享)，多个进程可以向队列里存/取数据。 123456789101112131415161718#!/usr/bin/env python3#_*_ coding:utf-8 _*_#Author:wdfrom multiprocessing import Queue,Processdef fun(q): q.put([1,2,3])if __name__ == &apos;__main__&apos;: Q=Queue(5)#设置进程队列长度 for i in range(2):#启动两个进程，想队列里put数据 process=Process(target=fun,args=(Q,))#创建一个进程，将Q传入，实际上是克隆了Q process.start() process.join() print(Q.get())#在主进程中获取元素 print(Q.get())结果：[1, 2, 3][1, 2, 3] 2.Pipes(管道) 正如其名，进程间的管道内部机制通过启动socket连接来维护两个进程间的通讯。 123456789101112131415from multiprocessing import Process,Pipedef son_process(con): con.send(&apos;hello wd&apos;)#向管道另一头发起 print(&quot;from father:&quot;,con.recv())#子进程收取数据，如果没收到会阻塞if __name__ == &apos;__main__&apos;: son,father=Pipe()#实例化管道，生成socket连接，一个客户端一个服务端 P=Process(target=son_process,args=(son,))#启动一个子进程，将生成的socket对象传递进去 P.start() #P.join() print(father.recv())#主进程收取子进程信息 father.send(&apos;ok,i know&apos;)结果：hello wdfrom father: ok,i know 3.Manager(数据共享) Manager实现了多个进程间的数据共享，支持的数据类型有 list, dict, Namespace, Lock, RLock, Semaphore, BoundedSemaphore, Condition, Event, Barrier, Queue, Value and Array。 123456789101112131415161718from multiprocessing import Manager,Processdef fun(l,d): l.append(1) l.append(2) d[&apos;name&apos;]=&apos;wd&apos;if __name__ == &apos;__main__&apos;: with Manager() as manager: L=manager.list()#定义共享列表 D=manager.dict()#定义共享字典 p1=Process(target=fun,args=(L,D))#启动一个进程，将定义的list和dict传入 p1.start()#启动进程 p1.join()#等带结果 print(L) print(D)结果：[1, 2]&#123;&apos;name&apos;: &apos;wd&apos;&#125; 进程锁(Lock) 进程锁和线程锁使用语法上完全一致。 1234567891011121314151617181920from multiprocessing import Process,Lockimport timedef fun(l,i): l.acquire()#获取锁 print(&quot;running process &quot;,i) time.sleep(2) l.release()#释放if __name__ == &apos;__main__&apos;: lock=Lock()#生成锁的实例 for i in range(5): p=Process(target=fun,args=(lock,i))#创建进程 p.start()#启动，这里没有join，看到的效果还是窜行的结果：running process 0running process 1running process 2running process 3running process 4 pool(进程池) 进程池内部维护一个进程序列，当使用时，则去进程池中获取一个进程，如果进程池序列中没有可供使用的进进程，那么程序就会等待，直到进程池中有可用进程为止。 主要方法： apply ：该方法启动进程为串行执行 apply_async：启动进程为并行执行 1234567891011121314151617181920212223242526272829303132333435from multiprocessing import Poolimport osimport timedef fun(i): print(&apos;runing process &apos;,i) time.sleep(2)def end_fun(i=None): print(&apos;done&apos;) print(&quot;call back process:&quot;,os.getpid())if __name__ == &apos;__main__&apos;: print(&quot;主进程:&quot;,os.getpid()) pool=Pool(3)#最多运行3个进程同时运行 for i in range(5): pool.apply_async(func=fun,args=(i,),callback=end_fun)#并行执行，callback回调由主程序回调 #pool.apply(func=fun,args=(i,))串行执行 pool.close()#关闭进程池 pool.join()#进程池中进程执行完毕后再关闭，如果注释，那么程序直接关闭结果：主进程: 12396runing process 0runing process 1runing process 2runing process 3runing process 4donecall back process: 12396donecall back process: 12396donecall back process: 12396donecall back process: 12396donecall back process: 12396 三、线程与进程的关系与区别 线程是执行的指令集，进程是资源的集合； 线程的启动速度要比进程的启动速度要快； 两个线程的执行速度是一样的； 进程与线程的运行速度是没有可比性的； 线程共享创建它的进程的内存空间，进程的内存是独立的。 两个线程共享的数据都是同一份数据，两个子进程的数据不是共享的，而且数据是独立的; 同一个进程的线程之间可以直接交流，同一个主进程的多个子进程之间是不可以进行交流，如果两个进程之间需要通信，就必须要通过一个中间代理来实现; 一个新的线程很容易被创建，一个新的进程创建需要对父进程进行一次克隆 一个线程可以控制和操作同一个进程里的其他线程，线程与线程之间没有隶属关系，但是进程只能操作子进程 改变主线程，有可能会影响到其他线程的行为，但是对于父进程的修改是不会影响子进程; 详细介绍可参考：https://my.oschina.net/cnyinlinux/blog/422207]]></content>
      <categories>
        <category>python高级篇</category>
      </categories>
      <tags>
        <tag>python高级篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是CPU密集型、IO密集型？]]></title>
    <url>%2F2019%2F09%2F08%2F%E4%BB%80%E4%B9%88%E6%98%AFCPU%E5%AF%86%E9%9B%86%E5%9E%8B%E3%80%81IO%E5%AF%86%E9%9B%86%E5%9E%8B%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[什么是CPU密集型、IO密集型？CPU密集型（CPU-bound）cpu密集型也叫计算密集型，指的是系统的硬盘，内存性能相对cpu要好很多，此时，系统运作大部分的状况是CPU Loadng 100%，cpu要读/写I/O(硬盘/内存),I/O在很短的时间就可以完成，而cpu还有许多运算要处理，CPU Loading很高。 在多重程序系统中，大部份时间用来做计算、逻辑判断等cpu运作的程序称之cpu bound。例如一个计算圆周率至小数点一千位以下的程序，在执行的过程当中绝大部分时间用在三角函数和开根号的计算，便是属于cpu bound的程序。 cpu bound和程序一般而言cpu占用率相当高，这可能是因为任务本身不太需要访问I/O设备，也可能是因为程序是多线程实现因此屏蔽掉了等待I/O的时间。 IO密集型（I/O bound）IO密集型指的是系统的CPU性能相对硬盘、内存要好很多，此时，系统运作，大部分的状况是CPU在等I/O (硬盘/内存) 的读/写操作，此时CPU Loading并不高。 I/O bound的程序一般在达到性能极限时，CPU占用率仍然较低。这可能是因为任务本身需要大量I/O操作，而pipeline做得不是很好，没有充分利用处理器能力。 CPU密集型 vs IO密集型我们可以把任务分为计算密集型和IO密集型。 计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。 计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。 第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。 IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。 总之，计算密集型程序适合C语言多线程，I/O密集型适合脚本语言开发的多线程。]]></content>
      <categories>
        <category>python高级篇</category>
      </categories>
      <tags>
        <tag>python高级篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[socket]]></title>
    <url>%2F2019%2F09%2F05%2Fsocket%2F</url>
    <content type="text"><![CDATA[socketSocket语法及相关 socket概念socket本质上就是在2台网络互通的电脑之间，架设一个通道，两台电脑通过这个通道来实现数据的互相传递。 我们知道网络 通信 都 是基于 ip+port 方能定位到目标的具体机器上的具体服务，操作系统有0-65535个端口，每个端口都可以独立对外提供服务，如果 把一个公司比做一台电脑 ，那公司的总机号码就相当于ip地址， 每个员工的分机号就相当于端口， 你想找公司某个人，必须 先打电话到总机，然后再转分机 。 建立一个socket必须至少有2端， 一个服务端，一个客户端， 服务端被动等待并接收请求，客户端主动发起请求， 连接建立之后，双方可以互发数据。 12345678910111213141516171819202122socket server#!/usr/bin/env python# -*- coding:utf-8 -*-import socketip_port = (&apos;127.0.0.1&apos;,9999)sk = socket.socket()sk.bind(ip_port)sk.listen(5)while True: print &apos;server waiting...&apos; conn,addr = sk.accept() client_data = conn.recv(1024) print client_data conn.sendall(&apos;不要回答,不要回答,不要回答&apos;) conn.close() 123456789101112131415socket client#!/usr/bin/env python# -*- coding:utf-8 -*-import socketip_port = (&apos;127.0.0.1&apos;,9999)sk = socket.socket()sk.connect(ip_port)sk.sendall(&apos;请求占领地球&apos;)server_reply = sk.recv(1024)print server_replysk.close() WEB服务应用： 123456789101112131415161718192021#!/usr/bin/env python#coding:utf-8import socket def handle_request(client): buf = client.recv(1024) client.send(&quot;HTTP/1.1 200 OK\r\n\r\n&quot;) client.send(&quot;Hello, World&quot;) def main(): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.bind((&apos;localhost&apos;,8080)) sock.listen(5) while True: connection, address = sock.accept() handle_request(connection) connection.close() if __name__ == &apos;__main__&apos;: main() 更多功能 sk = socket.socket(socket.AF_INET,socket.SOCK_STREAM,0) 参数一：地址簇 socket.AF_INET IPv4（默认） socket.AF_INET6 IPv6 socket.AF_UNIX 只能够用于单一的Unix系统进程间通信 参数二：类型 socket.SOCK_STREAM 流式socket , for TCP （默认） socket.SOCK_DGRAM 数据报式socket , for UDP socket.SOCK_RAW 原始套接字，普通的套接字无法处理ICMP、IGMP等网络报文，而SOCK_RAW可以；其次，SOCK_RAW也可以处理特殊的IPv4报文；此外，利用原始套接字，可以通过IP_HDRINCL套接字选项由用户构造IP头。 socket.SOCK_RDM 是一种可靠的UDP形式，即保证交付数据报但不保证顺序。SOCK_RAM用来提供对原始协议的低级访问，在需要执行某些特殊操作时使用，如发送ICMP报文。SOCK_RAM通常仅限于高级用户或管理员运行的程序使用。 socket.SOCK_SEQPACKET 可靠的连续数据包服务 参数三：协议 0 （默认）与特定的地址家族相关的协议,如果是 0 ，则系统就会根据地址格式和套接类别,自动选择一个合适的协议 1234567891011121314151617181920212223import socketip_port = (&apos;127.0.0.1&apos;,9999)sk = socket.socket(socket.AF_INET,socket.SOCK_DGRAM,0)sk.bind(ip_port)while True: data = sk.recv(1024) print dataimport socketip_port = (&apos;127.0.0.1&apos;,9999)sk = socket.socket(socket.AF_INET,socket.SOCK_DGRAM,0)while True: inp = raw_input(&apos;数据：&apos;).strip() if inp == &apos;exit&apos;: break sk.sendto(inp,ip_port)sk.close() sk.bind(address) s.bind(address) 将套接字绑定到地址。address地址的格式取决于地址族。在AF_INET下，以元组（host,port）的形式表示地址。 sk.listen(backlog) 开始监听传入连接。backlog指定在拒绝连接之前，可以挂起的最大连接数量。 ​ backlog等于5，表示内核已经接到了连接请求，但服务器还没有调用accept进行处理的连接个数最大为5​ 这个值不能无限大，因为要在内核中维护连接队列 sk.setblocking(bool) 是否阻塞（默认True），如果设置False，那么accept和recv时一旦无数据，则报错。 sk.accept() 接受连接并返回（conn,address）,其中conn是新的套接字对象，可以用来接收和发送数据。address是连接客户端的地址。 接收TCP 客户的连接（阻塞式）等待连接的到来 sk.connect(address) 连接到address处的套接字。一般，address的格式为元组（hostname,port）,如果连接出错，返回socket.error错误。 sk.connect_ex(address) 同上，只不过会有返回值，连接成功时返回 0 ，连接失败时候返回编码，例如：10061 sk.close() 关闭套接字 sk.recv(bufsize[,flag]) 接受套接字的数据。数据以字符串形式返回，bufsize指定最多可以接收的数量。flag提供有关消息的其他信息，通常可以忽略。 sk.recvfrom(bufsize[.flag]) 与recv()类似，但返回值是（data,address）。其中data是包含接收数据的字符串，address是发送数据的套接字地址。 sk.send(string[,flag]) 将string中的数据发送到连接的套接字。返回值是要发送的字节数量，该数量可能小于string的字节大小。即：可能未将指定内容全部发送。 sk.sendall(string[,flag]) 将string中的数据发送到连接的套接字，但在返回之前会尝试发送所有数据。成功返回None，失败则抛出异常。 ​ 内部通过递归调用send，将所有内容发送出去。 sk.sendto(string[,flag],address) 将数据发送到套接字，address是形式为（ipaddr，port）的元组，指定远程地址。返回值是发送的字节数。该函数主要用于UDP协议。 sk.settimeout(timeout) 设置套接字操作的超时期，timeout是一个浮点数，单位是秒。值为None表示没有超时期。一般，超时期应该在刚创建套接字时设置，因为它们可能用于连接的操作（如 client 连接最多等待5s ） sk.getpeername() 返回连接套接字的远程地址。返回值通常是元组（ipaddr,port）。 sk.getsockname() 返回套接字自己的地址。通常是一个元组(ipaddr,port) sk.fileno() 套接字的文件描述符 1234567891011121314151617181920212223242526# 服务端import socketip_port = (&apos;127.0.0.1&apos;,9999)sk = socket.socket(socket.AF_INET,socket.SOCK_DGRAM,0)sk.bind(ip_port)while True: data,(host,port) = sk.recvfrom(1024) print(data,host,port) sk.sendto(bytes(&apos;ok&apos;, encoding=&apos;utf-8&apos;), (host,port))#客户端import socketip_port = (&apos;127.0.0.1&apos;,9999)sk = socket.socket(socket.AF_INET,socket.SOCK_DGRAM,0)while True: inp = input(&apos;数据：&apos;).strip() if inp == &apos;exit&apos;: break sk.sendto(bytes(inp, encoding=&apos;utf-8&apos;),ip_port) data = sk.recvfrom(1024) print(data)sk.close() 实例：智能机器人 12345678910111213141516171819202122232425服务端#!/usr/bin/env python# -*- coding:utf-8 -*-import socketip_port = (&apos;127.0.0.1&apos;,8888)sk = socket.socket()sk.bind(ip_port)sk.listen(5)while True: conn,address = sk.accept() conn.sendall(&apos;欢迎致电 10086，请输入1xxx,0转人工服务.&apos;) Flag = True while Flag: data = conn.recv(1024) if data == &apos;exit&apos;: Flag = False elif data == &apos;0&apos;: conn.sendall(&apos;通过可能会被录音.balabala一大推&apos;) else: conn.sendall(&apos;请重新输入.&apos;) conn.close() 123456789101112131415161718192021客户端#!/usr/bin/env python# -*- coding:utf-8 -*-import socketip_port = (&apos;127.0.0.1&apos;,8005)sk = socket.socket()sk.connect(ip_port)sk.settimeout(5)while True: data = sk.recv(1024) print &apos;receive:&apos;,data inp = raw_input(&apos;please input:&apos;) sk.sendall(inp) if inp == &apos;exit&apos;: breaksk.close() IO多路复用I/O多路复用指：通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。 Linux Linux中的 select，poll，epoll 都是IO多路复用的机制。 12345678910111213141516171819select select最早于1983年出现在4.2BSD中，它通过一个select()系统调用来监视多个文件描述符的数组，当select()返回后，该数组中就绪的文件描述符便会被内核修改标志位，使得进程可以获得这些文件描述符从而进行后续的读写操作。select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点，事实上从现在看来，这也是它所剩不多的优点之一。select的一个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，不过可以通过修改宏定义甚至重新编译内核的方式提升这一限制。另外，select()所维护的存储大量文件描述符的数据结构，随着文件描述符数量的增大，其复制的开销也线性增长。同时，由于网络响应时间的延迟使得大量TCP连接处于非活跃状态，但调用select()会对所有socket进行一次线性扫描，所以这也浪费了一定的开销。 poll poll在1986年诞生于System V Release 3，它和select在本质上没有多大差别，但是poll没有最大文件描述符数量的限制。poll和select同样存在一个缺点就是，包含大量文件描述符的数组被整体复制于用户态和内核的地址空间之间，而不论这些文件描述符是否就绪，它的开销随着文件描述符数量的增加而线性增大。另外，select()和poll()将就绪的文件描述符告诉进程后，如果进程没有对其进行IO操作，那么下次调用select()和poll()的时候将再次报告这些文件描述符，所以它们一般不会丢失就绪的消息，这种方式称为水平触发（Level Triggered）。 epoll 直到Linux2.6才出现了由内核直接支持的实现方法，那就是epoll，它几乎具备了之前所说的一切优点，被公认为Linux2.6下性能最好的多路I/O就绪通知方法。epoll可以同时支持水平触发和边缘触发（Edge Triggered，只告诉进程哪些文件描述符刚刚变为就绪状态，它只说一遍，如果我们没有采取行动，那么它将不会再次告知，这种方式称为边缘触发），理论上边缘触发的性能要更高一些，但是代码实现相当复杂。epoll同样只告知那些就绪的文件描述符，而且当我们调用epoll_wait()获得就绪文件描述符时，返回的不是实际的描述符，而是一个代表就绪描述符数量的值，你只需要去epoll指定的一个数组中依次取得相应数量的文件描述符即可，这里也使用了内存映射（mmap）技术，这样便彻底省掉了这些文件描述符在系统调用时复制的开销。另一个本质的改进在于epoll采用基于事件的就绪通知方式。在select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait()时便得到通知。 Python Python中有一个select模块，其中提供了：select、poll、epoll三个方法，分别调用系统的 select，poll，epoll 从而实现IO多路复用。 123456Windows Python： 提供： selectMac Python： 提供： selectLinux Python： 提供： select、poll、epoll 注意：网络操作、文件操作、终端操作等均属于IO操作，对于windows只支持Socket操作，其他系统支持其他IO操作，但是无法检测 普通文件操作 自动上次读取是否已经变化。 对于select方法： 1234567891011句柄列表11, 句柄列表22, 句柄列表33 = select.select(句柄序列1, 句柄序列2, 句柄序列3, 超时时间) 参数： 可接受四个参数（前三个必须）返回值：三个列表 select方法用来监视文件句柄，如果句柄发生变化，则获取该句柄。1、当 参数1 序列中的句柄发生可读时（accetp和read），则获取发生变化的句柄并添加到 返回值1 序列中2、当 参数2 序列中含有句柄时，则将该序列中所有的句柄添加到 返回值2 序列中3、当 参数3 序列中的句柄发生错误时，则将该发生错误的句柄添加到 返回值3 序列中4、当 超时时间 未设置，则select会一直阻塞，直到监听的句柄发生变化 当 超时时间 ＝ 1时，那么如果监听的句柄均无任何变化，则select会阻塞 1 秒，之后返回三个空列表，如果监听的句柄有变化，则直接执行。 123456789101112利用select监听终端操作实例#!/usr/bin/env python# -*- coding:utf-8 -*-import selectimport threadingimport syswhile True: readable, writeable, error = select.select([sys.stdin,],[],[],1) if sys.stdin in readable: print &apos;select get stdin&apos;,sys.stdin.readline() 1234567891011121314151617181920212223242526272829303132333435利用select实现伪同时处理多个Socket客户端请求：服务端#!/usr/bin/env python# -*- coding:utf-8 -*-import socketimport selectsk1 = socket.socket(socket.AF_INET, socket.SOCK_STREAM)sk1.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)sk1.bind((&apos;127.0.0.1&apos;,8002))sk1.listen(5)sk1.setblocking(0)inputs = [sk1,]while True: readable_list, writeable_list, error_list = select.select(inputs, [], inputs, 1) for r in readable_list: # 当客户端第一次连接服务端时 if sk1 == r: print &apos;accept&apos; request, address = r.accept() request.setblocking(0) inputs.append(request) # 当客户端连接上服务端之后，再次发送数据时 else: received = r.recv(1024) # 当正常接收客户端发送的数据时 if received: print &apos;received data:&apos;, received # 当客户端关闭程序时 else: inputs.remove(r)sk1.close() 1234567891011121314利用select实现伪同时处理多个Socket客户端请求：客户端#!/usr/bin/env python# -*- coding:utf-8 -*-import socketip_port = (&apos;127.0.0.1&apos;,8002)sk = socket.socket()sk.connect(ip_port)while True: inp = raw_input(&apos;please input:&apos;) sk.sendall(inp)sk.close() 此处的Socket服务端相比与原生的Socket，他支持当某一个请求不再发送数据时，服务器端不会等待而是可以去处理其他请求的数据。但是，如果每个请求的耗时比较长时，select版本的服务器端也无法完成同时操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119基于select实现socket服务端#!/usr/bin/env python#coding:utf8&apos;&apos;&apos; 服务器的实现 采用select的方式&apos;&apos;&apos;import selectimport socketimport sysimport Queue#创建套接字并设置该套接字为非阻塞模式server = socket.socket(socket.AF_INET,socket.SOCK_STREAM)server.setblocking(0)#绑定套接字server_address = (&apos;localhost&apos;,10000)print &gt;&gt;sys.stderr,&apos;starting up on %s port %s&apos;% server_addressserver.bind(server_address)#将该socket变成服务模式#backlog等于5，表示内核已经接到了连接请求，但服务器还没有调用accept进行处理的连接个数最大为5#这个值不能无限大，因为要在内核中维护连接队列server.listen(5)#初始化读取数据的监听列表,最开始时希望从server这个套接字上读取数据inputs = [server]#初始化写入数据的监听列表，最开始并没有客户端连接进来，所以列表为空outputs = []#要发往客户端的数据message_queues = &#123;&#125;while inputs: print &gt;&gt;sys.stderr,&apos;waiting for the next event&apos; #调用select监听所有监听列表中的套接字，并将准备好的套接字加入到对应的列表中 readable,writable,exceptional = select.select(inputs,outputs,inputs)#列表中的socket 套接字 如果是文件呢？ #监控文件句柄有某一处发生了变化 可写 可读 异常属于Linux中的网络编程 #属于同步I/O操作，属于I/O复用模型的一种 #rlist--等待到准备好读 #wlist--等待到准备好写 #xlist--等待到一种异常 #处理可读取的套接字 &apos;&apos;&apos; 如果server这个套接字可读，则说明有新链接到来 此时在server套接字上调用accept,生成一个与客户端通讯的套接字 并将与客户端通讯的套接字加入inputs列表，下一次可以通过select检查连接是否可读 然后在发往客户端的缓冲中加入一项，键名为:与客户端通讯的套接字，键值为空队列 select系统调用是用来让我们的程序监视多个文件句柄(file descrīptor)的状态变化的。程序会停在select这里等待， 直到被监视的文件句柄有某一个或多个发生了状态改变 &apos;&apos;&apos; &apos;&apos;&apos; 若可读的套接字不是server套接字,有两种情况:一种是有数据到来，另一种是链接断开 如果有数据到来,先接收数据,然后将收到的数据填入往客户端的缓存区中的对应位置，最后 将于客户端通讯的套接字加入到写数据的监听列表: 如果套接字可读.但没有接收到数据，则说明客户端已经断开。这时需要关闭与客户端连接的套接字 进行资源清理 &apos;&apos;&apos; for s in readable: if s is server: connection,client_address = s.accept() print &gt;&gt;sys.stderr,&apos;connection from&apos;,client_address connection.setblocking(0)#设置非阻塞 inputs.append(connection) message_queues[connection] = Queue.Queue() else: data = s.recv(1024) if data: print &gt;&gt;sys.stderr,&apos;received &quot;%s&quot; from %s&apos;% \ (data,s.getpeername()) message_queues[s].put(data) if s not in outputs: outputs.append(s) else: print &gt;&gt;sys.stderr,&apos;closing&apos;,client_address if s in outputs: outputs.remove(s) inputs.remove(s) s.close() del message_queues[s] #处理可写的套接字 &apos;&apos;&apos; 在发送缓冲区中取出响应的数据，发往客户端。 如果没有数据需要写，则将套接字从发送队列中移除，select中不再监视 &apos;&apos;&apos; for s in writable: try: next_msg = message_queues[s].get_nowait() except Queue.Empty: print &gt;&gt;sys.stderr,&apos; &apos;,s,getpeername(),&apos;queue empty&apos; outputs.remove(s) else: print &gt;&gt;sys.stderr,&apos;sending &quot;%s&quot; to %s&apos;% \ (next_msg,s.getpeername()) s.send(next_msg) #处理异常情况 for s in exceptional: for s in exceptional: print &gt;&gt;sys.stderr,&apos;exception condition on&apos;,s.getpeername() inputs.remove(s) if s in outputs: outputs.remove(s) s.close() del message_queues[s] SocketServer模块SocketServer内部使用 IO多路复用 以及 “多线程” 和 “多进程” ，从而实现并发处理多个客户端请求的Socket服务端。即：每个客户端请求连接到服务器时，Socket服务端都会在服务器是创建一个“线程”或者“进程” 专门负责处理当前客户端的所有请求。 ThreadingTCPServer ThreadingTCPServer实现的Soket服务器内部会为每个client创建一个 “线程”，该线程用来和客户端进行交互。 1、ThreadingTCPServer基础 使用ThreadingTCPServer: 创建一个继承自 SocketServer.BaseRequestHandler 的类 类中必须定义一个名称为 handle 的方法 启动ThreadingTCPServer 12345678910111213141516171819202122232425SocketServer实现服务器#!/usr/bin/env python# -*- coding:utf-8 -*-import SocketServerclass MyServer(SocketServer.BaseRequestHandler): def handle(self): # print self.request,self.client_address,self.server conn = self.request conn.sendall(&apos;欢迎致电 10086，请输入1xxx,0转人工服务.&apos;) Flag = True while Flag: data = conn.recv(1024) if data == &apos;exit&apos;: Flag = False elif data == &apos;0&apos;: conn.sendall(&apos;通过可能会被录音.balabala一大推&apos;) else: conn.sendall(&apos;请重新输入.&apos;)if __name__ == &apos;__main__&apos;: server = SocketServer.ThreadingTCPServer((&apos;127.0.0.1&apos;,8009),MyServer) server.serve_forever() 123456789101112131415161718192021客户端#!/usr/bin/env python# -*- coding:utf-8 -*-import socketip_port = (&apos;127.0.0.1&apos;,8009)sk = socket.socket()sk.connect(ip_port)sk.settimeout(5)while True: data = sk.recv(1024) print &apos;receive:&apos;,data inp = raw_input(&apos;please input:&apos;) sk.sendall(inp) if inp == &apos;exit&apos;: breaksk.close() 2、ThreadingTCPServer源码剖析 ThreadingTCPServer的类图关系如下： 内部调用流程为： 启动服务端程序 执行 TCPServer.init 方法，创建服务端Socket对象并绑定 IP 和 端口 执行 BaseServer.init 方法，将自定义的继承自SocketServer.BaseRequestHandler 的类 MyRequestHandle赋值给 self.RequestHandlerClass 执行 BaseServer.server_forever 方法，While 循环一直监听是否有客户端请求到达 … 当客户端连接到达服务器 执行 ThreadingMixIn.process_request 方法，创建一个 “线程” 用来处理请求 执行 ThreadingMixIn.process_request_thread 方法 执行 BaseServer.finish_request 方法，执行 self.RequestHandlerClass() 即：执行 自定义 MyRequestHandler 的构造方法（自动调用基类BaseRequestHandler的构造方法，在该构造方法中又会调用 MyRequestHandler的handle方法） ThreadingTCPServer相关源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198BaseServerclass BaseServer: &quot;&quot;&quot;Base class for server classes. Methods for the caller: - __init__(server_address, RequestHandlerClass) - serve_forever(poll_interval=0.5) - shutdown() - handle_request() # if you do not use serve_forever() - fileno() -&gt; int # for select() Methods that may be overridden: - server_bind() - server_activate() - get_request() -&gt; request, client_address - handle_timeout() - verify_request(request, client_address) - server_close() - process_request(request, client_address) - shutdown_request(request) - close_request(request) - handle_error() Methods for derived classes: - finish_request(request, client_address) Class variables that may be overridden by derived classes or instances: - timeout - address_family - socket_type - allow_reuse_address Instance variables: - RequestHandlerClass - socket &quot;&quot;&quot; timeout = None def __init__(self, server_address, RequestHandlerClass): &quot;&quot;&quot;Constructor. May be extended, do not override.&quot;&quot;&quot; self.server_address = server_address self.RequestHandlerClass = RequestHandlerClass self.__is_shut_down = threading.Event() self.__shutdown_request = False def server_activate(self): &quot;&quot;&quot;Called by constructor to activate the server. May be overridden. &quot;&quot;&quot; pass def serve_forever(self, poll_interval=0.5): &quot;&quot;&quot;Handle one request at a time until shutdown. Polls for shutdown every poll_interval seconds. Ignores self.timeout. If you need to do periodic tasks, do them in another thread. &quot;&quot;&quot; self.__is_shut_down.clear() try: while not self.__shutdown_request: # XXX: Consider using another file descriptor or # connecting to the socket to wake this up instead of # polling. Polling reduces our responsiveness to a # shutdown request and wastes cpu at all other times. r, w, e = _eintr_retry(select.select, [self], [], [], poll_interval) if self in r: self._handle_request_noblock() finally: self.__shutdown_request = False self.__is_shut_down.set() def shutdown(self): &quot;&quot;&quot;Stops the serve_forever loop. Blocks until the loop has finished. This must be called while serve_forever() is running in another thread, or it will deadlock. &quot;&quot;&quot; self.__shutdown_request = True self.__is_shut_down.wait() # The distinction between handling, getting, processing and # finishing a request is fairly arbitrary. Remember: # # - handle_request() is the top-level call. It calls # select, get_request(), verify_request() and process_request() # - get_request() is different for stream or datagram sockets # - process_request() is the place that may fork a new process # or create a new thread to finish the request # - finish_request() instantiates the request handler class; # this constructor will handle the request all by itself def handle_request(self): &quot;&quot;&quot;Handle one request, possibly blocking. Respects self.timeout. &quot;&quot;&quot; # Support people who used socket.settimeout() to escape # handle_request before self.timeout was available. timeout = self.socket.gettimeout() if timeout is None: timeout = self.timeout elif self.timeout is not None: timeout = min(timeout, self.timeout) fd_sets = _eintr_retry(select.select, [self], [], [], timeout) if not fd_sets[0]: self.handle_timeout() return self._handle_request_noblock() def _handle_request_noblock(self): &quot;&quot;&quot;Handle one request, without blocking. I assume that select.select has returned that the socket is readable before this function was called, so there should be no risk of blocking in get_request(). &quot;&quot;&quot; try: request, client_address = self.get_request() except socket.error: return if self.verify_request(request, client_address): try: self.process_request(request, client_address) except: self.handle_error(request, client_address) self.shutdown_request(request) def handle_timeout(self): &quot;&quot;&quot;Called if no new request arrives within self.timeout. Overridden by ForkingMixIn. &quot;&quot;&quot; pass def verify_request(self, request, client_address): &quot;&quot;&quot;Verify the request. May be overridden. Return True if we should proceed with this request. &quot;&quot;&quot; return True def process_request(self, request, client_address): &quot;&quot;&quot;Call finish_request. Overridden by ForkingMixIn and ThreadingMixIn. &quot;&quot;&quot; self.finish_request(request, client_address) self.shutdown_request(request) def server_close(self): &quot;&quot;&quot;Called to clean-up the server. May be overridden. &quot;&quot;&quot; pass def finish_request(self, request, client_address): &quot;&quot;&quot;Finish one request by instantiating RequestHandlerClass.&quot;&quot;&quot; self.RequestHandlerClass(request, client_address, self) def shutdown_request(self, request): &quot;&quot;&quot;Called to shutdown and close an individual request.&quot;&quot;&quot; self.close_request(request) def close_request(self, request): &quot;&quot;&quot;Called to clean up an individual request.&quot;&quot;&quot; pass def handle_error(self, request, client_address): &quot;&quot;&quot;Handle an error gracefully. May be overridden. The default is to print a traceback and continue. &quot;&quot;&quot; print &apos;-&apos;*40 print &apos;Exception happened during processing of request from&apos;, print client_address import traceback traceback.print_exc() # XXX But this goes to stderr! print &apos;-&apos;*40 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126TCPServerclass TCPServer(BaseServer): &quot;&quot;&quot;Base class for various socket-based server classes. Defaults to synchronous IP stream (i.e., TCP). Methods for the caller: - __init__(server_address, RequestHandlerClass, bind_and_activate=True) - serve_forever(poll_interval=0.5) - shutdown() - handle_request() # if you don&apos;t use serve_forever() - fileno() -&gt; int # for select() Methods that may be overridden: - server_bind() - server_activate() - get_request() -&gt; request, client_address - handle_timeout() - verify_request(request, client_address) - process_request(request, client_address) - shutdown_request(request) - close_request(request) - handle_error() Methods for derived classes: - finish_request(request, client_address) Class variables that may be overridden by derived classes or instances: - timeout - address_family - socket_type - request_queue_size (only for stream sockets) - allow_reuse_address Instance variables: - server_address - RequestHandlerClass - socket &quot;&quot;&quot; address_family = socket.AF_INET socket_type = socket.SOCK_STREAM request_queue_size = 5 allow_reuse_address = False def __init__(self, server_address, RequestHandlerClass, bind_and_activate=True): &quot;&quot;&quot;Constructor. May be extended, do not override.&quot;&quot;&quot; BaseServer.__init__(self, server_address, RequestHandlerClass) self.socket = socket.socket(self.address_family, self.socket_type) if bind_and_activate: try: self.server_bind() self.server_activate() except: self.server_close() raise def server_bind(self): &quot;&quot;&quot;Called by constructor to bind the socket. May be overridden. &quot;&quot;&quot; if self.allow_reuse_address: self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) self.socket.bind(self.server_address) self.server_address = self.socket.getsockname() def server_activate(self): &quot;&quot;&quot;Called by constructor to activate the server. May be overridden. &quot;&quot;&quot; self.socket.listen(self.request_queue_size) def server_close(self): &quot;&quot;&quot;Called to clean-up the server. May be overridden. &quot;&quot;&quot; self.socket.close() def fileno(self): &quot;&quot;&quot;Return socket file number. Interface required by select(). &quot;&quot;&quot; return self.socket.fileno() def get_request(self): &quot;&quot;&quot;Get the request and client address from the socket. May be overridden. &quot;&quot;&quot; return self.socket.accept() def shutdown_request(self, request): &quot;&quot;&quot;Called to shutdown and close an individual request.&quot;&quot;&quot; try: #explicitly shutdown. socket.close() merely releases #the socket and waits for GC to perform the actual close. request.shutdown(socket.SHUT_WR) except socket.error: pass #some platforms may raise ENOTCONN here self.close_request(request) def close_request(self, request): &quot;&quot;&quot;Called to clean up an individual request.&quot;&quot;&quot; request.close() 12345678910111213141516171819202122232425262728ThreadingMixInclass ThreadingMixIn: &quot;&quot;&quot;Mix-in class to handle each request in a new thread.&quot;&quot;&quot; # Decides how threads will act upon termination of the # main process daemon_threads = False def process_request_thread(self, request, client_address): &quot;&quot;&quot;Same as in BaseServer but as a thread. In addition, exception handling is done here. &quot;&quot;&quot; try: self.finish_request(request, client_address) self.shutdown_request(request) except: self.handle_error(request, client_address) self.shutdown_request(request) def process_request(self, request, client_address): &quot;&quot;&quot;Start a new thread to process the request.&quot;&quot;&quot; t = threading.Thread(target = self.process_request_thread, args = (request, client_address)) t.daemon = self.daemon_threads t.start() 123ThreadingTCPServerclass ThreadingTCPServer(ThreadingMixIn, TCPServer): pass RequestHandler相关源码 123456789101112131415161718192021222324252627282930313233343536373839SocketServer.BaseRequestHandlerclass BaseRequestHandler: &quot;&quot;&quot;Base class for request handler classes. This class is instantiated for each request to be handled. The constructor sets the instance variables request, client_address and server, and then calls the handle() method. To implement a specific service, all you need to do is to derive a class which defines a handle() method. The handle() method can find the request as self.request, the client address as self.client_address, and the server (in case it needs access to per-server information) as self.server. Since a separate instance is created for each request, the handle() method can define arbitrary other instance variariables. &quot;&quot;&quot; def __init__(self, request, client_address, server): self.request = request self.client_address = client_address self.server = server self.setup() try: self.handle() finally: self.finish() def setup(self): pass def handle(self): pass def finish(self): pass 实例： 1234567891011121314151617181920212223242526服务端#!/usr/bin/env python# -*- coding:utf-8 -*-import SocketServerclass MyServer(SocketServer.BaseRequestHandler): def handle(self): # print self.request,self.client_address,self.server conn = self.request conn.sendall(&apos;欢迎致电 10086，请输入1xxx,0转人工服务.&apos;) Flag = True while Flag: data = conn.recv(1024) if data == &apos;exit&apos;: Flag = False elif data == &apos;0&apos;: conn.sendall(&apos;通过可能会被录音.balabala一大推&apos;) else: conn.sendall(&apos;请重新输入.&apos;)if __name__ == &apos;__main__&apos;: server = SocketServer.ThreadingTCPServer((&apos;127.0.0.1&apos;,8009),MyServer) server.serve_forever() 12345678910111213141516171819202122客户端#!/usr/bin/env python# -*- coding:utf-8 -*-import socketip_port = (&apos;127.0.0.1&apos;,8009)sk = socket.socket()sk.connect(ip_port)sk.settimeout(5)while True: data = sk.recv(1024) print &apos;receive:&apos;,data inp = raw_input(&apos;please input:&apos;) sk.sendall(inp) if inp == &apos;exit&apos;: breaksk.close() 源码精简： 12345678910111213141516171819202122232425262728293031323334import socketimport threadingimport selectdef process(request, client_address): print request,client_address conn = request conn.sendall(&apos;欢迎致电 10086，请输入1xxx,0转人工服务.&apos;) flag = True while flag: data = conn.recv(1024) if data == &apos;exit&apos;: flag = False elif data == &apos;0&apos;: conn.sendall(&apos;通过可能会被录音.balabala一大推&apos;) else: conn.sendall(&apos;请重新输入.&apos;)sk = socket.socket(socket.AF_INET, socket.SOCK_STREAM)sk.bind((&apos;127.0.0.1&apos;,8002))sk.listen(5)while True: r, w, e = select.select([sk,],[],[],1) print &apos;looping&apos; if sk in r: print &apos;get request&apos; request, client_address = sk.accept() t = threading.Thread(target=process, args=(request, client_address)) t.daemon = False t.start()sk.close() 如精简代码可以看出，SocketServer的ThreadingTCPServer之所以可以同时处理请求得益于 select 和 Threading 两个东西，其实本质上就是在服务器端为每一个客户端创建一个线程，当前线程用来处理对应客户端的请求，所以，可以支持同时n个客户端链接（长连接）。 ForkingTCPServer ForkingTCPServer和ThreadingTCPServer的使用和执行流程基本一致，只不过在内部分别为请求者建立 “线程” 和 “进程”。 基本使用： 123456789101112131415161718192021222324252627服务端#!/usr/bin/env python# -*- coding:utf-8 -*-import SocketServerclass MyServer(SocketServer.BaseRequestHandler): def handle(self): # print self.request,self.client_address,self.server conn = self.request conn.sendall(&apos;欢迎致电 10086，请输入1xxx,0转人工服务.&apos;) Flag = True while Flag: data = conn.recv(1024) if data == &apos;exit&apos;: Flag = False elif data == &apos;0&apos;: conn.sendall(&apos;通过可能会被录音.balabala一大推&apos;) else: conn.sendall(&apos;请重新输入.&apos;)if __name__ == &apos;__main__&apos;: server = SocketServer.ForkingTCPServer((&apos;127.0.0.1&apos;,8009),MyServer) server.serve_forever() 12345678910111213141516171819202122客户端#!/usr/bin/env python# -*- coding:utf-8 -*-import socketip_port = (&apos;127.0.0.1&apos;,8009)sk = socket.socket()sk.connect(ip_port)sk.settimeout(5)while True: data = sk.recv(1024) print &apos;receive:&apos;,data inp = raw_input(&apos;please input:&apos;) sk.sendall(inp) if inp == &apos;exit&apos;: breaksk.close() 以上ForkingTCPServer只是将 ThreadingTCPServer 实例中的代码： 123server = SocketServer.ThreadingTCPServer((&apos;127.0.0.1&apos;,8009),MyRequestHandler)变更为：server = SocketServer.ForkingTCPServer((&apos;127.0.0.1&apos;,8009),MyRequestHandler) SocketServer的ThreadingTCPServer之所以可以同时处理请求得益于 select 和 os.fork 两个东西，其实本质上就是在服务器端为每一个客户端创建一个进程，当前新创建的进程用来处理对应客户端的请求，所以，可以支持同时n个客户端链接（长连接）。 源码剖析参考 ThreadingTCPServer TwistedTwisted是一个事件驱动的网络框架，其中包含了诸多功能，例如：网络协议、线程、数据库管理、网络操作、电子邮件等。 事件驱动 简而言之，事件驱动分为二个部分：第一，注册事件；第二，触发事件。 自定义事件驱动框架，命名为：“弑君者”： 12345678910111213141516171819202122最牛逼的事件驱动框架#!/usr/bin/env python# -*- coding:utf-8 -*-# event_drive.pyevent_list = []def run(): for event in event_list: obj = event() obj.execute()class BaseHandler(object): &quot;&quot;&quot; 用户必须继承该类，从而规范所有类的方法（类似于接口的功能） &quot;&quot;&quot; def execute(self): raise Exception(&apos;you must overwrite execute&apos;) 程序员使用“弑君者框架”： 1234567891011121314#!/usr/bin/env python# -*- coding:utf-8 -*-from source import event_driveclass MyHandler(event_drive.BaseHandler): def execute(self): print &apos;event-drive execute MyHandler&apos;event_drive.event_list.append(MyHandler)event_drive.run() 如上述代码，事件驱动只不过是框架规定了执行顺序，程序员在使用框架时，可以向原执行顺序中注册“事件”，从而在框架执行时可以出发已注册的“事件”。 基于事件驱动Socket 12345678910111213141516171819#!/usr/bin/env python# -*- coding:utf-8 -*- from twisted.internet import protocolfrom twisted.internet import reactor class Echo(protocol.Protocol): def dataReceived(self, data): self.transport.write(data) def main(): factory = protocol.ServerFactory() factory.protocol = Echo reactor.listenTCP(8000,factory) reactor.run() if __name__ == &apos;__main__&apos;: main() 程序执行流程： 运行服务端程序 创建Protocol的派生类Echo 创建ServerFactory对象，并将Echo类封装到其protocol字段中 执行reactor的 listenTCP 方法，内部使用 tcp.Port 创建socket server对象，并将该对象添加到了 reactor的set类型的字段 _read 中 执行reactor的 run 方法，内部执行 while 循环，并通过 select 来监视 _read 中文件描述符是否有变化，循环中… 客户端请求到达 执行reactor的 _doReadOrWrite 方法，其内部通过反射调用 tcp.Port 类的 doRead 方法，内部 accept 客户端连接并创建Server对象实例（用于封装客户端socket信息）和 创建 Echo 对象实例（用于处理请求） ，然后调用 Echo 对象实例的 makeConnection 方法，创建连接。 执行 tcp.Server 类的 doRead 方法，读取数据， 执行 tcp.Server 类的 _dataReceived 方法，如果读取数据内容为空（关闭链接），否则，出发 Echo 的 dataReceived 方法 执行 Echo 的 dataReceived 方法 从源码可以看出，上述实例本质上使用了事件驱动的方法 和 IO多路复用的机制来进行Socket的处理。 12345678910111213141516171819202122232425262728293031323334353637383940异步IO操作#!/usr/bin/env python# -*- coding:utf-8 -*-from twisted.internet import reactor, protocolfrom twisted.web.client import getPagefrom twisted.internet import reactorimport timeclass Echo(protocol.Protocol): def dataReceived(self, data): deferred1 = getPage(&apos;http://cnblogs.com&apos;) deferred1.addCallback(self.printContents) deferred2 = getPage(&apos;http://baidu.com&apos;) deferred2.addCallback(self.printContents) for i in range(2): time.sleep(1) print &apos;execute &apos;,i def execute(self,data): self.transport.write(data) def printContents(self,content): print len(content),content[0:100],time.time()def main(): factory = protocol.ServerFactory() factory.protocol = Echo reactor.listenTCP(8000,factory) reactor.run()if __name__ == &apos;__main__&apos;: main() 更多请见： https://twistedmatrix.com/trac http://twistedmatrix.com/documents/current/api/ 出处：http://www.cnblogs.com/wupeiqi/]]></content>
      <categories>
        <category>python高级篇</category>
      </categories>
      <tags>
        <tag>python高级篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测试网络带宽的命令行界面]]></title>
    <url>%2F2019%2F09%2F03%2F%E6%B5%8B%E8%AF%95%E7%BD%91%E7%BB%9C%E5%B8%A6%E5%AE%BD%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%95%8C%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[测试网络带宽的命令行界面参考github地址:https://github.com/sivel/speedtest-cli 一、安装speedtest-clispeedtest-cli是一个用Python编写的轻量级Linux命令行工具，在Python2.4至3.4版本下均可运行。它基于Speedtest.net的基础架构来测量网络的上/下行速率。安装speedtest-cli很简单——只需要下载其Python脚本文件。网上的教程非常古老，用了只会报过期脚本 ，更新如下 123456789wget https://github.com/sivel/speedtest-cli/blob/master/speedtest.pychmod a+rx speedtest.pymv speedtest.py /usr/local/bin/speedtest-clichown root:root /usr/local/bin/speedtest-clispeedtest-cli 二、使用speedtest-cli测试网速使用speedtest-cli命令也很简单，它不需要任何参数即可工作。 1[root@localhost temp]# speedtest-cli 输入这个命令后，它会自动发现离你最近的Speedtest.net服务器（地理距离），然后打印出测试的网络上/下行速率。 12345678910[root@localhost temp]# speedtest-cli Retrieving speedtest.net configuration… Retrieving speedtest.net server list… Testing from China Telecom (219.135.214.145)… Selecting best server based on latency… Hosted by CTM Internet Services (Macau) [106.48 km]: 55.974 ms Testing download speed…………………………………. Download: 3.15 Mbit/s Testing upload speed………………………………………….. Upload: 0.58 Mbit/s 测试结果说明：上行为 0.58Mbit/s 下行为 3.15 Mbit/s]]></content>
      <categories>
        <category>测试工具</category>
      </categories>
      <tags>
        <tag>测试工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[漫话:什么是IO中的阻塞非阻塞同步异步]]></title>
    <url>%2F2019%2F08%2F29%2F%E6%BC%AB%E8%AF%9D%3A%E4%BB%80%E4%B9%88%E6%98%AFIO%E4%B8%AD%E7%9A%84%E9%98%BB%E5%A1%9E%E9%9D%9E%E9%98%BB%E5%A1%9E%E5%90%8C%E6%AD%A5%E5%BC%82%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[漫话：什么是IO中的阻塞、非阻塞、同步、异步？ 周末在家加班，正在疯狂的撸代码，女朋友很开心的跑过来，手里拿着他刚刚画好的一副漫画。 同步、异步、阻塞、非阻塞都是和IO（输入输出）有关的概念。最简单的文件读取就是IO操作。而在文件读取这件事儿上，可以有多种方式。 什么是同步和异步说到烧水，我们都是通过热水壶来烧水的。在很久之前，科技还没有这么发达的时候，如果我们要烧水，需要把水壶放到火炉上，我们通过观察水壶内的水的沸腾程度来判断水有没有烧开。 随着科技的发展，现在市面上的水壶都有了提醒功能，当我们把水壶插电之后，水壶水烧开之后会通过声音提醒我们水开了。 对于烧水这件事儿来说，传统水壶的烧水就是同步的，高科技水壶的烧水就是异步的。 同步请求，A调用B，B的处理是同步的，在处理完之前他不会通知A，只有处理完之后才会明确的通知A。 异步请求，A调用B，B的处理是异步的，B在接到请求后先告诉A我已经接到请求了，然后异步去处理，处理完之后通过回调等方式再通知A。 所以说，同步和异步最大的区别就是被调用方的执行方式和返回时机。同步指的是被调用方做完事情之后再返回，异步指的是被调用方先返回，然后再做事情，做完之后再想办法通知调用方。 什么是阻塞和非阻塞还是那个烧水的例子，当你把水放到水壶里面，按下开关后，你可以坐在水壶前面，别的事情什么都不做，一直等着水烧好。你还可以先去客厅看电视，等着水开就好了。 对于你来说，坐在水壶前面等就是阻塞的，去客厅看电视等着水开就是非阻塞的。 阻塞请求，A调用B，A一直等着B的返回，别的事情什么也不干。 非阻塞请求，A调用B，A不用一直等着B的返回，先去忙别的事情了。 所以说，阻塞和非阻塞最大的区别就是在被调用方返回结果之前的这段时间内，调用方是否一直等待。阻塞指的是调用方一直等待别的事情什么都不做。非阻塞指的是调用方先去忙别的事情。 阻塞、非阻塞和同步、异步的区别首先，前面已经提到过，阻塞、非阻塞和同步、异步其实针对的对象是不一样的。阻塞、非阻塞说的是调用者，同步、异步说的是被调用者。 有人认为阻塞和同步是一回事儿，非阻塞和异步是一回事。但是这是不对的。 先来看同步场景中是如何包含阻塞和非阻塞情况的。 我们是用传统的水壶烧水。在水烧开之前我们一直做在水壶前面，等着水开。这就是阻塞的。 我们是用传统的水壶烧水。在水烧开之前我们先去客厅看电视了，但是水壶不会主动通知我们，需要我们时不时的去厨房看一下水有没有烧开。这就是非阻塞的。 再来看异步场景中是如何包含阻塞和非阻塞情况的。 我们是用带有提醒功能的水壶烧水。在水烧发出提醒之前我们一直做在水壶前面，等着水开。这就是阻塞的。 我们是用带有提醒功能的水壶烧水。在水烧发出提醒之前我们先去客厅看电视了，等水壶发出声音提醒我们。这就是非阻塞的。 Java中的三种IO模型在Java语言中，一共提供了三种IO模型，分别是阻塞IO（BIO）、非阻塞IO（NIO）、异步IO（AIO）。 这里面的BIO和NIO都是同步的IO模型，即同步阻塞IO和同步非阻塞IO，异步IO指的是异步非阻塞IO。 BIO （Blocking I/O）：同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。 NIO （New I/O）：同时支持阻塞与非阻塞模式，但主要是使用同步非阻塞IO。 AIO （Asynchronous I/O）：异步非阻塞I/O模型。 BIO （Blocking I/O）：有一排水壶在烧开水，BIO的工作模式就是，叫一个线程停留在一个水壶那，直到这个水壶烧开，才去处理下一个水壶。但是实际上线程在等待水壶烧开的时间段什么都没有做。 NIO （New I/O）：NIO的做法是叫一个线程不断的轮询每个水壶的状态，看看是否有水壶的状态发生了改变，从而进行下一步的操作。 AIO （ Asynchronous I/O）：为每个水壶上面装了一个开关，水烧开之后，水壶会自动通知我水烧开了。 滴滴滴滴，这时候水壶响了，打断了女朋友的发问。女朋友去拿来烧好的热水，给我泡了一杯咖啡。 作者：漫话编程链接：https://juejin.im/post/5b94e2995188255c5c45d0ec]]></content>
      <categories>
        <category>python高级篇</category>
      </categories>
      <tags>
        <tag>python高级篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP中GET与POST的区别]]></title>
    <url>%2F2019%2F08%2F29%2FHTTP%E4%B8%ADGET%E4%B8%8EPOST%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[99%的人都理解错了HTTP中GET与POST的区别GET和POST是HTTP请求的两种基本方法，要说它们的区别，接触过WEB开发的人都能说出一二。 最直观的区别就是GET把参数包含在URL中，POST通过request body传递参数。 你可能自己写过无数个GET和POST请求，或者已经看过很多权威网站总结出的他们的区别，你非常清楚知道什么时候该用什么。 当你在面试中被问到这个问题，你的内心充满了自信和喜悦。 你轻轻松松的给出了一个“标准答案”： GET在浏览器回退时是无害的，而POST会再次提交请求。 GET产生的URL地址可以被Bookmark，而POST不可以。 GET请求会被浏览器主动cache，而POST不会，除非手动设置。 GET请求只能进行url编码，而POST支持多种编码方式。 GET请求参数会被完整保留在浏览器历史记录里，而POST中的参数不会被保留。 GET请求在URL中传送的参数是有长度限制的，而POST么有。 对参数的数据类型，GET只接受ASCII字符，而POST没有限制。 GET比POST更不安全，因为参数直接暴露在URL上，所以不能用来传递敏感信息。 GET参数通过URL传递，POST放在Request body中。 （本标准答案参考自w3schools） “很遗憾，这不是我们要的回答！” 请告诉我真相。。。 如果我告诉你GET和POST本质上没有区别你信吗？ 让我们扒下GET和POST的外衣，坦诚相见吧！ GET和POST是什么？HTTP协议中的两种发送请求的方法。 HTTP是什么？HTTP是基于TCP/IP的关于数据如何在万维网中如何通信的协议。 HTTP的底层是TCP/IP。所以GET和POST的底层也是TCP/IP，也就是说，GET/POST都是TCP链接。GET和POST能做的事情是一样一样的。你要给GET加上request body，给POST带上url参数，技术上是完全行的通的。 那么，“标准答案”里的那些区别是怎么回事？ 在我大万维网世界中，TCP就像汽车，我们用TCP来运输数据，它很可靠，从来不会发生丢件少件的现象。但是如果路上跑的全是看起来一模一样的汽车，那这个世界看起来是一团混乱，送急件的汽车可能被前面满载货物的汽车拦堵在路上，整个交通系统一定会瘫痪。为了避免这种情况发生，交通规则HTTP诞生了。HTTP给汽车运输设定了好几个服务类别，有GET, POST, PUT, DELETE等等，HTTP规定，当执行GET请求的时候，要给汽车贴上GET的标签（设置method为GET），而且要求把传送的数据放在车顶上（url中）以方便记录。如果是POST请求，就要在车上贴上POST的标签，并把货物放在车厢里。当然，你也可以在GET的时候往车厢内偷偷藏点货物，但是这是很不光彩；也可以在POST的时候在车顶上也放一些数据，让人觉得傻乎乎的。HTTP只是个行为准则，而TCP才是GET和POST怎么实现的基本。 但是，我们只看到HTTP对GET和POST参数的传送渠道（url还是requrest body）提出了要求。“标准答案”里关于参数大小的限制又是从哪来的呢？ 在我大万维网世界中，还有另一个重要的角色：运输公司。不同的浏览器（发起http请求）和服务器（接受http请求）就是不同的运输公司。 虽然理论上，你可以在车顶上无限的堆货物（url中无限加参数）。但是运输公司可不傻，装货和卸货也是有很大成本的，他们会限制单次运输量来控制风险，数据量太大对浏览器和服务器都是很大负担。业界不成文的规定是，（大多数）浏览器通常都会限制url长度在2K个字节，而（大多数）服务器最多处理64K大小的url。超过的部分，恕不处理。如果你用GET服务，在request body偷偷藏了数据，不同服务器的处理方式也是不同的，有些服务器会帮你卸货，读出数据，有些服务器直接忽略，所以，虽然GET可以带request body，也不能保证一定能被接收到哦。 好了，现在你知道，GET和POST本质上就是TCP链接，并无差别。但是由于HTTP的规定和浏览器/服务器的限制，导致他们在应用过程中体现出一些不同。 你以为本文就这么结束了？ 我们的大BOSS还等着出场呢。。。 这位BOSS有多神秘？当你试图在网上找“GET和POST的区别”的时候，那些你会看到的搜索结果里，从没有提到他。他究竟是什么呢。。。 GET和POST还有一个重大区别，简单的说： GET产生一个TCP数据包；POST产生两个TCP数据包。 长的说： 对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200（返回数据）； 而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok（返回数据）。 也就是说，GET只需要汽车跑一趟就把货送到了，而POST得跑两趟，第一趟，先去和服务器打个招呼“嗨，我等下要送一批货来，你们打开门迎接我”，然后再回头把货送过去。 因为POST需要两步，时间上消耗的要多一点，看起来GET比POST更有效。因此Yahoo团队有推荐用GET替换POST来优化网站性能。但这是一个坑！跳入需谨慎。为什么？ \1. GET与POST都有自己的语义，不能随便混用。 \2. 据研究，在网络环境好的情况下，发一次包的时间和发两次包的时间差别基本可以无视。而在网络环境差的情况下，两次包的TCP在验证数据包完整性上，有非常大的优点。 \3. 并不是所有浏览器都会在POST中发送两次包，Firefox就只发送一次。 现在，当面试官再问你“GET与POST的区别”的时候，你的内心是不是这样的？ （转自微信公众号WebTechGarden）]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[域名知识一]]></title>
    <url>%2F2019%2F08%2F29%2F%E5%9F%9F%E5%90%8D%E7%9F%A5%E8%AF%86%E4%B8%80%2F</url>
    <content type="text"><![CDATA[域名知识互联网中的地址是数字的 IP 地址，例如 61.135.169.125 就是百度的官网地址之一，如果每次访问百度都需要输入 IP 的话，估计到今天互联网都还没有走出鸿蒙阶段。 在网络发展历史上，最开始确实就是直接使用 IP 地址来访问远程主机的。早期联网的每台计算机都是采用主机文件（即我们俗称的 hosts 文件）来进行地址配置和解析的，后来联网机器越来越多，主机文件的更新和同步就成了很大的问题。于是，1983 年保罗·莫卡派乔斯发明了域名解析服务和域名系统，在 1985 年 1 月 1 日，世界上第一个域名 nordu.net 才被注册成功。 域名比 IP 地址更容易记忆，本质上只是为数字化的互联网资源提供了易于记忆的别名，就像在北京提起「故宫博物院」就都知道指的是「东城区景山前街 4 号」的那个大院子一样。如果把 IP 地址看成电话号码，那域名系统就是通讯录。我们在通讯录里保存了朋友和家人的信息，每次通过名字找到某人打电话的时候，通讯录就会查出与之关联的电话号码，然后拨号过去。我们可能记不下多少完整的电话号码，但是联系人的名字却是一定记得的。 既然「域名」只是一个别名，单凭这一个名字我们并不能访问到正确的地址，只有能将域名解析成实际的网络地址，网络访问才能成功。这种解析工作由专门的「域名系统」（Domain Name System，简称 DNS）完成，DNS 也是互联网的核心基础服务之一。 域名解析是怎么完成的DNS 解析的过程是什么样子的呢？在开始这个问题之前，我们先看一看域名的层次结构。 域名的层级结构在讨论域名的时候，我们经常听到有人说「顶级域名」、「一级域名」、「二级域名」等概念，域名级别究竟是怎么划分的呢？ 根域名。还是以百度为例，通过一些域名解析工具，我们可以看到百度官网域名显示为 www.baidu.com.，细心的人会注意到，这里最后有一个 .，这不是 bug，而是所有域名的尾部都有一个根域名。www.baidu.com 真正的域名是 www.baidu.com.root，简写为www.baidu.com.，又因为根域名 .root 对于所有域名都是一样的，所以平时是省略的，最终就变成了我们常见的样子。 根域名的下一级叫做顶级域名（top-level domain，缩写为 TLD），也叫做一级域名，常见的如 .com / .net / .org / .cn 等等，他们就是顶级域名。 再下一级叫做二级域名（second-level domain，缩写为 SLD），比如 baidu.com。这是我们能够购买和注册的最高级域名。 次级域名之下，就是主机名（host），也可以称为三级域名，比如 www.baidu.com，由此往下，基本上 N 级域名就是在 N-1 级域名前追加一级。 总结一下，常见的域名层级结构如下： 123主机名.次级域名.顶级域名.根域名# iewww.baidu.com.root 一般来说我们购买一个域名就是购买一个二级域名（SLD）的管理权（如 leancloud.cn），有了这个管理权我们就可以随意设置三级、四级域名了。 域名解析的过程与域名的分级结构对应，DNS 系统也是一个树状结构，不同级别的域名由不同的域名服务器来解析，整个过程是一个「层级式」的。 层级式域名解析体系的第一层就是根域名服务器，全世界 IPv4 根域名服务器只有 13 台（名字分别为 A 至 M），1 个为主根服务器在美国，其余 12 个均为辅根服务器，它们负责管理世界各国的域名信息。在根服务器下面是顶级域名服务器，即相关国家域名管理机构的数据库，如中国互联网络信息中心（CNNIC）。然后是再下一级的权威域名服务器和 ISP 的缓存服务器。 一个域名必须首先经过根数据库的解析后，才能转到顶级域名服务器进行解析，这一点与生活中问路的情形有几分相似。 假设北京市设立了一个专门的「道路咨询局」，里面设置了局长、部长、处长、科员好几个级别的公务员，不同的部门、科室、人员负责解答不同区域的道路问题。这里的人都有一个共同特点，信奉「好记性不如烂笔头」的哲理，喜欢将自己了解到的信息记录到笔记本上。但是有一点遗憾的是，他们写字用的墨水只有一种，叫「魔术墨水」，初写字迹浓厚，之后会慢慢变淡，1 小时之后则会完全消失。道路咨询局门口还有一个门卫大爷，所有的人要问路都需要通过他来传达和回复，市民并不能进入办公楼。 如果市民 A 先生来找门卫大爷询问「北海公园」的地址，门卫大爷会先看一下自己的笔记本，找找看之前有没有人问过北海公园，如果没有，他就会拨打内线去找局长求助。局长说北海是西城区，你去问负责西城区道路信息的赵部长吧。门卫大爷又去问赵部长，赵部长查了一下，说这个地址你去问负责核心区的钱处长吧。门卫大爷又给钱处长打过去电话，钱处长说这个地址我也不掌握啊，你去问一下负责景山片区的科员小孙吧。门卫大爷从小孙那里终于知道了北海公园地址，他赶紧记到自己的小本本上，然后把结果告诉了市民 A 先生。接下来一小时内，如果还有市民 B 先生再来问北海公园的话，门卫大爷就直接用笔记本上记载的结果回复了。当然，如果市民 C 女士过来问别的地址的话，门卫大爷就要把处理 A 先生问询的流程再走一遍了。 分级查询的实例现在我们来看一个实际的例子。如果我们在浏览器中输入 https://news.qq.com，那浏览器会从接收到的 URL 中抽取出域名字段（news.qq.com），然后将它传给 DNS 客户端（操作系统提供）来解析。 首先我们说明一下本机 DNS 配置（就是 /etc/resolv.conf 文件，里面指定了本地 DNS 服务器的地址，Windows 系统可能会有所不同）： 123$ cat /etc/resolv.conf nameserver 202.106.0.20nameserver 202.106.196.115 然后我们用 dig 这个工具查看一下 news.qq.com 的解析结果（其中中文部分是解释说明）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748$ dig news.qq.com; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; news.qq.com这是 dig 程序的版本号与要查询的域名;; global options: +cmd;; Got answer:以下是要获取的内容。;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 47559;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0这个是返回应答的头部信息：1. opcode：操作码，QUERY 代表查询操作；2. status: 状态，NOERROR 代表没有错误;3. id：编号，在 DNS 协议中通过编号匹配返回和查询；4. flags: 标志，含义如下: - qr：query，查询标志，代表是查询操作 - rd：recursion desired，代表希望进行递归查询操作; - ra：recursive available，代表查询的服务器支持递归查询操作;5. QUERY 查询数，与下面 QUESTION SECTION 的记录数一一对应；6. ANSWER 结果数，与下面的 ANSWER SECTION 的记录数一一对应；7. AUTHORITY 权威回复数，如果查询结果由管理域名的域名服务器而不是缓存服务器提供的，则称为权威回复。 0 表示所有结果都不是权威回复；8. ADDITIONAL 额外记录数；;; QUESTION SECTION:;news.qq.com. IN A查询部分,从左到右部分意义如下:1、要查询的域名；2、要查询信息的类别，IN 代表类别为 IP 协议，即 Internet。3、查询的记录类型，A 记录(Address)代表要查询 IPv4 地址。;; ANSWER SECTION:news.qq.com. 136 IN CNAME https.qq.com.https.qq.com. 476 IN A 125.39.52.26回应部分，从左到右各部分意义：1、对应的域名2、TTL，time to live，缓存时间，单位秒，代表缓存域名服务器可以在缓存中保存的期限。3、查询信息的类别4、查询的记录类型，CNAME 表示别名记录，A 记录(Address)代表 IPv4 地址。5、域名对应的 ip 地址。;; Query time: 56 msec;; SERVER: 202.106.0.20#53(202.106.0.20)查询使用的服务器地址和端口,其实就是本地 DNS 域名服务器;; WHEN: Thu Jul 11 15:59:37 CST 2019;; MSG SIZE rcvd: 65查询的时间与回应的大小，收到 65 字节的应答数据。 从这个应答可以看到，我们得到的结果不是权威回复，只是本地 DNS 服务器从缓存中给了应答。 接下来我们在 dig 命令中增加一个参数 +trace，看看完整的分级查询过程： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556$ dig +trace news.qq.com; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; +trace news.qq.com;; global options: +cmd. 432944 IN NS g.root-servers.net.. 432944 IN NS k.root-servers.net.. 432944 IN NS b.root-servers.net.. 432944 IN NS h.root-servers.net.. 432944 IN NS i.root-servers.net.. 432944 IN NS f.root-servers.net.. 432944 IN NS d.root-servers.net.. 432944 IN NS e.root-servers.net.. 432944 IN NS j.root-servers.net.. 432944 IN NS l.root-servers.net.. 432944 IN NS c.root-servers.net.. 432944 IN NS m.root-servers.net.. 432944 IN NS a.root-servers.net.;; Received 228 bytes from 202.106.0.20#53(202.106.0.20) in 45 ms这些就是神秘的根域名服务器，由本地 DNS 服务器返回了所有根域名服务器地址。com. 172800 IN NS g.gtld-servers.net.com. 172800 IN NS a.gtld-servers.net.com. 172800 IN NS b.gtld-servers.net.com. 172800 IN NS m.gtld-servers.net.com. 172800 IN NS d.gtld-servers.net.com. 172800 IN NS c.gtld-servers.net.com. 172800 IN NS j.gtld-servers.net.com. 172800 IN NS h.gtld-servers.net.com. 172800 IN NS f.gtld-servers.net.com. 172800 IN NS l.gtld-servers.net.com. 172800 IN NS e.gtld-servers.net.com. 172800 IN NS k.gtld-servers.net.com. 172800 IN NS i.gtld-servers.net.;; Received 1171 bytes from 192.36.148.17#53(i.root-servers.net) in 57 ms这里显示的是 .com 域名的 13 条 NS 记录，本地 DNS 服务器向这些顶级域名服务器发出查询请求，询问 qq.com 的 NS 记录。qq.com. 172800 IN NS ns1.qq.com.qq.com. 172800 IN NS ns2.qq.com.qq.com. 172800 IN NS ns3.qq.com.qq.com. 172800 IN NS ns4.qq.com.;; Received 805 bytes from 192.48.79.30#53(j.gtld-servers.net) in 331 ms这里显示的是 qq.com 的 4 条 NS 记录，由 j.gtld-servers.net 这台服务器最先返回。然后本地 DNS 服务器向这四台服务器查询下一级域名 news.qq.com 的 NS 记录。news.qq.com. 86400 IN NS ns-cnc1.qq.com.news.qq.com. 86400 IN NS ns-cnc2.qq.com.;; Received 180 bytes from 58.144.154.100#53(ns4.qq.com) in 37 ms这里显示的是 news.qq.com 的 NS 记录，它们是由上面的 ns4.qq.com 域名服务器返回的。然后本地 DNS 服务器向这两台机器查询 news.qq.com 的主机名。news.qq.com. 600 IN CNAME https.qq.com.https.qq.com. 600 IN A 125.39.52.26;; Received 76 bytes from 223.167.83.104#53(ns-cnc2.qq.com) in 29 ms这是上面的 ns-cnc2.qq.com 返回的最终查询结果：news.qq.com 是 https.qq.com 的别名，而 https.qq.com 的 A 记录地址是 125.39.52.26 实际的流程里面，本地 DNS 服务器相当于门卫大爷，根域名服务器相当于局长同志，其余以此类推。客户端与本地 DNS 服务器之间的查询叫递归查询，本地 DNS 服务器与其他域名服务器之间的查询就叫迭代查询。 域名记录的类型域名服务器之所以能知道域名与 IP 地址的映射信息，是因为我们在域名服务商那里提交了域名记录。购买了一个域名之后，我们需要在域名服务商那里设置域名解析的记录，域名服务商把这些记录推送到权威域名服务器，这样我们的域名才能正式生效。 在设置域名记录的时候，会遇到「A 记录」、「CNAME」 等不同类型，这正是前面做域名解析的时候我们碰到的结果。这些类型是什么意思，它们之间有什么区别呢？接下来我们看看常见的记录类型。 A 记录。A (Address) 记录用来直接指定主机名（或域名）对应的 IP 地址。主机名就是域名前缀，常见有如下几种： www：解析后的域名为 www.yourdomain.com，一般用于网站地址。 @：直接解析主域名。 *：泛解析，指将 *.yourdomain.com 解析到同一 IP。 CNAME 记录。CNAME 的全称是 Canonical Name，通常称别名记录。如果需要将域名指向另一个域名，再由另一个域名提供 IP 地址，就需要添加 CNAME 记录。 MX 记录。邮件交换记录，用于将以该域名为结尾的电子邮件指向对应的邮件服务器以进行处理。 NS 记录。域名服务器记录，如果需要把子域名交给其他 DNS 服务器解析，就需要添加 NS 记录。 AAAA 记录。用来指定主机名（或域名）对应的 IPv6 地址，不常用。 TXT 记录。可以填写任何东西，长度限制 255。绝大多数的 TXT 记录是用来做 SPF 记录（反垃圾邮件），MX 记录的作用是给寄信者指明某个域名的邮件服务器有哪些。SPF 的作用跟 MX 相反，它向收信者表明，哪些邮件服务器是经过某个域名认可会发送邮件的。 显性 URL。从一个地址 301 重定向（也叫「永久性转移」）到另一个地址的时候，就需要添加显性 URL 记录。 隐性 URL。从一个地址 302 跳转（也叫「临时跳转」）到另一个地址，需要添加隐性 URL 记录。它类似于显性 URL，区别在于隐性 URL 不会改变地址栏中的域名。 在填写各种记录的时候，我们还会碰到一个特殊的设置项——TTL，生存时间（Time To Live）。 TTL 表示解析记录在 DNS 服务器中的缓存时间，时间长度单位是秒，一般为3600秒。比如：在访问 news.qq.com时，如果在 DNS 服务器的缓存中没有该记录，就会向某个 NS 服务器发出请求，获得该记录后，该记录会在 DNS 服务器上保存 TTL 的时间长度，在 TTL 有效期内访问 news.qq.com，DNS 服务器会直接缓存中返回刚才的记录。 DNS 智能解析DNS 主要的工作就是完成域名到 IP 的映射，但是也不是简单到查查字典就可以搞定的程度。在设置 DNS 解析的时候，我们还有一些额外的需求，例如： 将一个域名解析到多个 IP 例如我们一个网站有多台前端机，希望用户访问的时候，可以随机分散到这些机器上，以增加网站承载能力。有一种解决的办法就是对同一个域名设置多条 A 记录，分别指定到不同的 IP 上。 根据特征差异将不同请求解析到不同 IP（智能解析） 国内互联网的架构其实远比我们想象的复杂，基本上还是根据运营商的不同切割成多个平行网络，只有在固定的几个节点这些平行网络才会有交叉。例如电信和联通之间的互联是通过「国家级互联网骨干直联点」接入的，目前我们一共建设了三批国家级互联网骨干直联点： 1.第一批 2001 年投入使用：北京，上海，广州 2.第二批 2014 年投入使用：成都，郑州，武汉，西安，沈阳，南京，重庆 3.第三批 2017 年投入使用：杭州，贵阳/贵安，福州 教育网目前还只能通过北上广三个点进行连接。这样的网络拓扑结构，给 DNS 解析带来了新的挑战。 传统 DNS 解析，不判断访问者来源，会随机选择其中一个 IP 地址返回给访问者。如果让电信用户使用了联通 IP 来访问网站，那结果自然不如使用电信 IP 访问来的快捷。而智能 DNS 解析，会判断访问者的来源特征，为不同的访问者返回不同的 IP 地址，能够减少解析时延，并提升网络访问速度。例如，国内某著名 DNS 服务商不光可以区分网络运营商，还可以根据访问者的地理位置来设置不同的解析线路，而且甚至还可以为搜索引擎设置特定的解析地址。 CNAME 和 A 记录区别 按照前面的解释，A 记录就是把一个域名解析到一个 IP 地址，而 CNAME 记录就是把一个域名解析到另外一个域名，其功能差不多。但是 CNAME 相当于将域名和 IP 地址之间加了一个中间层，可以带来很大的灵活性，特别是当你要使用但是并不拥有那些域名的时候。 例如我们使用 CDN 服务，服务商提供给我们的是一个 CNAME 地址，我们可以把自己的域名绑定到这一个地址上，这样万一以后服务商的 IP 地址更换了，我们自己的域名解析是不需要做任何变更的，只要服务商调整一下 CNAME 地址的解析结果，所有使用者都可以无感知的切换。 从 6 月底开始，LeanCloud 新推出了 绑定自定义域名 的功能，全面支持开发者设置自己的 API、文件、云引擎域名，也正是依赖于 CNAME 记录的这一特点来实现的。 DNS 污染与安全挑战DNS 是最早商用的大型分布式系统，虽然现在看起来已经很完备了，但是实际使用的时候，特别是国内复杂的网络环境，我们还是会遇到很多问题。 作为互联网早期产物，DNS 使用无连接的 UDP 协议虽然降低了开销也保证了高效的通信，但是没有太考虑安全问题。由于它使用目的端口为 53 的 UDP 明文进行通信，DNS 解析器识别是自己发出的数据包的唯一标准就是随机的源端口号，如果端口号匹配则认为是正确回复，而不会验证来源。所以也带来了诸如 DNS 欺骗、DNS Cache 污染、DNS 放大攻击等问题，同时给一些区域运营商带来了「商机」。 为此业界提出了 DNSSec（Domain Name System Security Extensions，也叫「DNS安全扩展」）机制，使用密码学方法，让客户端对域名来源身份进行验证，并且检查来自 DNS 域名服务器应答记录的完整性，以及验证是否在传输过程中被篡改过，等等一系列措施来保证数据通信的安全性。 作者：LeanCloud链接：https://juejin.im/post/5d37cf70e51d4510664d17d3]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[域名知识二]]></title>
    <url>%2F2019%2F08%2F29%2F%E5%9F%9F%E5%90%8D%E7%9F%A5%E8%AF%86%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[域名知识二 DNS 进行域名解析的过程： 客户端寻找本地 DNS 服务器，向它发出请求； 本地 DNS 再依照域名层次结构，向不同级别的域名服务器发出迭代查询请求。 所有这些请求都是通过无连接的 UDP 协议进行通信，DNS 服务器识别是自己发出的数据包的唯一标准就是随机的源端口号，如果端口号匹配则认为是正确回复。这样一种简单的方式在日益复杂的网络结构下，可能会出现各种说不清道不明的问题。 1. 域名劫持既然 DNS 响应数据包的源 IP 地址很容易仿冒或伪造，自然就有很多人开始利用这一漏洞，伪造权威域名服务器的应答，把目标网站域名解析到错误的地址，从而达到让用户无法访问目标网站的目的，这就是「域名劫持」。 这里很有代表性的例子就是防火长城（GFW）。我们国家依法对互联网进行管理和监控，为了封堵国外的非法内容，GFW 会对 UDP 53 端口上的所有请求进行检测，一经发现与黑名单关键词相匹配的域名查询请求，会马上伪装成目标域名的解析服务器返回虚假的查询结果。由于通常的域名查询没有任何认证机制，而且基于无连接不可靠的 UDP 协议，查询者只能接受最先到达的格式正确结果，并丢弃之后的结果。这样一来，如果我们直接访问一些国外的「非法」网站，拿到的就会是一个假的 IP 地址。 当然，也有一些黑客入侵导致的域名被劫持事件。黑客通过非法手段控制了域名管理密码和域名管理邮箱，然后将该域名的 NS 记录指向到黑客可以控制的 DNS 服务器，然后通过在该 DNS 服务器上添加虚假域名记录，从而所有到目标网站的流量全部导向黑客所指向的内容。2010 年 1 月 12 日针对百度的一次域名劫持事件在历史上影响巨大。当天，百度被自称是伊朗网军（Iranian Cyber Army）的黑客组织入侵，导致其首页瘫痪长达 8 小时，并随后引发中国黑客对多个伊朗政府网站以及伊朗广播大学网站的报复行为。 其实类似的事件还发生过很多次，不光百度，谷歌和微软他们的域名也曾经被劫持过，可以说互联网的背后一点都不像看起来风平浪静的样子。 2. 域名缓存污染如果没有上游的欺骗和黑客的破坏，LocalDNS 拿到正确的结果之后，大体上是可以正常服务的，但这里也仅仅只能说是「大体」上。LocalDNS 会把从权威域名服务器接收到的结果数据进行缓存，以便加速后续的解析流程。例如在有效期内再有人问门卫王大爷「北海公园」的地址，王大爷只需要查一眼自己的笔记本，就可以马上给出回答。这一设计看似 feature，但是在现实生活中有时候它会失效，反而带来危害。 首先很多时候运营商的缓存时间都不太靠谱，他们不会遵守权威 DNS 提供的 ttl（存活时间），而是统一设置一个固定的时间，所以通常我们改了一个域名的解析 IP 之后，会需要 0-48 小时（甚至更多）的时间才能让所有的客户端同步过来。而且但凡程序都会有 Bug，各运营商的运维水平也参差不齐，有时候还会因为缓存故障影响大面积的用户访问。例如门卫王大爷在查找地址的时候也可能会看走眼，对此我们也不能苛责；而更多的时候，在中国特色的互联网环境中，王大爷还有自己的一些「小算盘」。 我们的互联网看似四通八达，其实底层还是几个平行网络在有限的几个点铰接而成的，运营商总是喜欢缓存 DNS 结果，还有一些经济方面的考虑： 保证用户访问流量在本网内消化。国内的各互联网接入运营商，他们的带宽资源、网间结算费用、IDC 机房分布、网内 ICP 资源分布等存在较大差异，为了保证网内用户的访问质量，同时减少跨网结算，运营商在网内搭建了内容缓存服务器，通过把域名强行指向内容缓存服务器的 IP 地址，就实现了把本地本网流量完全留在了本地的目的； 推送广告。有部分区域运营商会把某些域名解析结果指向自己的内容缓存，并替换或者插入第三方广告联盟的广告，以此增加收入。。。 以上就是我们常说的「域名缓存污染」，它会导致终端用户访问目标网站时产生各种访问异常，或者夹杂莫名其妙的广告，这种异常在无线网络上更为常见。 3. 解析转发域名缓存是运营商「积极作为」带来的副问题，而运营商不作为有时也会引来麻烦。运营商的 LocalDNS 有时候还会偷懒，自身不进行域名递归解析，而简单把域名解析请求转发到其它运营商的递归 DNS 上去。 例如北京市「道路咨询局」有东西两个大门（相隔较远），分别由门卫王大爷和刘大爷把守，从道路咨询局去北海公园也有东西两条路。从东门进来的游客，他们会咨询王大爷，王大爷就会选择便捷一点的东边的路回复游客；从西门进来的游客，他们会咨询刘大爷，刘大爷会选择西边的路来回复游客。但是突然有一天，王大爷不想解答游客的问题了，他找到了一个便捷的办法，把所有问题都转到刘大爷那里去了，最后所有的游客都会选择从西边的路去北海公园。对于原本在东门的游客来说，可能就多走了一大段冤枉路。 实际的网络拓扑结构是很复杂的，为了尽可能覆盖更多用户，一个网站会接入多条运营商线路，DNS 那里则会根据请求者的特征为用户来选择最短访问路径（详见前一篇文章里的「DNS 智能解析」一节）。对这种转发的情况，可能最终的 DNS 会把转发 DNS 的 IP 当成访问者的来源 IP，这样极有可能一个电信用户最终访问到了联通的 IP，那么终端的访问速度可能就变慢了许多。还有一种解析错误是由于 LocalDNS 本地出口 NAT 配置错误导致的，这里不再赘述。 以 LeanCloud 为例，我们之前使用的域名是 avoscloud.com，由于这个域名存在敏感字符，所以某些区域解析经常出现问题，究其原因基本上都是运营商的 LocalDNS 导致的，而要解决好这些问题，则需要不断与运营商深度沟通或者找工信部投诉来解决，其流程之长和效率之低可想而知。这也就是我们后来切换到 leancloud.cn 这一域名的原因。换了域名之后，终端用户的连通性好了很多，但是还是不能 100% 解决所有网络问题，这时候怎么办呢？ DNSSECDNS 欺骗和污染的根本原因就是缺乏安全机制，所以自上世纪 90 年代起，人们就开始寻求这一问题的解决方案，最终 DNS 安全扩展 (DNS Security Extensions，DNSSEC) 应运而生。 基本原理DNSSEC 采用非对称加密的方式对 DNS 数据进行加密，加解密算法与 HTTPS 类似，但是与 HTTPS 协议不同的是，DNSSEC 并非对 DNS 查询和响应的数据包进行加密，而仅仅只对 DNS 数据（A 记录，CNAME 等等，统称为 Resource Record，缩写为 RR）进行签名，所以 DNSSEC 可以完全兼容 DNS。 为了支持非对称加密算法，DNSSEC 中增加了一个区（zone）的概念。域名系统每一级的权威域名服务器就是一个 zone，他们都配置有一个公私密钥对，这一点与 HTTPS 网站的 SSL 证书类似。权威域名服务器在返回应答数据的时候，除了通常的 Resource Record 之外，还会增加新的数据类型： RRSIG（Resource Record Signature）记录，存储 RR 集合的数字签名； DNSKEY（DNS Public Key）记录，存储当前 zone 的公钥； DS（Delegation Signer）记录，存储 DNSKEY 的散列值，用于验证 DNSKEY 的真实性； NSEC（Next Secure）记录，用于应答那些不存在的资源记录； 如此一来，DNSSEC 就可以额外提供三层安全保护： 数据完整性（data integrity）。DNSSEC 使用数字签名的技术，为每一个 RR 做签名产生 RRSIG，而接收方可以使用 Public Key + RRSIG 来反向验证数据是否被篡改。 来源可验证性（origin authentication of DNS data）。当数据完整性被验证之后，我们可以确认数据传输过程中没有被修改，而且确实由负责该域名的 DNS Server 提供，但是我们还不能确认这个 DNS Server 不是一个「中间人」。在 DNSSEC 中，每个 DNS Server 都需要将它的 Public Key 交由公正的第三方来保管，而这个公正第三方就是上一级的 DNS Server。回想一下我们之前说明的域名解析的树形结构，每一级 DNS Server 必须将自己的 Public Key（DNSKEY）做一个数字签名，存放到 Parent DNS Server，这就是 DS（Delegation Signer）记录。这样 Parent DNS Server 中的 DS 记录可以验证 Child DNS Server 的 DNSKEY 是否被修改，所以当我们相信 Parent Zone 的机器时，就可以信任 Child Zone 所询问得到的结果，而又该如何信任 Parent Zone 呢？就是信任 Parent of Parent Zone，如此层层确认，直到 Root Zone（毕竟这里节点有限，由专人管理，被侵入的可能性极小），这即是 DNSSEC 的「信任链」。 可验证之不存在性（authenticated denial of existence）。我们随便 ping 一个网址，得到「unknown host」的回应，我们又该如何相信这是确实不存在，而不是有人故意制造假象呢？在 DNSSEC 方案中，它将所有的 DNS 记录按照字母排序，而且在每两个记录之间加入一个 NSEC 记录并进行签名，当查询到不存在的网址时，DNSSEC 会传回对应的 NSEC 记录，查询者可以比对 NSEC 记录中前后对应的字母来确认是否真的不存在。 有兴趣的读者可以参考这篇文章，以了解更多细节。 验证 DNSSECdig 是一个非常强大的 DNS 查询工具，通过+dnssec 参数可以验证域名服务器和域名是否支持 DNSSec。例如，我们使用国外的 DNS 解析，如8.8.8.8，查询域名 paypal.com 是否支持 DNSSec，可以得到如下结果： 123456789101112131415161718$ dig @8.8.8.8 paypal.com +dnssec; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @8.8.8.8 paypal.com +dnssec; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 6628;; flags: qr rd ra ad; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags: do; udp: 512;; QUESTION SECTION:;paypal.com. IN A;; ANSWER SECTION:paypal.com. 9 IN A 64.4.250.36paypal.com. 9 IN A 64.4.250.37paypal.com. 9 IN RRSIG A 5 2 300 20190923004132 20190824003218 11811 paypal.com. E21d1Jc/fCdZneT9oC3xWgQ1gPBdO1v29LPNJw7CtydJVhsy3z3bs4U8 7vBSGScEIpmCkmbZxChW1h3UlZ++hAmCCRx+eZV7VvhynpPW30mvwjrk wx5PVxWhPAKiTs07i5h4ZgTcWwp/ZEQbvU0DEkTKlBs2SuGU6AWNgmgL jgU= 这里我们可以看到多出来的一条 RRSIG 记录，就是 DNSSEC 的签名数据。我们换到 114.114.114.114 的 DNS 服务器来查询 paypal.com 的域名，则得到如下结果： 1234567891011121314151617$ dig @114.114.114.114 paypal.com +dnssec; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @114.114.114.114 paypal.com +dnssec; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 17708;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 512;; QUESTION SECTION:;paypal.com. IN A;; ANSWER SECTION:paypal.com. 36 IN A 64.4.250.37paypal.com. 36 IN A 64.4.250.36 可以看到 114 的 DNS 服务器并不支持 DNSSEC，而且我们使用同样的命令，来查询 baidu.com、taobao.com 和 qq.com，就会发现他们都不支持 DNSSEC。为什么国内的大厂都不跟进这一功能呢？ DNSSEC 从原理上来说，能够比较理想的解决 DNS 上的安全问题，但是签名和校验 DNS 数据显然会产生额外的开销，从而影响网络和服务器的性能：1）签名和校验的计算量很大，会对域名服务器的计算能力提出更高要求；2）签名数据量比普通的 RR 大得多，会加重网络传输负担；3）签名和密钥数据占用的存储空间也会比之前大一个数量级，会导致域名服务器的数据库和管理系统都不得不进行升级和扩容。而且更重要的一点是，要想使用 DNSSEC，必须满足权威域名服务器和 LocalDNS 同时支持 DNSSec，这就需要现有的大量 DNS 解析服务的提供商对已有设备进行大范围修改，这在异构且复杂的现实环境中实施的难度较大，所以总体推进非常缓慢。 那么除了 DNSSEC 之外，我们还有其他办法来解决 DNS 问题吗？ DNS over HTTP(S)2016 年，RFC 添加了 DNS-over-TLS 的标准，它类似于 HTTP-over-TLS（HTTPS），就是基于 TLS 来进行报文加密的 DNS 请求交互。区别于 DNSSEC，DNS-over-TLS 更侧重于 DNS 交互报文的加密性，而 DNSSEC 更侧重于 DNS 交互报文的完整一致性。阿里云有做过 DNS over TLS 的尝试，详见这里的介绍，最终也因为 DNS Server 的性能问题而束之高阁，取而代之的是 DNS-over-HTTP(S) 方案。 DNS over HTTP(S)，顾名思义就是利用 HTTP(S) 协议与 DNS 服务器交互，绕开运营商的 LocalDNS，来防止域名劫持，提高域名解析效率。另外，由于 DNS 服务器端获取的是真实客户端 IP 而非 LocalDNS IP，能够精确定位客户端地理位置和运营商信息，从而也能有效改进路由选择的精确性。DNS over HTTP(S) 这一方案基于已经成熟并已广泛部署的 HTTP(S) 协议，客户端调用也非常方便，没有额外的成本负担。 到目前为止，有不少公共 DNS 已经支持 DNS over HTTPS，例如国外的 Cloudflare、Google Public DNS 支持 DNS over HTTPS，国内的 DNSPod 支持 DNS over HTTP，等等。以下便是 DNSPod 的 HttpDNS 请求流程图： LeanCloud 的 Java / Android SDK 扩展了 DNS 解析模块，通过公共 DNS 的 http 请求获取到域名解析结果，然后更新本地 DNS 缓存，这样来避免客户端的 LocalDNS 污染，效果还是很明显的。 从今年七月份开始，LeanCloud 已经开始支持应用绑定自己的访问域名，以确保能长期稳定提供服务，这里将我们实践中遇到的一些域名问题和解决办法总结出来，希望对广大开发者有所帮助。 作者：LeanCloud链接：https://juejin.im/post/5d677edc51882566ce35a332]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是IO]]></title>
    <url>%2F2019%2F08%2F29%2F%E4%BB%80%E4%B9%88%E6%98%AFIO%2F</url>
    <content type="text"><![CDATA[什么是IOIO编程 IO在计算机中指Input/Output，也就是输入和输出。由于程序和运行时数据是在内存中驻留，由cpu这个超快的计算核心来执行，涉及到数据交换的地方，通常是磁盘、网络等，就需要IO接口。 比如你打开浏览器，访问新浪首页，浏览器这个程序就需要通过网络IO获取新浪的网页。浏览器首先会发送数据给新浪服务器，告诉它我想要首页的HTML，这个动作是往外发数据，叫Output，随后新浪服务器把网页发过来，这个动作是从外面接收数据，叫input。所以，通常，程序完成IO操作会有Input和Output两个数据流。当然也有只用一个的情况，比如，从磁盘读取文件到内存，就只有Input操作，反过来，把数据写到磁盘文件里，就只是一个Output操作。 IO编程中，Stream（流）是一个很重要的概念，可以把流想象成一个水管，数据就是水管里的水，但是只能单向流动。Input Stream就是数据从外面（磁盘、网络）流进内存，Output Stream就是数据从内存流到外面去。对于浏览网页来说，浏览器和新浪服务器之间至少需要建立两根水管，才可以既能发数据，又能收数据。 由于CPU和内存的速度远远高于外设的速度，所以，在IO编程中，就存在速度严重不匹配的问题。举个例子来说，比如要把100M的数据写入磁盘，CPU输出100M的数据只需要0.01秒，可是磁盘要接收这100M数据可能需要10秒，怎么办呢？有两种办法： 第一种是CPU等着，也就是程序暂停执行后续代码，等100M的数据在10秒后写入磁盘，再接着往下执行，这种模式称为同步IO； 另一种方法是CPU不等待，只是告诉磁盘，“您老慢慢写，不着急，我接着干别的事去了”，于是，后续代码可以立刻接着执行，这种模式称为异步IO。 同步和异步的区别就在于是否等待IO执行的结果。好比你去麦当劳点餐，你说“来个汉堡”，服务员告诉你，对不起，汉堡要现做，需要等5分钟，于是你站在收银台前面等了5分钟，拿到汉堡再去逛商场，这是同步IO。 你说“来个汉堡”，服务员告诉你，汉堡需要等5分钟，你可以先去逛商场，等做好了，我们再通知你，这样你可以立刻去干别的事情（逛商场），这是异步IO。 很明显，使用异步IO来编写程序性能会远远高于同步IO，但是异步IO的缺点是编程模型复杂。想想看，你得知道什么时候通知你“汉堡做好了”，而通知你的方法也各不相同。如果是服务员跑过来找到你，这是回调模式，如果服务员发短信通知你，你就得不停地检查手机，这是轮询模式。总之，异步IO的复杂度远远高于同步IO。 操作IO的能力都是由操作系统提供的，每一种编程语言都会把操作系统提供的低级C接口封装起来方便使用，Python也不例外。我们后面会详细讨论Python的IO编程接口。]]></content>
      <categories>
        <category>python高级篇</category>
      </categories>
      <tags>
        <tag>python高级篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解python之反射机制]]></title>
    <url>%2F2019%2F08%2F29%2F%E8%AF%A6%E8%A7%A3python%E4%B9%8B%E5%8F%8D%E5%B0%84%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[详解python之反射机制一、前言1234567891011121314151617test.pydef f1(): print(&apos;f1&apos;)def f2(): print(&apos;f2&apos;)def f3(): print(&apos;f3&apos;)def f4(): print(&apos;f4&apos;)a = 1 12345import test as ssss.f1()ss.f2()print(ss.a) 我们要导入一个模块，可以使用import.现在有这样的需要，我动态输入一个模块名，可以随时访问到导入模块中的方法或者变量，怎么做呢? 123imp = input(“请输入你想导入的模块名:”) CC = __import__(imp) 這种方式就是通过输入字符串导入你所想导入的模块 CC.f1() # 执行模块中的f1方法 上面我们实现了动态输入模块名，从面使我们能够输入模块名并执行里面的函数。但是上面有一个缺点，那就是执行的函数被固定了。那么，我们能不能改进一下，动态输入函数名，并且来执行呢? 123456789#dynamic.pyimp = input(&quot;请输入模块:&quot;)dd = __import__(imp)# 等价于import impinp_func = input(&quot;请输入要执行的函数：&quot;)f = getattr(dd,inp_func，None)#作用:从导入模块中找到你需要调用的函数inp_func,然后返回一个该函数的引用.没有找到就烦会Nonef() # 执行该函数 上面我们就实现了，动态导入一个模块，并且动态输入函数名然后执行相应功能。 当然上面还存在一点点小问题：那就是我的模块名有可能不是在本级目录中存放着，有可能是如下图存放方式： 那么这种方式我们该如何搞定呢?看下面代码: 123456dd = __import__(&quot;lib.text.commons&quot;) #这样仅仅导入了lib模块dd = __import__(&quot;lib.text.commons&quot;,fromlist = True) #改用这种方式就能导入成功# 等价于import configinp_func = input(&quot;请输入要执行的函数：&quot;)f = getattr(dd,inp_func)f() 二、反射机制 上面说了那么多，到底什么是反射机制呢? 其实，反射就是通过字符串的形式，导入模块；通过字符串的形式，去模块寻找指定函数，并执行。利用字符串的形式去对象（模块）中操作（查找/获取/删除/添加）成员，一种基于字符串的事件驱动！ 先来介绍四个内置函数: 123456789101112131415161718192021222324251. getattr()函数是Python自省的核心函数，具体使用大体如下：class A: def __init__(self): self.name = &apos;zhangjing&apos;#self.age=&apos;24&apos;def method(self): print&quot;method print&quot; Instance = A() print getattr(Instance , &apos;name, &apos;not find&apos;) #如果Instance 对象中有属性name则打印self.name的值，否则打印&apos;not find&apos;print getattr(Instance , &apos;age&apos;, &apos;not find&apos;) #如果Instance 对象中有属性age则打印self.age的值，否则打印&apos;not find&apos;print getattr(a, &apos;method&apos;, &apos;default&apos;) #如果有方法method，否则打印其地址，否则打印default print getattr(a, &apos;method&apos;, &apos;default&apos;)() #如果有方法method，运行函数并打印None否则打印default 2. hasattr(object, name)说明：判断对象object是否包含名为name的特性（hasattr是通过调用getattr(ojbect, name)是否抛出异常来实现的）3. setattr(object, name, value)这是相对应的getattr()。参数是一个对象,一个字符串和一个任意值。字符串可能会列出一个现有的属性或一个新的属性。这个函数将值赋给属性的。该对象允许它提供。例如,setattr(x,“foobar”,123)相当于x.foobar = 123。4. delattr(object, name)与setattr()相关的一组函数。参数是由一个对象(记住python中一切皆是对象)和一个字符串组成的。string参数必须是对象属性名之一。该函数删除该obj的一个由string指定的属性。delattr(x, &apos;foobar&apos;)=del x.foobar 我们可以利用上述的四个函数,来对模块进行一系列操作. 12345678r = hasattr(commons,xxx)判断某个函数或者变量是否存在print(r) setattr(commons,&apos;age&apos;,18) 给commons模块增加一个全局变量age = 18，创建成功返回nonesetattr(config,&apos;age&apos;,lambda a:a+1) //给模块添加一个函数delattr(commons,&apos;age&apos;)//删除模块中某个变量或者函数 注：getattr,hasattr,setattr,delattr对模块的修改都在内存中进行，并不会影响文件中真实内容。 三、实例 基于反射机制模拟web框架路由 需求：比如我们输入:www.xxx.com/commons/f1，返回f1的结果。 123456789101112# 动态导入模块，并执行其中函数url = input(&quot;url: &quot;)target_module, target_func = url.split(&apos;/&apos;)m = __import__(&apos;lib.&apos;+target_module, fromlist=True)inp = url.split(&quot;/&quot;)[-1] # 分割url,并取出url最后一个字符串if hasattr(m,target_func): # 判断在commons模块中是否存在inp这个字符串 target_func = getattr(m,target_func) # 获取inp的引用 target_func() # 执行else: print(&quot;404&quot;)]]></content>
      <categories>
        <category>python高级篇</category>
      </categories>
      <tags>
        <tag>python高级篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python篇-IO多路复用详解]]></title>
    <url>%2F2019%2F08%2F28%2FPython%E7%AF%87-IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Python篇-IO多路复用详解IO多路复用是IO模式的一种,是一种单线程处理多并发的IO操作的方案,其他IO操作方案分别有 : 阻塞 I/O（blocking IO） 非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 异步 I/O（asynchronous IO） IO多路复用其实就是我们说的select，poll，epoll,它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 二 : 事件驱动模型与IO多路复用关系 事件驱动模型 : 好比中午全体员工1000人去饭,点餐完毕后我们回到座位,饭做好了服务员就会叫我们对应点餐的人去取餐. 目前大部分的UI编程都是事件驱动模型，如很多UI平台都会提供onClick()事件，这个事件就代表鼠标按下事件。事件驱动模型大体思路如下： 有一个事件（消息）队列； 鼠标按下时，往这个队列中增加一个点击事件（消息）； 有个循环，不断从队列取出事件，根据不同的事件，调用不同的函数，如onClick()、onKeyDown()等； 事件（消息）一般都各自保存各自的处理函数指针，这样，每个消息都有独立的处理函数； IO多路复用所用的编程范式就是事件驱动模型,是一种监测和回调 封装好的select(拿select举例)会不断询问处理,我们所放入的IO事件,哪个有数据,就返回.我们就可以拿到了. 协程也使用了事件驱动模型来绕过IO事件,遇到IO事件放入操作系统的读取IO列表,然后操作系统处理好给我们返回. 三 : 异步IO与同步IO 同步IO 1.阻塞 I/O（blocking IO） 2.非阻塞 I/O（nonblocking IO） 3.I/O 多路复用（ IO multiplexing） 为什么会发生阻塞呢?其根本原因就是当数据准备好之后,内核态向用户态转发的时候呼叫进程来接受,这个转化的过程是耗时的操作,等转发结束后,阻塞接受 异步IO 为什么异步是没有阻塞的操作呢,其根本原因也是因为,在内核态向用户态转发结束后,才叫进程来接收,进程不用等待,直接拿到数据. 同步异步区别 同步IO是快餐性质餐好了以后你过去取食物,拿食物回到座位,这个过程是耗时的操作. 异步IO是大餐性质,我们餐好了以后,大家还在座位上聊天,服务员就会把食物端到桌子上,然后我们吃就可以了,高端服务. 解析 :我们可以用IO多路复用写一个SocketServer实现单线程下的大并发.但是IO多路复用本质是还是同步IO,因为数据从内核态到用户态的拷贝需要等待操作系统完成,只有异步 I/O本质是是异步IO,本质上异步 I/O会当数据从内核态拷贝到用户态这一过程完成之后再去通知程序直接取走,因为I/O操作是系统完成,所以这才是真正意义上的异步IO操作. 四 : IO多路复用socketServer利用IO多路复用写一个socketServer,大多数情况下机几乎很难用到,因为有许许多多模块和框架已经为我们封装好了,简单了解一下底层的实践即可. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# Author:TianTianBabyimport selectimport socketimport queue#创建socket连接server = socket.socket()server.bind((&apos;localhost&apos;,9000))server.listen(1000)#设置非阻塞模式server.setblocking(False)#key 为接受消息对象实例,key 为接受的数据mes_dic = &#123;&#125;#需要检测的连接列表inputs = [server,]#返回上一次的数据列表outputs = []while True: readable,writeable,exceptional = select.select(inputs,outputs,inputs) for r in readable: if r is server:#代表来了一个新连接 conn,addr = server.accept() inputs.append(conn)#因为新建立的连接还没发数据过来,现在就接受的话程序就报错 #所以要想实现这个客户端发数据时server端能知道,就需要让select再监测 #初始化一个队列,后面存要返回给这个客户端的数据 mes_dic[conn] = queue.Queue() else: data = r.recv(1024) mes_dic[r].put(data) #放入返回的连接队列里 outputs.append(r) #要返回给客户端的连接列表 for w in writeable: data_to_client = mes_dic[w].get() #返回给客户端源数据 w.send(data_to_client) #确保下次循环的时候writeable,不反回这个已经处理完的连接 outputs.remove(w) #异常 for e in exceptional: if e in outputs: outputs.remove(e) inputs.remove(e) del mes_dic[e] 五 : 总结select，poll，epoll都是IO多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。]]></content>
      <categories>
        <category>python高级篇</category>
      </categories>
      <tags>
        <tag>python高级篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[socket网络编程入门]]></title>
    <url>%2F2019%2F08%2F27%2Fsocket%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Socket网络编程入门引子 从我们熟悉的web应用为例子，先不讨论socket,我们看一下如果两台机器之间要通讯，需要经过哪些步骤。我们看一下服务器回给我们一个简单的”hi”时，需要经过哪些步骤。 服务端: 1.服务器的应用层会产生一个hi字符串 2.数据封装后交给传输层 3.传输层封装后交给网络层 4.网络层数据封装后交给数据链路层 5.物理层接收后变成0和1字节流传输到internet网络中 客户端： 1.客户端物理层接收来自服务端的0和1字节流 2.数据链路层接收物理层数据并解读数据报 3.网络层接收物理层数据并解读数据报 4.传输层解读网络层数据并解读数据报 5.应用层接收到hi字符 现实中的网络数据传输远远比这复杂得多，数据必须经过5层协议栈。试想如果让一个开发者从0开始实现这样的一个数据传输，难度可想而知。但是目前从事网络开发的人千千万万，他们是怎么做到的呢，这一切都要归功于unix操作系统和socket。因为有了socket，网络编程变得快速而且简单了很多。 什么是socketsocket就是操作系统(unix)为应用进程提供的一个方便操作网络数据收发的api接口，只要你读懂了这一套api接口，就可以把数据从一个主机传输到另一个主机，或者从其他主机接受数据，而不需要知道还有五层协议的复杂性。这就好比我们开发者读取文件的时候，file系列函数就可以轻松的帮我们获取文件的内容，我们不需要知道磁盘的磁头是如何转动，如何实现定位读数据等。 这个图的上层是应用程序层，比如我们的nginx，apache系列的软件，下层是操作系统层，比如Unix。Socket层位于应用层和传输层之间，上层应用如果想实现网络编程，只需要调用操作系统提供的Socket层提供的api即可。操作系统通过Socket大大简化了网络编程的复杂性，使得从事网络编程的开发人员更加便捷。 Socket相关函数一览既然知道了Socket是操作系统为我们提供的一层网络编程接口封装，那就可以看一下操作系统给我们留了哪些接口，我们能用这些接口干什么 函数名称 用途 说明 socket 创建套接字 服务端/客户端需要使用 connect 连接远端服务器 仅客户端 bind 绑定套接字的本地ip和端口号 通常客户端不需要 listen 置服务端套接字为监听模式 仅服务端可用 accept 接收一个连接请求并创建新套接字 仅服务端可用 send 发送数据 服务端和客户端可用 recv 接收数据 服务端和客户端可用 close 关闭套接字 服务端和客户端可用 客户端编程创建 socket首先要创建 socket，用 Python 中 socket 模块的函数 socket 就可以完成： 12345678#Socket client example in python import socket #for sockets #create an AF_INET, STREAM socket (TCP)s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) print &apos;Socket Created&apos; 函数 socket.socket 创建一个 socket，返回该 socket 的描述符，将在后面相关函数中使用。该函数带有两个参数： Address Family：可以选择 AF_INET（用于 Internet 进程间通信） 或者 AF_UNIX（用于同一台机器进程间通信） Type：套接字类型，可以是 SOCKET_STREAM（流式套接字，主要用于 TCP 协议）或者SOCKET_DGRAM（数据报套接字，主要用于 UDP 协议） 错误处理 如果创建 socket 函数失败，会抛出一个 socket.error 的异常，需要捕获： 12345678910111213#handling errors in python socket programs import socket #for socketsimport sys #for exit try: #create an AF_INET, STREAM socket (TCP) s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)except socket.error, msg: print 'Failed to create socket. Error code: ' + str(msg[0]) + ' , Error message : ' + msg[1] sys.exit(); print 'Socket Created' 连接服务器本文开始也提到了，socket 使用 (IP地址，协议，端口号) 来标识一个进程，那么我们要想和服务器进行通信，就需要知道它的 IP地址以及端口号。 获得远程主机的 IP 地址 Python 提供了一个简单的函数 socket.gethostbyname 来获得远程主机的 IP 地址： 123456789101112host = 'www.google.com'port = 80 try: remote_ip = socket.gethostbyname( host ) except socket.gaierror: #could not resolve print 'Hostname could not be resolved. Exiting' sys.exit() print 'Ip address of ' + host + ' is ' + remote_ip 现在我们知道了服务器的 IP 地址，就可以使用连接函数 connect 连接到该 IP 的某个特定的端口上了，下面例子连接到 80 端口上（是 HTTP 服务的默认端口）： 1234#Connect to remote servers.connect((remote_ip , port)) print 'Socket Connected to ' + host + ' on ip ' + remote_ip 运行该程序： 1234$ python client.pySocket createdIp of remote host www.google.com is 173.194.38.145Socket Connected to www.google.com on ip 173.194.38.145 发送数据上面说明连接到 www.google.com 已经成功了，接下面我们可以向服务器发送一些数据，例如发送字符串GET / HTTP/1.1\r\n\r\n，这是一个 HTTP 请求网页内容的命令。 123456789101112#Send some data to remote servermessage = "GET / HTTP/1.1\r\n\r\n" try : #Set the whole string s.sendall(message)except socket.error: #Send failed print 'Send failed' sys.exit() print 'Message send successfully' 发送完数据之后，客户端还需要接受服务器的响应。 接收数据函数 recv 可以用来接收 socket 的数据： 1234#Now receive datareply = s.recv(4096) print reply 一起运行的结果如下： 12345678910111213141516171819Socket createdIp of remote host www.google.com is 173.194.38.145Socket Connected to www.google.com on ip 173.194.38.145Message send successfullyHTTP/1.1 302 FoundCache-Control: privateContent-Type: text/html; charset=UTF-8Location: http://www.google.com.sg/?gfe_rd=cr&amp;ei=PlqJVLCREovW8gfF0oG4CQContent-Length: 262Date: Thu, 11 Dec 2014 08:47:58 GMTServer: GFE/2.0Alternate-Protocol: 80:quic,p=0.02&lt;HTML&gt;&lt;HEAD&gt;&lt;meta http-equiv="content-type" content="text/html;charset=utf-8"&gt;&lt;TITLE&gt;302 Moved&lt;/TITLE&gt;&lt;/HEAD&gt;&lt;BODY&gt;&lt;H1&gt;302 Moved&lt;/H1&gt;The document has moved&lt;A HREF="http://www.google.com.sg/?gfe_rd=cr&amp;ei=PlqJVLCREovW8gfF0oG4CQ"&gt;here&lt;/A&gt;.&lt;/BODY&gt;&lt;/HTML&gt; 关闭 socket当我们不想再次请求服务器数据时，可以将该 socket 关闭，结束这次通信： 1s.close() 小结上面我们学到了如何： 创建 socket 连接到远程服务器 发送数据 接收数据 关闭 socket 当我们打开 www.google.com 时，浏览器所做的就是这些，知道这些是非常有意义的。在 socket 中具有这种行为特征的被称为CLIENT，客户端主要是连接远程系统获取数据。 socket 中另一种行为称为SERVER，服务器使用 socket 来接收连接以及提供数据，和客户端正好相反。所以 www.google.com 是服务器，你的浏览器是客户端，或者更准确地说，www.google.com 是 HTTP 服务器，你的浏览器是 HTTP 客户端。 那么上面介绍了客户端的编程，现在轮到服务器端如果使用 socket 了。 服务器端编程服务器端主要做以下工作： 打开 socket 绑定到特定的地址以及端口上 监听连接 建立连接 接收/发送数据 上面已经介绍了如何创建 socket 了，下面一步是绑定。 绑定 socket函数 bind 可以用来将 socket 绑定到特定的地址和端口上，它需要一个 sockaddr_in 结构作为参数： 12345678910111213141516import socketimport sys HOST = '' # Symbolic name meaning all available interfacesPORT = 8888 # Arbitrary non-privileged port s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)print 'Socket created' try: s.bind((HOST, PORT))except socket.error , msg: print 'Bind failed. Error Code : ' + str(msg[0]) + ' Message ' + msg[1] sys.exit() print 'Socket bind complete' 绑定完成之后，接下来就是监听连接了。 监听连接函数 listen 可以将 socket 置于监听模式： 12s.listen(10)print 'Socket now listening' 该函数带有一个参数称为 backlog，用来控制连接的个数。如果设为 10，那么有 10 个连接正在等待处理，此时第 11 个请求过来时将会被拒绝。 接收连接当有客户端向服务器发送连接请求时，服务器会接收连接： 12345#wait to accept a connection - blocking callconn, addr = s.accept() #display client informationprint 'Connected with ' + addr[0] + ':' + str(addr[1]) 运行该程序的，输出结果如下： 1234$ python server.pySocket createdSocket bind completeSocket now listening 此时，该程序在 8888 端口上等待请求的到来。不要关掉这个程序，让它一直运行，现在客户端可以通过该端口连接到 socket。我们用 telnet 客户端来测试，打开一个终端，输入 telnet localhost 8888： 12345$ telnet localhost 8888Trying 127.0.0.1...Connected to localhost.Escape character is '^]'.Connection closed by foreign host. 这时服务端输出会显示： 12345$ python server.pySocket createdSocket bind completeSocket now listeningConnected with 127.0.0.1:59954 我们观察到客户端已经连接上服务器了。在建立连接之后，我们可以用来与客户端进行通信。下面例子演示的是，服务器建立连接之后，接收客户端发送来的数据，并立即将数据发送回去，下面是完整的服务端程序： 12345678910111213141516171819202122232425262728293031import socketimport sys HOST = '' # Symbolic name meaning all available interfacesPORT = 8888 # Arbitrary non-privileged port s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)print 'Socket created' try: s.bind((HOST, PORT))except socket.error , msg: print 'Bind failed. Error Code : ' + str(msg[0]) + ' Message ' + msg[1] sys.exit() print 'Socket bind complete' s.listen(10)print 'Socket now listening' #wait to accept a connection - blocking callconn, addr = s.accept() print 'Connected with ' + addr[0] + ':' + str(addr[1]) #now keep talking with the clientdata = conn.recv(1024)conn.sendall(data) conn.close()s.close() 在一个终端中运行这个程序，打开另一个终端，使用 telnet 连接服务器，随便输入字符串，你会看到： 1234567$ telnet localhost 8888Trying 127.0.0.1...Connected to localhost.Escape character is '^]'.happyhappyConnection closed by foreign host. 客户端（telnet）接收了服务器的响应。 我们在完成一次响应之后服务器立即断开了连接，而像 www.google.com 这样的服务器总是一直等待接收连接的。我们需要将上面的服务器程序改造成一直运行，最简单的办法是将 accept 放到一个循环中，那么就可以一直接收连接了。 保持服务我们可以将代码改成这样让服务器一直工作： 1234567891011121314151617181920212223242526272829303132333435import socketimport sys HOST = '' # Symbolic name meaning all available interfacesPORT = 5000 # Arbitrary non-privileged port s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)print 'Socket created' try: s.bind((HOST, PORT))except socket.error , msg: print 'Bind failed. Error Code : ' + str(msg[0]) + ' Message ' + msg[1] sys.exit() print 'Socket bind complete' s.listen(10)print 'Socket now listening' #now keep talking with the clientwhile 1: #wait to accept a connection - blocking call conn, addr = s.accept() print 'Connected with ' + addr[0] + ':' + str(addr[1]) data = conn.recv(1024) reply = 'OK...' + data if not data: break conn.sendall(reply) conn.close()s.close() 现在在一个终端下运行上面的服务器程序，再开启三个终端，分别用 telnet 去连接，如果一个终端连接之后不输入数据其他终端是没办法进行连接的，而且每个终端只能服务一次就断开连接。这从代码上也是可以看出来的。 这显然也不是我们想要的，我们希望多个客户端可以随时建立连接，而且每个客户端可以跟服务器进行多次通信，这该怎么修改呢？ 处理连接为了处理每个连接，我们需要将处理的程序与主程序的接收连接分开。一种方法可以使用线程来实现，主服务程序接收连接，创建一个线程来处理该连接的通信，然后服务器回到接收其他连接的逻辑上来。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import socketimport sysfrom thread import * HOST = '' # Symbolic name meaning all available interfacesPORT = 8888 # Arbitrary non-privileged port s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)print 'Socket created' #Bind socket to local host and porttry: s.bind((HOST, PORT))except socket.error , msg: print 'Bind failed. Error Code : ' + str(msg[0]) + ' Message ' + msg[1] sys.exit() print 'Socket bind complete' #Start listening on sockets.listen(10)print 'Socket now listening' #Function for handling connections. This will be used to create threadsdef clientthread(conn): #Sending message to connected client conn.send('Welcome to the server. Type something and hit enter\n') #send only takes string #infinite loop so that function do not terminate and thread do not end. while True: #Receiving from client data = conn.recv(1024) reply = 'OK...' + data if not data: break conn.sendall(reply) #came out of loop conn.close() #now keep talking with the clientwhile 1: #wait to accept a connection - blocking call conn, addr = s.accept() print 'Connected with ' + addr[0] + ':' + str(addr[1]) #start new thread takes 1st argument as a function name to be run, second is the tuple of arguments to the function. start_new_thread(clientthread ,(conn,)) s.close() 再次运行上面的程序，打开三个终端来与主服务器建立 telnet 连接，这时候三个客户端可以随时接入，而且每个客户端可以与主服务器进行多次通信。 telnet 终端下可能输出如下： 1234567891011$ telnet localhost 8888Trying 127.0.0.1...Connected to localhost.Escape character is '^]'.Welcome to the server. Type something and hit enterhiOK...hiasdOK...asdcvOK...cv 要结束 telnet 的连接，按下 Ctrl-] 键，再输入 close 命令。 服务器终端的输出可能是这样的： 123456$ python server.pySocket createdSocket bind completeSocket now listeningConnected with 127.0.0.1:60730Connected with 127.0.0.1:60731]]></content>
      <categories>
        <category>python高级篇</category>
      </categories>
      <tags>
        <tag>python高级篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面向对象及相关]]></title>
    <url>%2F2019%2F08%2F27%2F%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E5%8F%8A%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[面向对象及相关其他相关一、isinstance(obj, cls) 检查是否obj是否是类 cls 的对象 123456class Foo(object): pass obj = Foo() isinstance(obj, Foo) 二、issubclass(sub, super) 检查sub类是否是 super 类的派生类 1234567class Foo(object): pass class Bar(Foo): pass issubclass(Bar, Foo) 三、异常处理 1、异常基础 在编程过程中为了增加友好性，在程序出现bug时一般不会将错误信息显示给用户，而是现实一个提示的页面，通俗来说就是不让用户看见大黄页！！！ 1234try: passexcept Exception,ex: pass 需求：将用户输入的两个数字相加 12345678910while True: num1 = raw_input(&apos;num1:&apos;) num2 = raw_input(&apos;num2:&apos;) try: num1 = int(num1) num2 = int(num2) result = num1 + num2 except Exception, e: print &apos;出现异常，信息如下：&apos; print e 2、异常种类 python中的异常种类非常多，每个异常专门用于处理某一项异常！！！ 1234567891011121314常用异常AttributeError 试图访问一个对象没有的树形，比如foo.x，但是foo没有属性xIOError 输入/输出异常；基本上是无法打开文件ImportError 无法引入模块或包；基本上是路径问题或名称错误IndentationError 语法错误（的子类） ；代码没有正确对齐IndexError 下标索引超出序列边界，比如当x只有三个元素，却试图访问x[5]KeyError 试图访问字典里不存在的键KeyboardInterrupt Ctrl+C被按下NameError 使用一个还未被赋予对象的变量SyntaxError Python代码非法，代码不能编译(个人认为这是语法错误，写错了）TypeError 传入对象类型与要求的不符合UnboundLocalError 试图访问一个还未被设置的局部变量，基本上是由于另有一个同名的全局变量，导致你以为正在访问它ValueError 传入一个调用者不期望的值，即使值的类型是正确的 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849更多异常ArithmeticErrorAssertionErrorAttributeErrorBaseExceptionBufferErrorBytesWarningDeprecationWarningEnvironmentErrorEOFErrorExceptionFloatingPointErrorFutureWarningGeneratorExitImportErrorImportWarningIndentationErrorIndexErrorIOErrorKeyboardInterruptKeyErrorLookupErrorMemoryErrorNameErrorNotImplementedErrorOSErrorOverflowErrorPendingDeprecationWarningReferenceErrorRuntimeErrorRuntimeWarningStandardErrorStopIterationSyntaxErrorSyntaxWarningSystemErrorSystemExitTabErrorTypeErrorUnboundLocalErrorUnicodeDecodeErrorUnicodeEncodeErrorUnicodeErrorUnicodeTranslateErrorUnicodeWarningUserWarningValueErrorWarningZeroDivisionError 123456实例：IndexErrordic = [&quot;wupeiqi&quot;, &apos;alex&apos;]try: dic[10]except IndexError, e: print e 123456实例：KeyErrordic = &#123;&apos;k1&apos;:&apos;v1&apos;&#125;try: dic[&apos;k20&apos;]except KeyError, e: print e 123456实例：ValueErrors1 = &apos;hello&apos;try: int(s1)except ValueError, e: print e 对于上述实例，异常类只能用来处理指定的异常情况，如果非指定异常则无法处理。 1234567# 未捕获到异常，程序直接报错 s1 = &apos;hello&apos;try: int(s1)except IndexError,e: print e 所以，写程序时需要考虑到try代码块中可能出现的任意异常，可以这样写： 123456789s1 = &apos;hello&apos;try: int(s1)except IndexError,e: print eexcept KeyError,e: print eexcept ValueError,e: print e 万能异常 在python的异常中，有一个万能异常：Exception，他可以捕获任意异常，即： 12345s1 = &apos;hello&apos;try: int(s1)except Exception,e: print e 接下来你可能要问了，既然有这个万能异常，其他异常是不是就可以忽略了！ 答：当然不是，对于特殊处理或提醒的异常需要先定义，最后定义Exception来确保程序正常运行。 123456789s1 = &apos;hello&apos;try: int(s1)except KeyError,e: print &apos;键错误&apos;except IndexError,e: print &apos;索引错误&apos;except Exception, e: print &apos;错误&apos; 3、异常其他结构 123456789101112try: # 主代码块 passexcept KeyError,e: # 异常时，执行该块 passelse: # 主代码块执行完，执行该块 passfinally: # 无论异常与否，最终执行该块 pass 4、主动触发异常 1234try: raise Exception(&apos;错误了。。。&apos;)except Exception,e: print e 5、自定义异常 123456789101112class WupeiqiException(Exception): def __init__(self, msg): self.message = msg def __str__(self): return self.message try: raise WupeiqiException(&apos;我的异常&apos;)except WupeiqiException,e: print e 6、断言 12345# assert 条件 assert 1 == 1 assert 1 == 2 四、反射 python中的反射功能是由以下四个内置函数提供：hasattr、getattr、setattr、delattr，改四个函数分别用于对对象内部执行：检查是否含有某成员、获取成员、设置成员、删除成员。 12345678910111213141516171819202122232425class Foo(object): def __init__(self): self.name = &apos;wupeiqi&apos; def func(self): return &apos;func&apos; obj = Foo() # #### 检查是否含有成员 ####hasattr(obj, &apos;name&apos;)hasattr(obj, &apos;func&apos;) # #### 获取成员 ####getattr(obj, &apos;name&apos;)getattr(obj, &apos;func&apos;) # #### 设置成员 ####setattr(obj, &apos;age&apos;, 18)setattr(obj, &apos;show&apos;, lambda num: num + 1) # #### 删除成员 ####delattr(obj, &apos;name&apos;)delattr(obj, &apos;func&apos;) 1234567891011121314151617详细解析：当我们要访问一个对象的成员时，应该是这样操作：class Foo(object): def __init__(self): self.name = &apos;alex&apos; def func(self): return &apos;func&apos; obj = Foo() # 访问字段obj.name# 执行方法obj.func() 那么问题来了？ a、上述访问对象成员的 name 和 func 是什么？ 答：是变量名 b、obj.xxx 是什么意思？ 答：obj.xxx 表示去obj中或类中寻找变量名 xxx，并获取对应内存地址中的内容。 c、需求：请使用其他方式获取obj对象中的name变量指向内存中的值 “alex” 1234567class Foo(object): def __init__(self): self.name = &apos;alex&apos; # 不允许使用 obj.nameobj = Foo() 答：有两种方式，如下： 123456789101112131.class Foo(object): def __init__(self): self.name = &apos;alex&apos; def func(self): return &apos;func&apos;# 不允许使用 obj.nameobj = Foo()print obj.__dict__[&apos;name&apos;] 123456789101112132.class Foo(object): def __init__(self): self.name = &apos;alex&apos; def func(self): return &apos;func&apos;# 不允许使用 obj.nameobj = Foo()print getattr(obj, &apos;name&apos;) d、比较三种访问方式 obj.name obj.dict[‘name’] getattr(obj, ‘name’) 答：第一种和其他种比，… 第二种和第三种比，… 12345678910111213141516171819202122232425262728293031Web框架实例#!/usr/bin/env python#coding:utf-8from wsgiref.simple_server import make_serverclass Handler(object): def index(self): return &apos;index&apos; def news(self): return &apos;news&apos;def RunServer(environ, start_response): start_response(&apos;200 OK&apos;, [(&apos;Content-Type&apos;, &apos;text/html&apos;)]) url = environ[&apos;PATH_INFO&apos;] temp = url.split(&apos;/&apos;)[1] obj = Handler() is_exist = hasattr(obj, temp) if is_exist: func = getattr(obj, temp) ret = func() return ret else: return &apos;404 not found&apos;if __name__ == &apos;__main__&apos;: httpd = make_server(&apos;&apos;, 8001, RunServer) print &quot;Serving HTTP on port 8000...&quot; httpd.serve_forever() 结论：反射是通过字符串的形式操作对象相关的成员。一切事物都是对象！！！ 12345678910111213141516171819反射当前模块成员#!/usr/bin/env python# -*- coding:utf-8 -*-import sysdef s1(): print &apos;s1&apos;def s2(): print &apos;s2&apos;this_module = sys.modules[__name__]hasattr(this_module, &apos;s1&apos;)getattr(this_module, &apos;s2&apos;) 类也是对象 1234567891011121314151617class Foo(object): staticField = &quot;old boy&quot; def __init__(self): self.name = &apos;wupeiqi&apos; def func(self): return &apos;func&apos; @staticmethod def bar(): return &apos;bar&apos; print getattr(Foo, &apos;staticField&apos;)print getattr(Foo, &apos;func&apos;)print getattr(Foo, &apos;bar&apos;) 模块也是对象 12345#!/usr/bin/env python# -*- coding:utf-8 -*-def dev(): return &apos;dev&apos; 1234567891011121314151617181920#!/usr/bin/env python# -*- coding:utf-8 -*- &quot;&quot;&quot;程序目录： home.py index.py 当前文件： index.py&quot;&quot;&quot; import home as obj #obj.dev() func = getattr(obj, &apos;dev&apos;)func() 设计模式 设计模式一、单例模式 单例，顾名思义单个实例。 学习单例之前，首先来回顾下面向对象的内容： python的面向对象由两个非常重要的两个“东西”组成：类、实例 面向对象场景一： 如：创建三个游戏人物，分别是： 苍井井，女，18，初始战斗力1000 东尼木木，男，20，初始战斗力1800 波多多，女，19，初始战斗力2500 12345678910111213141516171819# ##################### 定义类 #####################class Person: def __init__(self, na, gen, age, fig): self.name = na self.gender = gen self.age = age self.fight =fig def grassland(self): &quot;&quot;&quot;注释：草丛战斗，消耗200战斗力&quot;&quot;&quot; self.fight = self.fight - 200# ##################### 创建实例 #####################cang = Person(&apos;苍井井&apos;, &apos;女&apos;, 18, 1000) # 创建苍井井角色dong = Person(&apos;东尼木木&apos;, &apos;男&apos;, 20, 1800) # 创建东尼木木角色bo = Person(&apos;波多多&apos;, &apos;女&apos;, 19, 2500) # 创建波多多角色 面向对象场景二： 如：创建对数据库操作的公共类 增 删 改 查 1234567891011121314151617181920212223242526272829303132333435363738# #### 定义类 ####class DbHelper(object): def __init__(self): self.hostname = &apos;1.1.1.1&apos; self.port = 3306 self.password = &apos;pwd&apos; self.username = &apos;root&apos; def fetch(self): # 连接数据库 # 拼接sql语句 # 操作 pass def create(self): # 连接数据库 # 拼接sql语句 # 操作 pass def remove(self): # 连接数据库 # 拼接sql语句 # 操作 pass def modify(self): # 连接数据库 # 拼接sql语句 # 操作 pass# #### 操作类 ####db = DbHelper()db.create() 实例：结合场景二实现Web应用程序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768Web应用程序实例#!/usr/bin/env python#coding:utf-8from wsgiref.simple_server import make_serverclass DbHelper(object): def __init__(self): self.hostname = &apos;1.1.1.1&apos; self.port = 3306 self.password = &apos;pwd&apos; self.username = &apos;root&apos; def fetch(self): # 连接数据库 # 拼接sql语句 # 操作 return &apos;fetch&apos; def create(self): # 连接数据库 # 拼接sql语句 # 操作 return &apos;create&apos; def remove(self): # 连接数据库 # 拼接sql语句 # 操作 return &apos;remove&apos; def modify(self): # 连接数据库 # 拼接sql语句 # 操作 return &apos;modify&apos;class Handler(object): def index(self): # 创建对象 db = DbHelper() db.fetch() return &apos;index&apos; def news(self): return &apos;news&apos;def RunServer(environ, start_response): start_response(&apos;200 OK&apos;, [(&apos;Content-Type&apos;, &apos;text/html&apos;)]) url = environ[&apos;PATH_INFO&apos;] temp = url.split(&apos;/&apos;)[1] obj = Handler() is_exist = hasattr(obj, temp) if is_exist: func = getattr(obj, temp) ret = func() return ret else: return &apos;404 not found&apos;if __name__ == &apos;__main__&apos;: httpd = make_server(&apos;&apos;, 8001, RunServer) print &quot;Serving HTTP on port 8001...&quot; httpd.serve_forever() 对于上述实例，每个请求到来，都需要在内存里创建一个实例，再通过该实例执行指定的方法。 那么问题来了…如果并发量大的话，内存里就会存在非常多功能上一模一样的对象。存在这些对象肯定会消耗内存，对于这些功能相同的对象可以在内存中仅创建一个，需要时都去调用，也是极好的！！！ 铛铛 铛铛 铛铛铛铛铛，单例模式出马，单例模式用来保证内存中仅存在一个实例！！！ 通过面向对象的特性，构造出单例模式： 123456789101112131415# ########### 单例类定义 ###########class Foo(object): __instance = None @staticmethod def singleton(): if Foo.__instance: return Foo.__instance else: Foo.__instance = Foo() return Foo.__instance # ########### 获取实例 ###########obj = Foo.singleton() 对于Python单例模式，创建对象时不能再直接使用：obj = Foo()，而应该调用特殊的方法：obj = Foo.singleton() 。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778Web应用实例-单例模式#!/usr/bin/env python#coding:utf-8from wsgiref.simple_server import make_server# ########### 单例类定义 ###########class DbHelper(object): __instance = None def __init__(self): self.hostname = &apos;1.1.1.1&apos; self.port = 3306 self.password = &apos;pwd&apos; self.username = &apos;root&apos; @staticmethod def singleton(): if DbHelper.__instance: return DbHelper.__instance else: DbHelper.__instance = DbHelper() return DbHelper.__instance def fetch(self): # 连接数据库 # 拼接sql语句 # 操作 pass def create(self): # 连接数据库 # 拼接sql语句 # 操作 pass def remove(self): # 连接数据库 # 拼接sql语句 # 操作 pass def modify(self): # 连接数据库 # 拼接sql语句 # 操作 passclass Handler(object): def index(self): obj = DbHelper.singleton() print id(single) obj.create() return &apos;index&apos; def news(self): return &apos;news&apos;def RunServer(environ, start_response): start_response(&apos;200 OK&apos;, [(&apos;Content-Type&apos;, &apos;text/html&apos;)]) url = environ[&apos;PATH_INFO&apos;] temp = url.split(&apos;/&apos;)[1] obj = Handler() is_exist = hasattr(obj, temp) if is_exist: func = getattr(obj, temp) ret = func() return ret else: return &apos;404 not found&apos;if __name__ == &apos;__main__&apos;: httpd = make_server(&apos;&apos;, 8001, RunServer) print &quot;Serving HTTP on port 8001...&quot; httpd.serve_forever() 总结：单利模式存在的目的是保证当前内存中仅存在单个实例，避免内存浪费！！！]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python面向对象-进阶篇]]></title>
    <url>%2F2019%2F08%2F24%2Fpython%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1-%E8%BF%9B%E9%98%B6%E7%AF%87%2F</url>
    <content type="text"><![CDATA[python面向对象(进阶篇) 本篇将详细介绍Python 类的成员、成员修饰符、类的特殊成员。 类的成员类的成员可以分为三大类：字段、方法和属性 注：所有成员中，只有普通字段的内容保存对象中，即：根据此类创建了多少对象，在内存中就有多少个普通字段。而其他的成员，则都是保存在类中，即：无论对象的多少，在内存中只创建一份。 一、字段 字段包括：普通字段和静态字段，他们在定义和使用中有所区别，而最本质的区别是内存中保存的位置不同， 普通字段属于对象 静态字段属于类 123456789101112131415161718字段的定义和使用class Province: # 静态字段 country ＝ &apos;中国&apos; def __init__(self, name): # 普通字段 self.name = name# 直接访问普通字段obj = Province(&apos;河北省&apos;)print obj.name# 直接访问静态字段Province.country 由上述代码可以看出【普通字段需要通过对象来访问】【静态字段通过类访问】，在使用上可以看出普通字段和静态字段的归属是不同的。其在内容的存储方式类似如下图： 由上图可是： 静态字段在内存中只保存一份 普通字段在每个对象中都要保存一份 应用场景： 通过类创建对象时，如果每个对象都具有相同的字段，那么就使用静态字段 二、方法 方法包括：普通方法、静态方法和类方法，三种方法在内存中都归属于类，区别在于调用方式不同。 普通方法：由对象调用；至少一个self参数；执行普通方法时，自动将调用该方法的对象赋值给self； 类方法：由类调用； 至少一个cls参数；执行类方法时，自动将调用该方法的类复制给cls； 静态方法：由类调用；无默认参数； 12345678910111213141516171819202122232425262728293031323334方法的定义和使用class Foo: def __init__(self, name): self.name = name def ord_func(self): &quot;&quot;&quot; 定义普通方法，至少有一个self参数 &quot;&quot;&quot; # print self.name print &apos;普通方法&apos; @classmethod def class_func(cls): &quot;&quot;&quot; 定义类方法，至少有一个cls参数 &quot;&quot;&quot; print &apos;类方法&apos; @staticmethod def static_func(): &quot;&quot;&quot; 定义静态方法 ，无默认参数&quot;&quot;&quot; print &apos;静态方法&apos;# 调用普通方法f = Foo()f.ord_func()# 调用类方法Foo.class_func()# 调用静态方法Foo.static_func() 相同点：对于所有的方法而言，均属于类（非对象）中，所以，在内存中也只保存一份。 不同点：方法调用者不同、调用方法时自动传入的参数不同。 三、属性 如果你已经了解Python类中的方法，那么属性就非常简单了，因为Python中的属性其实是普通方法的变种。 对于属性，有以下三个知识点： 属性的基本使用 属性的两种定义方式 1、属性的基本使用 12345678910111213141516属性的定义和使用# ############### 定义 ###############class Foo: def func(self): pass # 定义属性 @property def prop(self): pass# ############### 调用 ###############foo_obj = Foo()foo_obj.func()foo_obj.prop #调用属性 由属性的定义和调用要注意一下几点： 定义时，在普通方法的基础上添加 @property 装饰器； 定义时，属性仅有一个self参数 调用时，无需括号方法：foo_obj.func() 属性：foo_obj.prop 注意：属性存在意义是：访问属性时可以制造出和访问字段完全相同的假象 ​ 属性由方法变种而来，如果Python中没有属性，方法完全可以代替其功能。 实例：对于主机列表页面，每次请求不可能把数据库中的所有内容都显示到页面上，而是通过分页的功能局部显示，所以在向数据库中请求数据时就要显示的指定获取从第m条到第n条的所有数据（即：limit m,n），这个分页的功能包括： 根据用户请求的当前页和总数据条数计算出 m 和 n 根据m 和 n 去数据库中请求数据 12345678910111213141516171819202122232425# ############### 定义 ###############class Pager: def __init__(self, current_page): # 用户当前请求的页码（第一页、第二页...） self.current_page = current_page # 每页默认显示10条数据 self.per_items = 10 @property def start(self): val = (self.current_page - 1) * self.per_items return val @property def end(self): val = self.current_page * self.per_items return val# ############### 调用 ###############p = Pager(1)p.start 就是起始值，即：mp.end 就是结束值，即：n 从上述可见，Python的属性的功能是：属性内部进行一系列的逻辑计算，最终将计算结果返回。 2、属性的两种定义方式 属性的定义有两种方式： 装饰器 即：在方法上应用装饰器 静态字段 即：在类中定义值为property对象的静态字段 装饰器方式：在类的普通方法上应用@property装饰器 我们知道Python中的类有经典类和新式类，新式类的属性比经典类的属性丰富。（ 如果类继object，那么该类是新式类 ）经典类，具有一种@property装饰器（如上一步实例） 123456789# ############### 定义 ############### class Goods: @property def price(self): return &quot;wupeiqi&quot;# ############### 调用 ###############obj = Goods()result = obj.price # 自动执行 @property 修饰的 price 方法，并获取方法的返回值 新式类，具有三种@property装饰器 1234567891011121314151617181920212223# ############### 定义 ###############class Goods(object): @property def price(self): print &apos;@property&apos; @price.setter def price(self, value): print &apos;@price.setter&apos; @price.deleter def price(self): print &apos;@price.deleter&apos;# ############### 调用 ###############obj = Goods()obj.price # 自动执行 @property 修饰的 price 方法，并获取方法的返回值obj.price = 123 # 自动执行 @price.setter 修饰的 price 方法，并将 123 赋值给方法的参数del obj.price # 自动执行 @price.deleter 修饰的 price 方法 注：经典类中的属性只有一种访问方式，其对应被 @property 修饰的方法 新式类中的属性有三种访问方式，并分别对应了三个被@property、@方法名.setter、@方法名.deleter修饰的方法 由于新式类中具有三种访问方式，我们可以根据他们几个属性的访问特点，分别将三个方法定义为对同一个属性：获取、修改、删除 1234567891011121314151617181920212223242526class Goods(object): def __init__(self): # 原价 self.original_price = 100 # 折扣 self.discount = 0.8 @property def price(self): # 实际价格 = 原价 * 折扣 new_price = self.original_price * self.discount return new_price @price.setter def price(self, value): self.original_price = value @price.deltter def price(self, value): del self.original_priceobj = Goods()obj.price # 获取商品价格obj.price = 200 # 修改商品原价del obj.price # 删除商品原价 静态字段方式，创建值为property对象的静态字段 当使用静态字段的方式创建属性时，经典类和新式类无区别 12345678910class Foo: def get_bar(self): return &apos;wupeiqi&apos; BAR = property(get_bar)obj = Foo()reuslt = obj.BAR # 自动调用get_bar方法，并获取方法的返回值print reuslt property的构造方法中有个四个参数 第一个参数是方法名，调用 对象.属性 时自动触发执行方法 第二个参数是方法名，调用 对象.属性 ＝ XXX 时自动触发执行方法 第三个参数是方法名，调用 del 对象.属性 时自动触发执行方法 第四个参数是字符串，调用 对象.属性.__doc__ ，此参数是该属性的描述信息 1234567891011121314151617181920class Foo： def get_bar(self): return &apos;wupeiqi&apos; # *必须两个参数 def set_bar(self, value): return return &apos;set value&apos; + value def del_bar(self): return &apos;wupeiqi&apos; BAR ＝ property(get_bar, set_bar, del_bar, &apos;description...&apos;)obj = Foo()obj.BAR # 自动调用第一个参数中定义的方法：get_barobj.BAR = &quot;alex&quot; # 自动调用第二个参数中定义的方法：set_bar方法，并将“alex”当作参数传入del Foo.BAR # 自动调用第三个参数中定义的方法：del_bar方法obj.BAE.__doc__ # 自动获取第四个参数中设置的值：description... 由于静态字段方式创建属性具有三种访问方式，我们可以根据他们几个属性的访问特点，分别将三个方法定义为对同一个属性：获取、修改、删除 12345678910111213141516171819202122232425class Goods(object): def __init__(self): # 原价 self.original_price = 100 # 折扣 self.discount = 0.8 def get_price(self): # 实际价格 = 原价 * 折扣 new_price = self.original_price * self.discount return new_price def set_price(self, value): self.original_price = value def del_price(self, value): del self.original_price PRICE = property(get_price, set_price, del_price, &apos;价格属性描述...&apos;)obj = Goods()obj.PRICE # 获取商品价格obj.PRICE = 200 # 修改商品原价del obj.PRICE # 删除商品原价 注意：Python WEB框架 Django 的视图中 request.POST 就是使用的静态字段的方式创建的属性 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576Django源码class WSGIRequest(http.HttpRequest): def __init__(self, environ): script_name = get_script_name(environ) path_info = get_path_info(environ) if not path_info: # Sometimes PATH_INFO exists, but is empty (e.g. accessing # the SCRIPT_NAME URL without a trailing slash). We really need to # operate as if they&apos;d requested &apos;/&apos;. Not amazingly nice to force # the path like this, but should be harmless. path_info = &apos;/&apos; self.environ = environ self.path_info = path_info self.path = &apos;%s/%s&apos; % (script_name.rstrip(&apos;/&apos;), path_info.lstrip(&apos;/&apos;)) self.META = environ self.META[&apos;PATH_INFO&apos;] = path_info self.META[&apos;SCRIPT_NAME&apos;] = script_name self.method = environ[&apos;REQUEST_METHOD&apos;].upper() _, content_params = cgi.parse_header(environ.get(&apos;CONTENT_TYPE&apos;, &apos;&apos;)) if &apos;charset&apos; in content_params: try: codecs.lookup(content_params[&apos;charset&apos;]) except LookupError: pass else: self.encoding = content_params[&apos;charset&apos;] self._post_parse_error = False try: content_length = int(environ.get(&apos;CONTENT_LENGTH&apos;)) except (ValueError, TypeError): content_length = 0 self._stream = LimitedStream(self.environ[&apos;wsgi.input&apos;], content_length) self._read_started = False self.resolver_match = None def _get_scheme(self): return self.environ.get(&apos;wsgi.url_scheme&apos;) def _get_request(self): warnings.warn(&apos;`request.REQUEST` is deprecated, use `request.GET` or &apos; &apos;`request.POST` instead.&apos;, RemovedInDjango19Warning, 2) if not hasattr(self, &apos;_request&apos;): self._request = datastructures.MergeDict(self.POST, self.GET) return self._request @cached_property def GET(self): # The WSGI spec says &apos;QUERY_STRING&apos; may be absent. raw_query_string = get_bytes_from_wsgi(self.environ, &apos;QUERY_STRING&apos;, &apos;&apos;) return http.QueryDict(raw_query_string, encoding=self._encoding) # ############### 看这里看这里 ############### def _get_post(self): if not hasattr(self, &apos;_post&apos;): self._load_post_and_files() return self._post # ############### 看这里看这里 ############### def _set_post(self, post): self._post = post @cached_property def COOKIES(self): raw_cookie = get_str_from_wsgi(self.environ, &apos;HTTP_COOKIE&apos;, &apos;&apos;) return http.parse_cookie(raw_cookie) def _get_files(self): if not hasattr(self, &apos;_files&apos;): self._load_post_and_files() return self._files # ############### 看这里看这里 ############### POST = property(_get_post, _set_post) FILES = property(_get_files) REQUEST = property(_get_request) 所以，定义属性共有两种方式，分别是【装饰器】和【静态字段】，而【装饰器】方式针对经典类和新式类又有所不同。 类成员的修饰符类的所有成员在上一步骤中已经做了详细的介绍，对于每一个类的成员而言都有两种形式： 公有成员，在任何地方都能访问 私有成员，只有在类的内部才能方法 私有成员和公有成员的定义不同：私有成员命名时，前两个字符是下划线。（特殊成员除外，例如：init、call、dict等） 12345class C: def __init__(self): self.name = &apos;公有字段&apos; self.__foo = &quot;私有字段&quot; 私有成员和公有成员的访问限制不同： 静态字段 公有静态字段：类可以访问；类内部可以访问；派生类中可以访问 私有静态字段：仅类内部可以访问； 123456789101112131415161718192021公有静态字段class C: name = &quot;公有静态字段&quot; def func(self): print C.nameclass D(C): def show(self): print C.nameC.name # 类访问obj = C()obj.func() # 类内部可以访问obj_son = D()obj_son.show() # 派生类中可以访问 123456789101112131415161718192021私有静态字段class C: __name = &quot;公有静态字段&quot; def func(self): print C.__nameclass D(C): def show(self): print C.__nameC.__name # 类访问 ==&gt; 错误obj = C()obj.func() # 类内部可以访问 ==&gt; 正确obj_son = D()obj_son.show() # 派生类中可以访问 ==&gt; 错误 普通字段 公有普通字段：对象可以访问；类内部可以访问；派生类中可以访问 私有普通字段：仅类内部可以访问； ps：如果想要强制访问私有字段，可以通过 【对象._类名私有字段明 】访问（如：obj._Cfoo），不建议强制访问私有成员。 123456789101112131415161718192021公有字段class C: def __init__(self): self.foo = &quot;公有字段&quot; def func(self): print self.foo # 类内部访问class D(C): def show(self): print self.foo ＃ 派生类中访问obj = C()obj.foo # 通过对象访问obj.func() # 类内部访问obj_son = D();obj_son.show() # 派生类中访问 123456789101112131415161718192021私有字段class C: def __init__(self): self.__foo = &quot;私有字段&quot; def func(self): print self.foo # 类内部访问class D(C): def show(self): print self.foo ＃ 派生类中访问obj = C()obj.__foo # 通过对象访问 ==&gt; 错误obj.func() # 类内部访问 ==&gt; 正确obj_son = D();obj_son.show() # 派生类中访问 ==&gt; 错误 方法、属性的访问于上述方式相似，即：私有成员只能在类内部使用 ps：非要访问私有属性的话，可以通过 对象._类__属性名 类的特殊成员上文介绍了Python的类成员以及成员修饰符，从而了解到类中有字段、方法和属性三大类成员，并且成员名前如果有两个下划线，则表示该成员是私有成员，私有成员只能由类内部调用。无论人或事物往往都有不按套路出牌的情况，Python的类成员也是如此，存在着一些具有特殊含义的成员，详情如下： 1. doc 表示类的描述信息 12345678class Foo: &quot;&quot;&quot; 描述类信息，这是用于看片的神奇 &quot;&quot;&quot; def func(self): passprint Foo.__doc__#输出：类的描述信息 2. module 和 class module 表示当前操作的对象在那个模块 class 表示当前操作的对象的类是什么 12345678lib/aa.py#!/usr/bin/env python# -*- coding:utf-8 -*-class C: def __init__(self): self.name = &apos;wupeiqi&apos; 123456index.pyfrom lib.aa import Cobj = C()print obj.__module__ # 输出 lib.aa，即：输出模块print obj.__class__ # 输出 lib.aa.C，即：输出类 3. init 构造方法，通过类创建对象时，自动触发执行。 12345678class Foo: def __init__(self, name): self.name = name self.age = 18obj = Foo(&apos;wupeiqi&apos;) # 自动执行类中的 __init__ 方法 4. del 析构方法，当对象在内存中被释放时，自动触发执行。 注：此方法一般无须定义，因为Python是一门高级语言，程序员在使用时无需关心内存的分配和释放，因为此工作都是交给Python解释器来执行，所以，析构函数的调用是由解释器在进行垃圾回收时自动触发执行的。 1234class Foo: def __del__(self): pass 5. call 对象后面加括号，触发执行。 注：构造方法的执行是由创建对象触发的，即：对象 = 类名() ；而对于 call 方法的执行是由对象后加括号触发的，即：对象() 或者 类()() 123456789101112class Foo: def __init__(self): pass def __call__(self, *args, **kwargs): print &apos;__call__&apos;obj = Foo() # 执行 __init__obj() # 执行 __call__ 6. dict 类或对象中的所有成员 上文中我们知道：类的普通字段属于对象；类中的静态字段和方法等属于类，即： 123456789101112131415161718192021222324class Province: country = &apos;China&apos; def __init__(self, name, count): self.name = name self.count = count def func(self, *args, **kwargs): print &apos;func&apos;# 获取类的成员，即：静态字段、方法、print Province.__dict__# 输出：&#123;&apos;country&apos;: &apos;China&apos;, &apos;__module__&apos;: &apos;__main__&apos;, &apos;func&apos;: &lt;function func at 0x10be30f50&gt;, &apos;__init__&apos;: &lt;function __init__ at 0x10be30ed8&gt;, &apos;__doc__&apos;: None&#125;obj1 = Province(&apos;HeBei&apos;,10000)print obj1.__dict__# 获取 对象obj1 的成员# 输出：&#123;&apos;count&apos;: 10000, &apos;name&apos;: &apos;HeBei&apos;&#125;obj2 = Province(&apos;HeNan&apos;, 3888)print obj2.__dict__# 获取 对象obj1 的成员# 输出：&#123;&apos;count&apos;: 3888, &apos;name&apos;: &apos;HeNan&apos;&#125; 7. str 如果一个类中定义了str方法，那么在打印 对象 时，默认输出该方法的返回值。 123456789class Foo: def __str__(self): return &apos;wupeiqi&apos;obj = Foo()print obj# 输出：wupeiqi 8、getitem、setitem、delitem 用于索引操作，如字典。以上分别表示获取、设置、删除数据 1234567891011121314151617181920#!/usr/bin/env python# -*- coding:utf-8 -*- class Foo(object): def __getitem__(self, key): print &apos;__getitem__&apos;,key def __setitem__(self, key, value): print &apos;__setitem__&apos;,key,value def __delitem__(self, key): print &apos;__delitem__&apos;,key obj = Foo() result = obj[&apos;k1&apos;] # 自动触发执行 __getitem__obj[&apos;k2&apos;] = &apos;wupeiqi&apos; # 自动触发执行 __setitem__del obj[&apos;k1&apos;] # 自动触发执行 __delitem__ 9、getslice、setslice、delslice 该三个方法用于分片操作，如：列表 12345678910111213141516171819#!/usr/bin/env python# -*- coding:utf-8 -*- class Foo(object): def __getslice__(self, i, j): print &apos;__getslice__&apos;,i,j def __setslice__(self, i, j, sequence): print &apos;__setslice__&apos;,i,j def __delslice__(self, i, j): print &apos;__delslice__&apos;,i,j obj = Foo() obj[-1:1] # 自动触发执行 __getslice__obj[0:1] = [11,22,33,44] # 自动触发执行 __setslice__del obj[0:2] # 自动触发执行 __delslice__ 10. iter 用于迭代器，之所以列表、字典、元组可以进行for循环，是因为类型内部定义了 iter 1234567891011第一步class Foo(object): passobj = Foo()for i in obj: print i # 报错：TypeError: &apos;Foo&apos; object is not iterable 123456789101112131415第二步#!/usr/bin/env python# -*- coding:utf-8 -*-class Foo(object): def __iter__(self): passobj = Foo()for i in obj: print i# 报错：TypeError: iter() returned non-iterator of type &apos;NoneType&apos; 12345678910111213141516第三步#!/usr/bin/env python# -*- coding:utf-8 -*-class Foo(object): def __init__(self, sq): self.sq = sq def __iter__(self): return iter(self.sq)obj = Foo([11,22,33,44])for i in obj: print i 以上步骤可以看出，for循环迭代的其实是 iter([11,22,33,44]) ，所以执行流程可以变更为： 1234567#!/usr/bin/env python# -*- coding:utf-8 -*- obj = iter([11,22,33,44]) for i in obj: print i 123456789For循环语法内部#!/usr/bin/env python# -*- coding:utf-8 -*-obj = iter([11,22,33,44])while True: val = obj.next() print val 11. new 和 metaclass 阅读以下代码： 123456class Foo(object): def __init__(self): pass obj = Foo() # obj是通过Foo类实例化的对象 上述代码中，obj 是通过 Foo 类实例化的对象，其实，不仅 obj 是一个对象，Foo类本身也是一个对象，因为在Python中一切事物都是对象。 如果按照一切事物都是对象的理论：obj对象是通过执行Foo类的构造方法创建，那么Foo类对象应该也是通过执行某个类的 构造方法 创建。 12print type(obj) # 输出：&lt;class &apos;__main__.Foo&apos;&gt; 表示，obj 对象由Foo类创建print type(Foo) # 输出：&lt;type &apos;type&apos;&gt; 表示，Foo类对象由 type 类创建 所以，obj对象是Foo类的一个实例，Foo类对象是 type 类的一个实例，即：Foo类对象 是通过type类的构造方法创建。 那么，创建类就可以有两种方式： a). 普通方式 1234class Foo(object): def func(self): print &apos;hello wupeiqi&apos; b).特殊方式（type类的构造函数） 1234567def func(self): print &apos;hello wupeiqi&apos; Foo = type(&apos;Foo&apos;,(object,), &#123;&apos;func&apos;: func&#125;)#type第一个参数：类名#type第二个参数：当前类的基类#type第三个参数：类的成员 ＝＝》 类 是由 type 类实例化产生 那么问题来了，类默认是由 type 类实例化产生，type类中如何实现的创建类？类又是如何创建对象？ 答：类中有一个属性 metaclass，其用来表示该类由 谁 来实例化创建，所以，我们可以为 metaclass 设置一个type类的派生类，从而查看 类 创建的过程。 1234567891011121314151617181920212223class MyType(type): def __init__(self, what, bases=None, dict=None): super(MyType, self).__init__(what, bases, dict) def __call__(self, *args, **kwargs): obj = self.__new__(self, *args, **kwargs) self.__init__(obj)class Foo(object): __metaclass__ = MyType def __init__(self, name): self.name = name def __new__(cls, *args, **kwargs): return object.__new__(cls, *args, **kwargs)# 第一阶段：解释器从上到下执行代码创建Foo类# 第二阶段：通过Foo类创建obj对象obj = Foo() 类的生成 调用 顺序依次是 new –&gt; init –&gt; call metaclass 详解文章：http://stackoverflow.com/questions/100003/what-is-a-metaclass-in-python 得票最高那个答案写的非常好]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python面向对象(初级篇)]]></title>
    <url>%2F2019%2F08%2F24%2Fpython%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1(%E5%88%9D%E7%BA%A7%E7%AF%87)%2F</url>
    <content type="text"><![CDATA[python面向对象(初级篇)概述 面向过程：根据业务逻辑从上到下写垒代码 函数式：将某功能代码封装到函数中，日后便无需重复编写，仅调用函数即可 面向对象：对函数进行分类和封装，让开发“更快更好更强…” 面向过程编程最易被初学者接受，其往往用一长段代码来实现指定功能，开发过程中最常见的操作就是粘贴复制，即：将之前实现的代码块复制到现需功能处。 1234567891011121314151617while True： if cpu利用率 &gt; 90%: #发送邮件提醒 连接邮箱服务器 发送邮件 关闭连接 if 硬盘使用空间 &gt; 90%: #发送邮件提醒 连接邮箱服务器 发送邮件 关闭连接 if 内存占用 &gt; 80%: #发送邮件提醒 连接邮箱服务器 发送邮件 随着时间的推移，开始使用了函数式编程，增强代码的重用性和可读性，就变成了这样： 12345678910111213141516def 发送邮件(内容) #发送邮件提醒 连接邮箱服务器 发送邮件 关闭连接 while True： if cpu利用率 &gt; 90%: 发送邮件(&apos;CPU报警&apos;) if 硬盘使用空间 &gt; 90%: 发送邮件(&apos;硬盘报警&apos;) if 内存占用 &gt; 80%: 发送邮件(&apos;内存报警&apos;) 今天我们来学习一种新的编程方式：面向对象编程（Object Oriented Programming，OOP，面向对象程序设计）注：Java和C#来说只支持面向对象编程，而python比较灵活即支持面向对象编程也支持函数式编程 创建类和对象面向对象编程是一种编程方式，此编程方式的落地需要使用 “类” 和 “对象” 来实现，所以，面向对象编程其实就是对 “类” 和 “对象” 的使用。 类就是一个模板，模板里可以包含多个函数，函数里实现一些功能 对象则是根据模板创建的实例，通过实例对象可以执行类中的函数 class是关键字，表示类 创建对象，类名称后加括号即可 ps：类中的函数第一个参数必须是self（详细见：类的三大特性之封装） 类中定义的函数叫做 “方法” 12345678910111213# 创建类class Foo: def Bar(self): print &apos;Bar&apos; def Hello(self, name): print &apos;i am %s&apos; %name # 根据类Foo创建对象objobj = Foo()obj.Bar() #执行Bar方法obj.Hello(&apos;wupeiqi&apos;) #执行Hello方法 *诶，你在这里是不是有疑问了？使用函数式编程和面向对象编程方式来执行一个“方法”时函数要比面向对象简便* 面向对象：【创建对象】【通过对象执行方法】 函数编程：【执行函数】 观察上述对比答案则是肯定的，然后并非绝对，场景的不同适合其的编程方式也不同。 总结：函数式的应用场景 –&gt; 各个函数之间是独立且无共用的数据 面向对象三大特性面向对象的三大特性是指：封装、继承和多态。 一、封装 封装，顾名思义就是将内容封装到某个地方，以后再去调用被封装在某处的内容。 所以，在使用面向对象的封装特性时，需要： 将内容封装到某处 从某处调用被封装的内容 第一步：将内容封装到某处 self 是一个形式参数，当执行 obj1 = Foo(‘wupeiqi’, 18 ) 时，self 等于 obj1 ​ 当执行 obj2 = Foo(‘alex’, 78 ) 时，self 等于 obj2 所以，内容其实被封装到了对象 obj1 和 obj2 中，每个对象中都有 name 和 age 属性，在内存里类似于下图来保存。 第二步：从某处调用被封装的内容 调用被封装的内容时，有两种情况： 通过对象直接调用 通过self间接调用 1、通过对象直接调用被封装的内容 上图展示了对象 obj1 和 obj2 在内存中保存的方式，根据保存格式可以如此调用被封装的内容：对象.属性名 12345678910111213class Foo: def __init__(self, name, age): self.name = name self.age = age obj1 = Foo(&apos;wupeiqi&apos;, 18)print obj1.name # 直接调用obj1对象的name属性print obj1.age # 直接调用obj1对象的age属性 obj2 = Foo(&apos;alex&apos;, 73)print obj2.name # 直接调用obj2对象的name属性print obj2.age # 直接调用obj2对象的age属性 2、通过self间接调用被封装的内容 执行类中的方法时，需要通过self间接调用被封装的内容 123456789101112131415class Foo: def __init__(self, name, age): self.name = name self.age = age def detail(self): print self.name print self.age obj1 = Foo(&apos;wupeiqi&apos;, 18)obj1.detail() # Python默认会将obj1传给self参数，即：obj1.detail(obj1)，所以，此时方法内部的 self ＝ obj1，即：self.name 是 wupeiqi ；self.age 是 18 obj2 = Foo(&apos;alex&apos;, 73)obj2.detail() # Python默认会将obj2传给self参数，即：obj1.detail(obj2)，所以，此时方法内部的 self ＝ obj2，即：self.name 是 alex ； self.age 是 78 综上所述，对于面向对象的封装来说，其实就是使用构造方法将内容封装到 对象 中，然后通过对象直接或者self间接获取被封装的内容。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960练习一：在终端输出如下信息小明，10岁，男，上山去砍柴小明，10岁，男，开车去东北小明，10岁，男，最爱大保健老李，90岁，男，上山去砍柴老李，90岁，男，开车去东北老李，90岁，男，最爱大保健老张...函数式编程def kanchai(name, age, gender): print &quot;%s,%s岁,%s,上山去砍柴&quot; %(name, age, gender)def qudongbei(name, age, gender): print &quot;%s,%s岁,%s,开车去东北&quot; %(name, age, gender)def dabaojian(name, age, gender): print &quot;%s,%s岁,%s,最爱大保健&quot; %(name, age, gender)kanchai(&apos;小明&apos;, 10, &apos;男&apos;)qudongbei(&apos;小明&apos;, 10, &apos;男&apos;)dabaojian(&apos;小明&apos;, 10, &apos;男&apos;)kanchai(&apos;老李&apos;, 90, &apos;男&apos;)qudongbei(&apos;老李&apos;, 90, &apos;男&apos;)dabaojian(&apos;老李&apos;, 90, &apos;男&apos;)面向对象class Foo: def __init__(self, name, age ,gender): self.name = name self.age = age self.gender = gender def kanchai(self): print &quot;%s,%s岁,%s,上山去砍柴&quot; %(self.name, self.age, self.gender) def qudongbei(self): print &quot;%s,%s岁,%s,开车去东北&quot; %(self.name, self.age, self.gender) def dabaojian(self): print &quot;%s,%s岁,%s,最爱大保健&quot; %(self.name, self.age, self.gender)xiaoming = Foo(&apos;小明&apos;, 10, &apos;男&apos;)xiaoming.kanchai()xiaoming.qudongbei()xiaoming.dabaojian()laoli = Foo(&apos;老李&apos;, 90, &apos;男&apos;)laoli.kanchai()laoli.qudongbei()laoli.dabaojian() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273练习二：游戏人生程序1、创建三个游戏人物，分别是：苍井井，女，18，初始战斗力1000东尼木木，男，20，初始战斗力1800波多多，女，19，初始战斗力25002、游戏场景，分别：草丛战斗，消耗200战斗力自我修炼，增长100战斗力多人游戏，消耗500战斗力游戏人生# -*- coding:utf-8 -*-# ##################### 定义实现功能的类 #####################class Person: def __init__(self, na, gen, age, fig): self.name = na self.gender = gen self.age = age self.fight =fig def grassland(self): &quot;&quot;&quot;注释：草丛战斗，消耗200战斗力&quot;&quot;&quot; self.fight = self.fight - 200 def practice(self): &quot;&quot;&quot;注释：自我修炼，增长100战斗力&quot;&quot;&quot; self.fight = self.fight + 200 def incest(self): &quot;&quot;&quot;注释：多人游戏，消耗500战斗力&quot;&quot;&quot; self.fight = self.fight - 500 def detail(self): &quot;&quot;&quot;注释：当前对象的详细情况&quot;&quot;&quot; temp = &quot;姓名:%s ; 性别:%s ; 年龄:%s ; 战斗力:%s&quot; % (self.name, self.gender, self.age, self.fight) print temp # ##################### 开始游戏 #####################cang = Person(&apos;苍井井&apos;, &apos;女&apos;, 18, 1000) # 创建苍井井角色dong = Person(&apos;东尼木木&apos;, &apos;男&apos;, 20, 1800) # 创建东尼木木角色bo = Person(&apos;波多多&apos;, &apos;女&apos;, 19, 2500) # 创建波多多角色cang.incest() #苍井空参加一次多人游戏dong.practice()#东尼木木自我修炼了一次bo.grassland() #波多多参加一次草丛战斗#输出当前所有人的详细情况cang.detail()dong.detail()bo.detail()cang.incest() #苍井空又参加一次多人游戏dong.incest() #东尼木木也参加了一个多人游戏bo.practice() #波多多自我修炼了一次#输出当前所有人的详细情况cang.detail()dong.detail()bo.detail() 二、继承 继承，面向对象中的继承和现实生活中的继承相同，即：子可以继承父的内容。 例如： 猫可以：喵喵叫、吃、喝、拉、撒 狗可以：汪汪叫、吃、喝、拉、撒 如果我们要分别为猫和狗创建一个类，那么就需要为 猫 和 狗 实现他们所有的功能，如下所示： 123456789101112131415161718192021222324252627282930313233class 猫： def 喵喵叫(self): print &apos;喵喵叫&apos; def 吃(self): # do something def 喝(self): # do something def 拉(self): # do something def 撒(self): # do somethingclass 狗： def 汪汪叫(self): print &apos;喵喵叫&apos; def 吃(self): # do something def 喝(self): # do something def 拉(self): # do something def 撒(self): # do something 上述代码不难看出，吃、喝、拉、撒是猫和狗都具有的功能，而我们却分别的猫和狗的类中编写了两次。如果使用 继承 的思想，如下实现： 动物：吃、喝、拉、撒 猫：喵喵叫（猫继承动物的功能） 狗：汪汪叫（狗继承动物的功能） 1234567891011121314151617181920212223242526伪代码class 动物: def 吃(self): # do something def 喝(self): # do something def 拉(self): # do something def 撒(self): # do something# 在类后面括号中写入另外一个类名，表示当前类继承另外一个类class 猫(动物)： def 喵喵叫(self): print &apos;喵喵叫&apos; # 在类后面括号中写入另外一个类名，表示当前类继承另外一个类class 狗(动物)： def 汪汪叫(self): print &apos;喵喵叫&apos; 123456789101112131415161718192021222324252627282930313233343536373839404142434445代码实例class Animal: def eat(self): print &quot;%s 吃 &quot; %self.name def drink(self): print &quot;%s 喝 &quot; %self.name def shit(self): print &quot;%s 拉 &quot; %self.name def pee(self): print &quot;%s 撒 &quot; %self.nameclass Cat(Animal): def __init__(self, name): self.name = name self.breed ＝ &apos;猫&apos; def cry(self): print &apos;喵喵叫&apos;class Dog(Animal): def __init__(self, name): self.name = name self.breed ＝ &apos;狗&apos; def cry(self): print &apos;汪汪叫&apos; # ######### 执行 #########c1 = Cat(&apos;小白家的小黑猫&apos;)c1.eat()c2 = Cat(&apos;小黑的小白猫&apos;)c2.drink()d1 = Dog(&apos;胖子家的小瘦狗&apos;)d1.eat() 所以，对于面向对象的继承来说，其实就是将多个类共有的方法提取到父类中，子类仅需继承父类而不必一一实现每个方法。 注：除了子类和父类的称谓，你可能看到过 派生类 和 基类 ，他们与子类和父类只是叫法不同而已。 学习了继承的写法之后，我们用代码来是上述阿猫阿狗的功能： 123456789101112131415161718192021222324252627282930313233343536373839404142434445代码实例class Animal: def eat(self): print &quot;%s 吃 &quot; %self.name def drink(self): print &quot;%s 喝 &quot; %self.name def shit(self): print &quot;%s 拉 &quot; %self.name def pee(self): print &quot;%s 撒 &quot; %self.nameclass Cat(Animal): def __init__(self, name): self.name = name self.breed ＝ &apos;猫&apos; def cry(self): print &apos;喵喵叫&apos;class Dog(Animal): def __init__(self, name): self.name = name self.breed ＝ &apos;狗&apos; def cry(self): print &apos;汪汪叫&apos; # ######### 执行 #########c1 = Cat(&apos;小白家的小黑猫&apos;)c1.eat()c2 = Cat(&apos;小黑的小白猫&apos;)c2.drink()d1 = Dog(&apos;胖子家的小瘦狗&apos;)d1.eat() 那么问题又来了，多继承呢？ 是否可以继承多个类 如果继承的多个类每个类中都定了相同的函数，那么那一个会被使用呢？ 1、Python的类可以继承多个类，Java和C#中则只能继承一个类 2、Python的类如果继承了多个类，那么其寻找方法的方式有两种，分别是：深度优先和广度优先 当类是经典类时，多继承情况下，会按照深度优先方式查找 当类是新式类时，多继承情况下，会按照广度优先方式查找 经典类和新式类，从字面上可以看出一个老一个新，新的必然包含了跟多的功能，也是之后推荐的写法，从写法上区分的话，如果 当前类或者父类继承了object类，那么该类便是新式类，否则便是经典类。 123456789101112131415161718192021222324252627282930经典类多继承class D: def bar(self): print &apos;D.bar&apos;class C(D): def bar(self): print &apos;C.bar&apos;class B(D): def bar(self): print &apos;B.bar&apos;class A(B, C): def bar(self): print &apos;A.bar&apos;a = A()# 执行bar方法时# 首先去A类中查找，如果A类中没有，则继续去B类中找，如果B类中么有，则继续去D类中找，如果D类中么有，则继续去C类中找，如果还是未找到，则报错# 所以，查找顺序：A --&gt; B --&gt; D --&gt; C# 在上述查找bar方法的过程中，一旦找到，则寻找过程立即中断，便不会再继续找了a.bar() 123456789101112131415161718192021222324252627282930新式类多继承class D(object): def bar(self): print &apos;D.bar&apos;class C(D): def bar(self): print &apos;C.bar&apos;class B(D): def bar(self): print &apos;B.bar&apos;class A(B, C): def bar(self): print &apos;A.bar&apos;a = A()# 执行bar方法时# 首先去A类中查找，如果A类中没有，则继续去B类中找，如果B类中么有，则继续去C类中找，如果C类中么有，则继续去D类中找，如果还是未找到，则报错# 所以，查找顺序：A --&gt; B --&gt; C --&gt; D# 在上述查找bar方法的过程中，一旦找到，则寻找过程立即中断，便不会再继续找了a.bar() 经典类：首先去A类中查找，如果A类中没有，则继续去B类中找，如果B类中么有，则继续去D类中找，如果D类中么有，则继续去C类中找，如果还是未找到，则报错 新式类：首先去A类中查找，如果A类中没有，则继续去B类中找，如果B类中么有，则继续去C类中找，如果C类中么有，则继续去D类中找，如果还是未找到，则报错 注意：在上述查找过程中，一旦找到，则寻找过程立即中断，便不会再继续找了 三、多态 Pyhon不支持Java和C#这一类强类型语言中多态的写法，但是原生多态，其Python崇尚“鸭子类型”。 12345678910111213141516171819202122232425262728293031 Python伪代码实现Java或C#的多态 class F1: passclass S1(F1): def show(self): print &apos;S1.show&apos;class S2(F1): def show(self): print &apos;S2.show&apos;# 由于在Java或C#中定义函数参数时，必须指定参数的类型# 为了让Func函数既可以执行S1对象的show方法，又可以执行S2对象的show方法，所以，定义了一个S1和S2类的父类# 而实际传入的参数是：S1对象和S2对象def Func(F1 obj): &quot;&quot;&quot;Func函数需要接收一个F1类型或者F1子类的类型&quot;&quot;&quot; print obj.show() s1_obj = S1()Func(s1_obj) # 在Func函数中传入S1类的对象 s1_obj，执行 S1 的show方法，结果：S1.shows2_obj = S2()Func(s2_obj) # 在Func函数中传入Ss类的对象 ss_obj，执行 Ss 的show方法，结果：S2.show 123456789101112131415161718192021222324Python “鸭子类型”class F1: passclass S1(F1): def show(self): print &apos;S1.show&apos;class S2(F1): def show(self): print &apos;S2.show&apos;def Func(obj): print obj.show()s1_obj = S1()Func(s1_obj) s2_obj = S2()Func(s2_obj) 总结以上就是本节对于面向对象初级知识的介绍，总结如下： 面向对象是一种编程方式，此编程方式的实现是基于对 类 和 对象 的使用 类 是一个模板，模板中包装了多个“函数”供使用 对象，根据模板创建的实例（即：对象），实例用于调用被包装在类中的函数 面向对象三大特性：封装、继承和多态 问答专区 问题一：什么样的代码才是面向对象？ 答：从简单来说，如果程序中的所有功能都是用 类 和 对象 来实现，那么就是面向对象编程了。 问题二：函数式编程 和 面向对象 如何选择？分别在什么情况下使用？ 答：须知：对于 C# 和 Java 程序员来说不存在这个问题，因为该两门语言只支持面向对象编程（不支持函数式编程）。而对于 Python 和 PHP 等语言却同时支持两种编程方式，且函数式编程能完成的操作，面向对象都可以实现；而面向对象的能完成的操作，函数式编程不行（函数式编程无法实现面向对象的封装功能）。 所以，一般在Python开发中，*全部使用面向对象** 或 面向对象和函数式混合使用* 面向对象的应用场景: 多函数需使用共同的值，如：数据库的增、删、改、查操作都需要连接数据库字符串、主机名、用户名和密码 12345678910111213141516171819202122232425262728Democlass SqlHelper: def __init__(self, host, user, pwd): self.host = host self.user = user self.pwd = pwd def 增(self): # 使用主机名、用户名、密码（self.host 、self.user 、self.pwd）打开数据库连接 # do something # 关闭数据库连接 def 删(self): # 使用主机名、用户名、密码（self.host 、self.user 、self.pwd）打开数据库连接 # do something # 关闭数据库连接 def 改(self): # 使用主机名、用户名、密码（self.host 、self.user 、self.pwd）打开数据库连接 # do something # 关闭数据库连接 def 查(self): # 使用主机名、用户名、密码（self.host 、self.user 、self.pwd）打开数据库连接 # do something # 关闭数据库连接# do something 需要创建多个事物，每个事物属性个数相同，但是值的需求如：张三、李四、杨五，他们都有姓名、年龄、血型，但其都是不相同。即：属性个数相同，但值不相同 1234567891011121314151617democlass Person: def __init__(self, name ,age ,blood_type): self.name = name self.age = age self.blood_type = blood_type def detail(self): temp = &quot;i am %s, age %s , blood type %s &quot; % (self.name, self.age, self.blood_type) print tempzhangsan = Person(&apos;张三&apos;, 18, &apos;A&apos;)lisi = Person(&apos;李四&apos;, 73, &apos;AB&apos;)yangwu = Person(&apos;杨五&apos;, 84, &apos;A&apos;) 问题三：类和对象在内存中是如何保存？ 答：类以及类中的方法在内存中只有一份，而根据类创建的每一个对象都在内存中需要存一份，大致如下图： 如上图所示，根据类创建对象时，对象中除了封装 name 和 age 的值之外，还会保存一个类对象指针，该值指向当前对象的类。 当通过 obj1 执行 【方法一】 时，过程如下： 根据当前对象中的 类对象指针 找到类中的方法 将对象 obj1 当作参数传给 方法的第一个参数 self]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的闭包与装饰器]]></title>
    <url>%2F2019%2F08%2F23%2Fpython%E4%B8%AD%E7%9A%84%E9%97%AD%E5%8C%85%E4%B8%8E%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[python中的闭包与装饰器1.Python中一切皆对象这恐怕是学习Python最有用的一句话。想必你已经知道Python中的list, tuple, dict等内置数据结构，当你执行： 1alist = [1, 2, 3] 时，你就创建了一个列表对象，并且用alist这个变量引用它： 当然你也可以自己定义一个类： 12345678class House(object): def __init__(self, area, city): self.area = area self.city = city def sell(self, price): [...] #other code return price 然后创建一个类的对象： 1house = House(200, &apos;Shanghai&apos;) OK，你立马就在上海有了一套200平米的房子，它有一些属性(area, city)，和一些方法(init, self)： 2.函数式第一类对象和list, tuple, dict以及用House创建的对象一样，当你定义一个函数时，函数也是对象： 12def func(a, b): return a+b 在全局域，函数对象被函数名引用着，它接收两个参数a和b，计算这两个参数的和作为返回值。所谓第一类对象，意思是可以用标识符给对象命名，并且对象可以被当作数据处理，例如赋值、作为参数传递给函数，或者作为返回值return 等 因此，你完全可以用其他变量名引用这个函数对象： 1add = func1 这样，你就可以像调用func(1, 2)一样，通过新的引用调用函数了： 12print func(1, 2)print add(1, 2) #the same as func(1, 2)12 或者将函数对象作为参数，传递给另一个函数： 12345def caller_func(f): return f(1, 2)if __name__ == &quot;__main__&quot;: print caller_func(func)12345 可以看到，1.函数对象func作为参数传递给caller_func函数，传参过程类似于一个赋值操作f=func；2.于是func函数对象，被caller_func函数作用域中的局部变量f引用，f实际指向了函数func；cc3.当执行return f(1, 2)的时候，相当于执行了return func(1, 2)；因此输出结果为3。 3.函数对象 vs 函数调用无论是把函数赋值给新的标识符，还是作为参数传递给新的函数，针对的都是函数对象本身，而不是函数的调用。用一个更加简单，但从外观上看，更容易产生混淆的例子来说明这个问题。例如定义了下面这个函数： 12def func(): return &quot;hello,world&quot; 然后分别执行两次赋值： 12ref1 = func #将函数对象赋值给ref1ref2 = func() #调用函数，将函数的返回值(&quot;hello,world&quot;字符串)赋值给ref2 很多初学者会混淆这两种赋值，通过Python内建的type函数，可以查看一下这两次赋值的结果： 12345In [4]: type(ref1)Out[4]: functionIn [5]: type(ref2)Out[5]: str 可以看到，ref1引用了函数对象本身，而ref2则引用了函数的返回值。通过内建的callable函数，可以进一步验证ref1是可调用的，而ref2是不可调用的： 12345In [9]: callable(ref1)Out[9]: TrueIn [10]: callable(ref2)Out[10]: False 传参的效果与之类似。 4.闭包&amp;LEGB法则所谓闭包，就是将组成函数的语句和这些语句的执行环境打包在一起时，得到的对象听上去的确有些复杂，还是用一个栗子来帮助理解一下。假设我们在foo.py模块中做了如下定义： 12345#foo.pyfilename = &quot;foo.py&quot;def call_func(f): return f() #如前面介绍的，f引用一个函数对象，然后调用它 在另一个func.py模块中，写下了这样的代码： 123456789#func.pyimport foo #导入foo.pyfilename = &quot;func.py&quot;def show_filename(): return &quot;filename: %s&quot; % filenameif __name__ == &quot;__main__&quot;: print foo.call_func(show_filename) #注意：实际发生调用的位置，是在foo.call_func函数中 当我们用python func.py命令执行func.py时输出结果为： 12chiyu@chiyu-PC:~$ python func.py filename:func.py 很显然show_filename()函数使用的filename变量的值，是在与它相同环境(func.py模块)中定义的那个。尽管foo.py模块中也定义了同名的filename变量，而且实际调用show_filename的位置也是在foo.py的call_func内部。而对于嵌套函数，这一机制则会表现的更加明显：闭包将会捕捉内层函数执行所需的整个环境： 1234567#enclosed.pyimport foodef wrapper(): filename = &quot;enclosed.py&quot; def show_filename(): return &quot;filename: %s&quot; % filename print foo.call_func(show_filename) #输出：filename: enclosed.py 实际上，每一个函数对象，都有一个指向了该函数定义时所在全局名称空间的globals属性： 123456789101112#show_filename inside wrapper#show_filename.__globals__&#123;&apos;__builtins__&apos;: &lt;module &apos;__builtin__&apos; (built-in)&gt;, #内建作用域环境&apos;__file__&apos;: &apos;enclosed.py&apos;, &apos;wrapper&apos;: &lt;function wrapper at 0x7f84768b6578&gt;, #直接外围环境&apos;__package__&apos;: None, &apos;__name__&apos;: &apos;__main__&apos;, &apos;foo&apos;: &lt;module &apos;foo&apos; from &apos;/home/chiyu/foo.pyc&apos;&gt;, #全局环境&apos;__doc__&apos;: None &#125; 当代码执行到show_filename中的return “filename: %s” % filename语句时，解析器按照下面的顺序查找filename变量：1.Local - 本地函数(show_filename)内部，通过任何方式赋值的，而且没有被global关键字声明为全局变量的filename变量；2.Enclosing - 直接外围空间(上层函数wrapper)的本地作用域，查找filename变量(如果有多层嵌套，则由内而外逐层查找，直至最外层的函数)；3.Global - 全局空间(模块enclosed.py)，在模块顶层赋值的filename变量；4.Builtin - 内置模块(builtin)中预定义的变量名中查找filename变量； 在任何一层先找到了符合要求的filename变量，则不再向更外层查找。如果直到Builtin层仍然没有找到符合要求的变量，则抛出NameError异常。这就是变量名解析的：LEGB法则。 总结：1.闭包最重要的使用价值在于：封存函数执行的上下文环境；2.闭包在其捕捉的执行环境(def语句块所在上下文)中，也遵循LEGB规则逐层查找，直至找到符合要求的变量，或者抛出异常。 5.装饰器&amp;语法糖(syntax sugar)那么闭包和装饰器又有什么关系呢？上文提到闭包的重要特性：封存上下文，这一特性可以巧妙的被用于现有函数的包装，从而为现有函数更加功能。而这就是装饰器。还是举个例子，代码如下： 123#alist = [1, 2, 3, ..., 100] --&gt; 1+2+3+...+100 = 5050def lazy_sum(): return reduce(lambda x, y: x+y, alist) 我们定义了一个函数lazy_sum，作用是对alist中的所有元素求和后返回。alist假设为1到100的整数列表： 1alist = range(1, 101) 但是出于某种原因，我并不想马上返回计算结果，而是在之后的某个地方，通过显示的调用输出结果。于是我用一个wrapper函数对其进行包装： 12345678910def wrapper(): alist = range(1, 101) def lazy_sum(): return reduce(lambda x, y: x+y, alist) return lazy_sumlazy_sum = wrapper() #wrapper() 返回的是lazy_sum函数对象if __name__ == &quot;__main__&quot;: lazy_sum() #5050 这是一个典型的Lazy Evaluation的例子。我们知道，一般情况下，局部变量在函数返回时，就会被垃圾回收器回收，而不能再被使用。但是这里的alist却没有，它随着lazy_sum函数对象的返回被一并返回了(这个说法不准确，实际是包含在了lazy_sum的执行环境中，通过globals)，从而延长了生命周期。 当在if语句块中调用lazy_sum()的时候，解析器会从上下文中(这里是Enclosing层的wrapper函数的局部作用域中)找到alist列表，计算结果，返回5050。 当你需要动态的给已定义的函数增加功能时，比如：参数检查，类似的原理就变得很有用： 12def add(a, b): return a+b 这是很简单的一个函数：计算a+b的和返回，但我们知道Python是 动态类型+强类型 的语言，你并不能保证用户传入的参数a和b一定是两个整型，他有可能传入了一个整型和一个字符串类型的值： 1234567891011121314151617In [2]: add(1, 2)Out[2]: 3In [3]: add(1.2, 3.45)Out[3]: 4.65In [4]: add(5, &apos;hello&apos;)---------------------------------------------------------------------------TypeError Traceback (most recent call last)/home/chiyu/&lt;ipython-input-4-f2f9e8aa5eae&gt; in &lt;module&gt;()----&gt; 1 add(5, &apos;hello&apos;)/home/chiyu/&lt;ipython-input-1-02b3d3d6caec&gt; in add(a, b) 1 def add(a, b):----&gt; 2 return a+bTypeError: unsupported operand type(s) for +: &apos;int&apos; and &apos;str&apos; 于是，解析器无情的抛出了一个TypeError异常。1.动态类型：在运行期间确定变量的类型，python确定一个变量的类型是在你第一次给他赋值的时候；2.强类型：有强制的类型定义，你有一个整数，除非显示的类型转换，否则绝不能将它当作一个字符串(例如直接尝试将一个整型和一个字符串做+运算)； 因此，为了更加优雅的使用add函数，我们需要在执行+运算前，对a和b进行参数检查。这时候装饰器就显得非常有用： 12345678910111213141516171819202122import logginglogging.basicConfig(level = logging.INFO)def add(a, b): return a + bdef checkParams(fn): def wrapper(a, b): if isinstance(a, (int, float)) and isinstance(b, (int, float)): #检查参数a和b是否都为整型或浮点型 return fn(a, b) #是则调用fn(a, b)返回计算结果 #否则通过logging记录错误信息，并友好退出 logging.warning(&quot;variable &apos;a&apos; and &apos;b&apos; cannot be added&quot;) return return wrapper #fn引用add，被封存在闭包的执行环境中返回if __name__ == &quot;__main__&quot;: #将add函数对象传入，fn指向add #等号左侧的add，指向checkParams的返回值wrapper add = checkParams(add) add(3, &apos;hello&apos;) #经过类型检查，不会计算结果，而是记录日志并退出 注意checkParams函数：1.首先看参数fn，当我们调用checkParams(add)的时候，它将成为函数对象add的一个本地(Local)引用；2.在checkParams内部，我们定义了一个wrapper函数，添加了参数类型检查的功能，然后调用了fn(a, b)，根据LEGB法则，解释器将搜索几个作用域，并最终在(Enclosing层)checkParams函数的本地作用域中找到fn；3.注意最后的return wrapper，这将创建一个闭包，fn变量(add函数对象的一个引用)将会封存在闭包的执行环境中，不会随着checkParams的返回而被回收； 当调用add = checkParams(add)时，add指向了新的wrapper对象，它添加了参数检查和记录日志的功能，同时又能够通过封存的fn，继续调用原始的add进行+运算。 因此调用add(3, ‘hello’)将不会返回计算结果，而是打印出日志： 12chiyu@chiyu-PC:~$ python func.py WARNING:root:variable &apos;a&apos; and &apos;b&apos; cannot be added 有人觉得add = checkParams(add)这样的写法未免太过麻烦，于是python提供了一种更优雅的写法，被称为语法糖： 123@checkParamsdef add(a, b): return a + b 这只是一种写法上的优化，解释器仍然会将它转化为add = checkParams(add)来执行。 6. 回归问题123456789def addspam(fn): def new(*args): print &quot;spam,spam,spam&quot; return fn(*args) return new@addspamdef useful(a,b): print a**2+b**2 首先看第二段代码：@addspam装饰器，相当于执行了useful = addspam(useful)。在这里题主有一个理解误区：传递给addspam的参数，是useful这个函数对象本身，而不是它的一个调用结果；再回到addspam函数体：1.return new 返回一个闭包，fn被封存在闭包的执行环境中，不会随着addspam函数的返回被回收；2.而fn此时是useful的一个引用，当执行return fn(args)时，实际相当于执行了return useful(args)；]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解python中的闭包]]></title>
    <url>%2F2019%2F08%2F23%2F%E7%90%86%E8%A7%A3python%E4%B8%AD%E7%9A%84%E9%97%AD%E5%8C%85%2F</url>
    <content type="text"><![CDATA[理解Python中的闭包1.定义闭包是函数式编程的一个重要的语法结构，函数式编程是一种编程范式(而面向过程编程和面向对象编程也都是编程范式)。在面向过程编程中，我们见过函数(function);在面向对象编程中，我们见过对象(object)。函数和对象的根本目的是对某种逻辑方式组织代码，并提高代码的可重复使用性(reusability)。闭包也是一种组织代码的结构，它同样提高了代码的可重复使用性。 不同编程语言实现闭包的方式是不同的，python中闭包从表现形式上看，如果 一个内部函数里，对在外部作用域(但不是在全局作用域)的变量进行引用，那么内部函数就被认为是闭包(clsoure)。 1234def outer(x): def inner(y): return x + y return inner 结合这段简单的代码和定义来说明闭包： inner(y)就是这个内部函数，对在外部作用域（但不是在全局作用域）的变量进行引用：x就是被引用的变量，x在外部作用域outer里面，但不在全局作用域里，则这个内部函数inner就是一个闭包。 再稍微讲究一点的解释是，闭包=函数块+定义函数时的环境，inner就是函数块，x就是环境，当然这个环境可以有很多，不止一个简单的x。 在函数outer中定义了一个inner函数，inner函数访问外部函数outer的（参数）变量，并且把inner函数作为返回值返回给outer函数。 123a = outer(2)print(&apos;function:&apos;,a) print(&apos;result:&apos;,a(3)) 上面的代码中a就是一个函数，代码的执行结果为： 从结果我们不难看出，a是函数inner而不是outer，这个有点绕，但是并不难理解，因为return回来的是inner函数。 1print(&apos;a.func_name&apos;,a.func_name) 输出结果为： 调用函数a，得到的结果是传入参数的值相加。 上面的和这句是一样的：print(&#39;result:&#39;,outer(2)(3)) 2.使用闭包注意的地方2.1闭包无法修改外部函数的局部变量 如果innerFunc可以修改x的值的话，x的值前后会发生变化，但结果是： 在innerFunc中x的值发生了改变，但是在outerFunc中x的值并未发生变化。 再来看一个例子 2.2闭包无法直接访问外部函数的局部变量12345678def outer(): x = 5 def inner(): #上面一行的x相对inner函数来说是函数外的局部变量（非全局变量） x *= x return x return innerouter()() 解决的方法： 1.在python3之前没有直接的解决方法，只能间接地通过容器类型来解决，因为容器类型不是存放在栈空间的，inner函数可以访问到。 12345678def outer(): x = [5] def inner(): x[0] *= x[0] return x[0] return inner print(outer()()) #25 2.python3通过nonlocal关键字来解决，该语句显式的指定a不是闭包的局部变量。 123456789def outer(): x = 5 def inner(): nonlocal x #把x声明为非局部变量 x *= x return x return inner print(outer()()) 3.闭包的作用 说了这么多，不免有人要问，那这个闭包在实际的开发中有什么用呢？闭包主要是在函数式开发过程中使用。以下介绍两种闭包主要的用途。 用途1：当闭包执行完后，仍然能够保持住当前的运行环境。 比如说，如果你希望函数的每次执行结果，都是基于这个函数上次的运行结果。我以一个类似棋盘游戏的例子来说明。假设棋盘大小为50*50，左上角为坐标系原点(0,0)，我需要一个函数，接收2个参数，分别为方向(direction)，步长(step)，该函数控制棋子的运动。棋子运动的新的坐标除了依赖于方向和步长以外，当然还要根据原来所处的坐标点，用闭包就可以保持住这个棋子原来所处的坐标。 12345678910111213141516171819202122232425origin = [0, 0] legal_x = [0, 50] legal_y = [0, 50] def create(pos=origin): def player(direction,step): # 这里应该首先判断参数direction,step的合法性，比如direction不能斜着走，step不能为负等 # 然后还要对新生成的x，y坐标的合法性进行判断处理，这里主要是想介绍闭包，就不详细写了。 new_x = pos[0] + direction[0]*step new_y = pos[1] + direction[1]*step pos[0] = new_x pos[1] = new_y #注意！此处不能写成 pos = [new_x, new_y]，因为参数变量不能被修改，而pos[]是容器类的解决方法 return pos return player player = create() # 创建棋子player，起点为原点 print player([1,0],10) # 向x轴正方向移动10步 print player([0,1],20) # 向y轴正方向移动20步 print player([-1,0],10) # 向x轴负方向移动10步 输出为： [10, 0] [10, 20] [0, 20] 用途2：闭包可以根据外部作用域的局部变量来得到不同的结果 这有点像一种类似配置功能的作用，我们可以修改外部的变量，闭包根据这个变量展现出不同的功能。比如有时我们需要对某些文件的特殊行进行分析，先要提取出这些特殊行。 12345678def make_filter(keep): def the_filter(file_name): file = open(file_name) lines = file.readlines() file.close() filter_doc = [i for i in lines if keep in i] return filter_doc return the_filter 如果我们需要取得文件”result.txt”中含有”pass”关键字的行，则可以这样使用例子程序 1filter = make_filter(&quot;pass&quot;) filter_result = filter(&quot;result.txt&quot;) 以上两种使用场景，用面向对象也是可以很简单的实现的，但是在用Python进行函数式编程时，闭包对数据的持久化以及按配置产生不同的功能，是很有帮助的。]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ftp服务器快速搭建]]></title>
    <url>%2F2019%2F08%2F22%2Fftp%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[一行代码架设个简单的FTP服务器 安装环境因为这个组件 python 默认没有 , 所以要先安装下 1pip install pyftpdlib 使用最简单的. 请放到你要 ftp 的目录 , 它会把该目录变成根目录 . 并且匿名访问 : 默认端口是 2121 1python -m pyftpdlib 如果需要使用用户名和密码1python -m pyftpdlib -u 91yun -P www.91yun.co -u 是指定用户名 -P 是指定密码 ( 这个 P 是大写 ). 这个语句的含义就是 : 用户名是 91yun, 密码是 http://www.91yun.co 后台运行1nohup python -m pyftpdlib -u 91yun -P www.91yun.co &amp; 后台运行后 , 如果要删除进程关闭 ftp 的话可以执行命令 : 1ps aux|grep pyftpdlib|awk &apos;&#123;print $2&#125;&apos;|xargs kill -9 开机启动把后台运行的那个代码写入 /etc/rc.local 1echo &quot;nohup python -m pyftpdlib -u 91yun -p www.91yun.co &amp;&quot; &gt;&gt; /etc/rc.local 其他一些参数 除上之外，还有一些可选参数： p 指定端口（默认为 2121 ） w 写权限（默认为只读） d 指定目录 （默认为当前目录]]></content>
      <categories>
        <category>ftp服务器</category>
      </categories>
      <tags>
        <tag>ftp服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python面向对象]]></title>
    <url>%2F2019%2F08%2F22%2Fpython%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[面向对象学习本节内容： 面向对象编程介绍 为什么要用面向对象进行开发？ 面向对象的特性：封装、继承、多态 类、方法、 引子 你现在是一家游戏公司的开发人员，现在需要你开发一款叫做&lt;人狗大战&gt;的游戏，你就思考呀，人狗作战，那至少需要2个角色，一个是人， 一个是狗，且人和狗都有不同的技能，比如人拿棍打狗， 狗可以咬人，怎么描述这种不同的角色和他们的功能呢？ 你搜罗了自己掌握的所有技能，写出了下面的代码来描述这两个角色 12345678910111213141516def person(name,age,sex,job): data = &#123; 'name':name, 'age':age, 'sex':sex, 'job':job &#125; return data def dog(name,dog_type): data = &#123; 'name':name, 'type':dog_type &#125; return data 上面两个方法相当于造了两个模子，游戏开始，你得生成一个人和狗的实际对象吧，怎么生成呢？ 12345d1 = dog("李磊","京巴") p1 = person("严帅",36,"F","运维") p2 = person("林海峰",27,"F","Teacher") 两个角色对象生成了，狗和人还有不同的功能呀，狗会咬人，人会打狗，对不对? 怎么实现呢，。。想到了， 可以每个功能再写一个函数，想执行哪个功能，直接 调用 就可以了，对不？ 123456def bark(d): print(&quot;dog %s:wang.wang..wang...&quot;%d[&apos;name&apos;]) def walk(p): print(&quot;person %s is walking...&quot; %p[&apos;name&apos;])&lt;br&gt;&lt;br&gt; 123d1 = dog(&quot;李磊&quot;,&quot;京巴&quot;)p1 = person(&quot;严帅&quot;,36,&quot;F&quot;,&quot;运维&quot;)p2 = person(&quot;林海峰&quot;,27,&quot;F&quot;,&quot;Teacher&quot;) 1walk(p1) bark(d1) 上面的功能实现的简直是完美！ 但是仔细玩耍一会，你就不小心干了下面这件事 12p1 = person(&quot;严帅&quot;,36,&quot;F&quot;,&quot;运维&quot;)bark(p1) #把人的对象传给了狗的方法 事实 上，这并没出错。很显然，人是不能调用狗的功能的，如何在代码级别实现这个限制呢？ 1234567891011121314151617181920212223242526def person(name,age,sex,job): def walk(p): print("person %s is walking..." % p['name']) data = &#123; 'name':name, 'age':age, 'sex':sex, 'job':job, 'walk':walk &#125; return data def dog(name,dog_type): def bark(d): print("dog %s:wang.wang..wang..."%d['name']) data = &#123; 'name':name, 'type':dog_type, 'bark':bark &#125; return data 123d1 = dog(&quot;李磊&quot;,&quot;京巴&quot;)p1 = person(&quot;严帅&quot;,36,&quot;F&quot;,&quot;运维&quot;)p2 = person(&quot;林海峰&quot;,27,&quot;F&quot;,&quot;Teacher&quot;)&lt;br&gt;&lt;br&gt; 1d1[&apos;bark&apos;](p1) #把人的对象传给了狗的方法 你是如此的机智，这样就实现了限制人只能用人自己的功能啦。 但，我的哥，不要高兴太早，刚才你只是阻止了两个完全 不同的角色 之前的功能混用， 但有没有可能 ，同一个种角色，但有些属性是不同的呢？ 比如 ，大家都打过cs吧，cs里有警察和恐怖份子，但因为都 是人， 所以你写一个角色叫 person(), 警察和恐怖份子都 可以 互相射击，但警察不可以杀人质，恐怖分子可以，这怎么实现呢？ 你想了说想，说，简单，只需要在杀人质的功能里加个判断，如果是警察，就不让杀不就ok了么。 没错， 这虽然 解决了杀人质的问题，但其实你会发现，警察和恐怖分子的区别还有很多，同时又有很多共性，如果 在每个区别处都 单独做判断，那得累死。 你想了想说， 那就直接写2个角色吧， 反正 这么多区别， 我的哥， 不能写两个角色呀，因为他们还有很多共性 ， 写两个不同的角色，就代表 相同的功能 也要重写了，是不是我的哥？ 。。。 好了， 话题就给你点到这， 再多说你的智商 也理解不了了！ 面向过程 VS 面向对象编程范式编程是 程序 员 用特定的语法+数据结构+算法组成的代码来告诉计算机如何执行任务的过程 ， 一个程序是程序员为了得到一个任务结果而编写的一组指令的集合，正所谓条条大路通罗马，实现一个任务的方式有很多种不同的方式， 对这些不同的编程方式的特点进行归纳总结得出来的编程方式类别，即为编程范式。 不同的编程范式本质上代表对各种类型的任务采取的不同的解决问题的思路， 大多数语言只支持一种编程范式，当然也有些语言可以同时支持多种编程范式。 两种最重要的编程范式分别是面向过程编程和面向对象编程。 面向过程编程(Procedural Programming)Procedural programming uses a list of instructions to tell the computer what to do step-by-step.面向过程编程依赖 - 你猜到了- procedures，一个procedure包含一组要被进行计算的步骤， 面向过程又被称为top-down languages， 就是程序从上到下一步步执行，一步步从上到下，从头到尾的解决问题 。基本设计思路就是程序一开始是要着手解决一个大的问题，然后把一个大问题分解成很多个小问题或子过程，这些子过程再执行的过程再继续分解直到小问题足够简单到可以在一个小步骤范围内解决。 举个典型的面向过程的例子， 数据库备份， 分三步，连接数据库，备份数据库，测试备份文件可用性。 代码如下 123456789101112131415161718192021def db_conn(): print(&quot;connecting db...&quot;) def db_backup(dbname): print(&quot;导出数据库...&quot;,dbname) print(&quot;将备份文件打包，移至相应目录...&quot;) def db_backup_test(): print(&quot;将备份文件导入测试库，看导入是否成功&quot;) def main(): db_conn() db_backup(&apos;my_db&apos;) db_backup_test() if __name__ == &apos;__main__&apos;: main() 这样做的问题也是显而易见的，就是如果你要对程序进行修改，对你修改的那部分有依赖的各个部分你都也要跟着修改， 举个例子，如果程序开头你设置了一个变量值 为1 ， 但如果其它子过程依赖这个值 为1的变量才能正常运行，那如果你改了这个变量，那这个子过程你也要修改，假如又有一个其它子程序依赖这个子过程 ， 那就会发生一连串的影响，随着程序越来越大， 这种编程方式的维护难度会越来越高。所以我们一般认为， 如果你只是写一些简单的脚本，去做一些一次性任务，用面向过程的方式是极好的，但如果你要处理的任务是复杂的，且需要不断迭代和维护 的， 那还是用面向对象最方便了。 面向对象编程OOP编程是利用“类”和“对象”来创建各种模型来实现对真实世界的描述，使用面向对象编程的原因一方面是因为它可以使程序的维护和扩展变得更简单，并且可以大大提高程序开发效率 ，另外，基于面向对象的程序可以使它人更加容易理解你的代码逻辑，从而使团队开发变得更从容。 面向对象的几个核心特性如下 Class 类一个类即是对一类拥有相同属性的对象的抽象、蓝图、原型。在类中定义了这些对象的都具备的属性（variables(data)）、共同的方法 Object 对象一个对象即是一个类的实例化后实例，一个类必须经过实例化后方可在程序中调用，一个类可以实例化多个对象，每个对象亦可以有不同的属性，就像人类是指所有人，每个人是指具体的对象，人与人之前有共性，亦有不同 Encapsulation 封装在类中对数据的赋值、内部调用对外部用户是透明的，这使类变成了一个胶囊或容器，里面包含着类的数据和方法 Inheritance 继承一个类可以派生出子类，在这个父类里定义的属性、方法自动被子类继承 Polymorphism 多态多态是面向对象的重要特性,简单点说:“一个接口，多种实现”，指一个基类中派生出了不同的子类，且每个子类在继承了同样的方法名的同时又对父类的方法做了不同的实现，这就是同一种事物表现出的多种形态。编程其实就是一个将具体世界进行抽象化的过程，多态就是抽象化的一种体现，把一系列具体事物的共同点抽象出来, 再通过这个抽象的事物, 与不同的具体事物进行对话。对不同类的对象发出相同的消息将会有不同的行为。比如，你的老板让所有员工在九点钟开始工作, 他只要在九点钟的时候说：“开始工作”即可，而不需要对销售人员说：“开始销售工作”，对技术人员说：“开始技术工作”, 因为“员工”是一个抽象的事物, 只要是员工就可以开始工作，他知道这一点就行了。至于每个员工，当然会各司其职，做各自的工作。多态允许将子类的对象当作父类的对象使用，某父类型的引用指向其子类型的对象,调用的方法是该子类型的方法。这里引用和调用方法的代码编译前就已经决定了,而引用所指向的对象可以在运行期间动态绑定 面向对象编程(Object-Oriented Programming )介绍对于编程语言的初学者来讲，OOP不是一个很容易理解的编程方式，大家虽然都按老师讲的都知道OOP的三大特性是继承、封装、多态，并且大家也都知道了如何定义类、方法等面向对象的常用语法，但是一到真正写程序的时候，还是很多人喜欢用函数式编程来写代码，特别是初学者，很容易陷入一个窘境就是“我知道面向对象，我也会写类，但我依然没发现在使用了面向对象后，对我们的程序开发效率或其它方面带来什么好处，因为我使用函数编程就可以减少重复代码并做到程序可扩展了，为啥子还用面向对象？”。 对于此，我个人觉得原因应该还是因为你没有充分了解到面向对象能带来的好处，今天我就写一篇关于面向对象的入门文章，希望能帮大家更好的理解和使用面向对象编程。 无论用什么形式来编程，我们都要明确记住以下原则： 写重复代码是非常不好的低级行为 你写的代码需要经常变更 开发正规的程序跟那种写个运行一次就扔了的小脚本一个很大不同就是，你的代码总是需要不断的更改，不是修改bug就是添加新功能等，所以为了日后方便程序的修改及扩展，你写的代码一定要遵循易读、易改的原则（专业数据叫可读性好、易扩展）。 如果你把一段同样的代码复制、粘贴到了程序的多个地方以实现在程序的各个地方调用 这个功能，那日后你再对这个功能进行修改时，就需要把程序里多个地方都改一遍，这种写程序的方式是有问题的，因为如果你不小心漏掉了一个地方没改，那可能会导致整个程序的运行都 出问题。 因此我们知道 在开发中一定要努力避免写重复的代码，否则就相当于给自己再挖坑。 还好，函数的出现就能帮我们轻松的解决重复代码的问题，对于需要重复调用的功能，只需要把它写成一个函数，然后在程序的各个地方直接调用这个函数名就好了，并且当需要修改这个功能时，只需改函数代码，然后整个程序就都更新了。 其实OOP编程的主要作用也是使你的代码修改和扩展变的更容易，那么小白要问了，既然函数都能实现这个需求了，还要OOP干毛线用呢？ 呵呵，说这话就像，古时候，人们打仗杀人都用刀，后来出来了枪，它的主要功能跟刀一样，也是杀人，然后小白就问，既然刀能杀人了，那还要枪干毛线，哈哈，显而易见，因为枪能更好更快更容易的杀人。函数编程与OOP的主要区别就是OOP可以使程序更加容易扩展和易更改。 小白说，我读书少，你别骗我，口说无凭，证明一下，好吧，那我们就下面的例子证明给小白看。 相信大家都打过CS游戏吧，我们就自己开发一个简单版的CS来玩一玩。 暂不考虑开发场地等复杂的东西，我们先从人物角色下手， 角色很简单，就俩个，恐怖份子、警察，他们除了角色不同，其它基本都 一样，每个人都有生命值、武器等。 咱们先用非OOP的方式写出游戏的基本角色 1234567891011#role 1name = &apos;Alex&apos;role = &apos;terrorist&apos;weapon = &apos;AK47&apos;life_value = 100 #rolw 2name2 = &apos;Jack&apos;role2 = &apos;police&apos;weapon2 = &apos;B22&apos;life_value2 = 100 上面定义了一个恐怖份子Alex和一个警察Jack,但只2个人不好玩呀，一干就死了，没意思，那我们再分别一个恐怖分子和警察吧， 123456789101112131415161718192021222324252627#role 1name = &apos;Alex&apos;role = &apos;terrorist&apos;weapon = &apos;AK47&apos;life_value = 100money = 10000 #rolw 2name2 = &apos;Jack&apos;role2 = &apos;police&apos;weapon2 = &apos;B22&apos;life_value2 = 100money2 = 10000 #role 3name3 = &apos;Rain&apos;role3 = &apos;terrorist&apos;weapon3 = &apos;C33&apos;life_value3 = 100money3 = 10000 #rolw 4name4 = &apos;Eric&apos;role4 = &apos;police&apos;weapon4 = &apos;B51&apos;life_value4 = 100money4 = 10000 4个角色虽然创建好了，但是有个问题就是，每创建一个角色，我都要单独命名，name1,name2,name3,name4…，后面的调用的时候这个变量名你还都得记着，要是再让多加几个角色，估计调用时就很容易弄混啦，所以我们想一想，能否所有的角色的变量名都是一样的，但调用的时候又能区分开分别是谁？ 当然可以，我们只需要把上面的变量改成字典的格式就可以啦。 1234567891011121314151617181920212223242526272829roles = &#123; 1:&#123;&apos;name&apos;:&apos;Alex&apos;, &apos;role&apos;:&apos;terrorist&apos;, &apos;weapon&apos;:&apos;AK47&apos;, &apos;life_value&apos;: 100, &apos;money&apos;: 15000, &#125;, 2:&#123;&apos;name&apos;:&apos;Jack&apos;, &apos;role&apos;:&apos;police&apos;, &apos;weapon&apos;:&apos;B22&apos;, &apos;life_value&apos;: 100, &apos;money&apos;: 15000, &#125;, 3:&#123;&apos;name&apos;:&apos;Rain&apos;, &apos;role&apos;:&apos;terrorist&apos;, &apos;weapon&apos;:&apos;C33&apos;, &apos;life_value&apos;: 100, &apos;money&apos;: 15000, &#125;, 4:&#123;&apos;name&apos;:&apos;Eirc&apos;, &apos;role&apos;:&apos;police&apos;, &apos;weapon&apos;:&apos;B51&apos;, &apos;life_value&apos;: 100, &apos;money&apos;: 15000, &#125;,&#125; print(roles[1]) #Alexprint(roles[2]) #Jack 很好，这个以后调用这些角色时只需要roles[1],roles[2]就可以啦，角色的基本属性设计完了后，我们接下来为每个角色开发以下几个功能 被打中后就会掉血的功能 开枪功能 换子弹 买枪 跑、走、跳、下蹲等动作 保护人质（仅适用于警察） 不能杀同伴 。。。 我们可以把每个功能写成一个函数，类似如下： 1234567891011def shot(by_who): #开了枪后要减子弹数 passdef got_shot(who): #中枪后要减血 who[‘life_value’] -= 10 passdef buy_gun(who,gun_name): #检查钱够不够,买了枪后要扣钱 pass... so far so good, 继续按照这个思路设计，再完善一下代码，游戏的简单版就出来了，但是在往下走之前，我们来看看上面的这种代码写法有没有问题，至少从上面的代码设计中，我看到以下几点缺陷： 每个角色定义的属性名称是一样的，但这种命名规则是我们自己约定的，从程序上来讲，并没有进行属性合法性检测，也就是说role 1定义的代表武器的属性是weapon, role 2 ,3,4也是一样的，不过如果我在新增一个角色时不小心把weapon 写成了wepon , 这个程序本身是检测 不到的 terrorist 和police这2个角色有些功能是不同的，比如police是不能杀人质的，但是terrorist可能，随着这个游戏开发的更复杂，我们会发现这2个角色后续有更多的不同之处， 但现在的这种写法，我们是没办法 把这2个角色适用的功能区分开来的，也就是说，每个角色都可以直接调用任意功能，没有任何限制。 我们在上面定义了got_shot()后要减血，也就是说减血这个动作是应该通过被击中这个事件来引起的,我们调用get_shot()，got_shot（）这个函数再调用每个角色里的life_value变量来减血。 但其实我不通过got_shot()，直接调用角色roles[role_id][‘life_value’] 减血也可以呀，但是如果这样调用的话，那可以就是简单粗暴啦，因为减血之前其它还应该判断此角色是否穿了防弹衣等，如果穿了的话，伤害值肯定要减少，got_shot()函数里就做了这样的检测，你这里直接绕过的话，程序就乱了。 因此这里应该设计 成除了通过got_shot(),其它的方式是没有办法给角色减血的，不过在上面的程序设计里，是没有办法实现的。 现在需要给所有角色添加一个可以穿防弹衣的功能，那很显然你得在每个角色里放一个属性来存储此角色是否穿 了防弹衣，那就要更改每个角色的代码，给添加一个新属性，这样太low了，不符合代码可复用的原则 上面这4点问题如果不解决，以后肯定会引出更大的坑，有同学说了，解决也不复杂呀，直接在每个功能调用时做一下角色判断啥就好了，没错，你要非得这么霸王硬上弓的搞也肯定是可以实现的，那你自己就开发相应的代码来对上面提到的问题进行处理好啦。 但这些问题其实能过OOP就可以很简单的解决。 12345678910111213141516171819class Role(object): def __init__(self,name,role,weapon,life_value=100,money=15000): self.name = name self.role = role self.weapon = weapon self.life_value = life_value self.money = money def shot(self): print(&quot;shooting...&quot;) def got_shot(self): print(&quot;ah...,I got shot...&quot;) def buy_gun(self,gun_name): print(&quot;just bought %s&quot; %gun_name) r1 = Role(&apos;Alex&apos;,&apos;police&apos;,&apos;AK47’) #生成一个角色r2 = Role(&apos;Jack&apos;,&apos;terrorist&apos;,&apos;B22’) #生成一个角色 先不考虑语法细节，相比靠函数拼凑出来的写法，上面用面向对象中的类来写最直接的改进有以下2点： 代码量少了近一半 角色和它所具有的功能可以一目了然看出来 在真正开始分解上面代码含义之之前，我们现来了解一些类的基本定义 类的语法 123456789class Dog(object): print(&quot;hello,I am a dog!&quot;) d = Dog() #实例化这个类，#此时的d就是类Dog的实例化对象 #实例化，其实就是以Dog类为模版，在内存里开辟一块空间，存上数据，赋值成一个变量名 上面的代码其实有问题，想给狗起名字传不进去。 12345678910111213class Dog(object): def __init__(self,name,dog_type): self.name = name self.type = dog_type def sayhi(self): print(&quot;hello,I am a dog, my name is &quot;,self.name) d = Dog(&apos;LiChuang&apos;,&quot;京巴&quot;)d.sayhi() 为什么有__init__? 为什么有self? 此时的你一脸蒙逼，相信不画个图，你的智商是理解不了的！ 画图之前， 你先注释掉这两句 1234# d = Dog(&apos;LiChuang&apos;, &quot;京巴&quot;)# d.sayhi() print(Dog) 没实例直接打印Dog输出如下 1&lt;class &apos;__main__.Dog&apos;&gt; 这代表什么?代表 即使不实例化，这个Dog类本身也是已经存在内存里的对不对， yes, cool，那实例化时，会产生什么化学反应呢？ 根据上图我们得知，其实self,就是实例本身！你实例化时python会自动把这个实例本身通过self参数传进去。 你说好吧，假装懂了， 但下面这段代码你又不明白了， 为何sayhi(self),要写个self呢？ 12345class Dog(object): ... def sayhi(self): print(&quot;hello,I am a dog, my name is &quot;,self.name) 好了，明白 了类的基本定义，接下来我们一起分解一下上面的代码分别 是什么意思 1234567class Role(object): #定义一个类， class是定义类的语法，Role是类名，(object)是新式类的写法，必须这样写，以后再讲为什么 def __init__(self,name,role,weapon,life_value=100,money=15000): #初始化函数，在生成一个角色时要初始化的一些属性就填写在这里 self.name = name #__init__中的第一个参数self,和这里的self都 是什么意思？ 看下面解释 self.role = role self.weapon = weapon self.life_value = life_value self.money = money 上面的这个__init__()叫做初始化方法(或构造方法)， 在类被调用时，这个方法(虽然它是函数形式，但在类中就不叫函数了,叫方法)会自动执行，进行一些初始化的动作，所以我们这里写的__init__(self,name,role,weapon,life_value=100,money=15000)就是要在创建一个角色时给它设置这些属性，那么这第一个参数self是干毛用的呢？ 初始化一个角色，就需要调用这个类一次： 12r1 = Role(&apos;Alex&apos;,&apos;police&apos;,&apos;AK47’) #生成一个角色 , 会自动把参数传给Role下面的__init__(...)方法r2 = Role(&apos;Jack&apos;,&apos;terrorist&apos;,&apos;B22’) #生成一个角色 我们看到，上面的创建角色时，我们并没有给__init__传值，程序也没未报错，是因为，类在调用它自己的__init__(…)时自己帮你给self参数赋值了， 12r1 = Role(&apos;Alex&apos;,&apos;police&apos;,&apos;AK47’) #此时self 相当于 r1 , Role(r1,&apos;Alex&apos;,&apos;police&apos;,&apos;AK47’)r2 = Role(&apos;Jack&apos;,&apos;terrorist&apos;,&apos;B22’)#此时self 相当于 r2, Role(r2,&apos;Jack&apos;,&apos;terrorist&apos;,&apos;B22’) 为什么这样子？你拉着我说你有些犹豫，怎么会这样子？ 你执行r1 = Role(‘Alex’,’police’,’AK47’)时，python的解释器其实干了两件事： 1231. 在内存中开辟一块空间指向r1这个变量名2. 调用Role这个类并执行其中的__init__(…)方法，相当于Role.__init__(r1,&apos;Alex&apos;,&apos;police&apos;,**’****AK47’),**这么做是为什么呢？ 是为了把&apos;Alex&apos;,&apos;police&apos;,’AK47’这3个值跟刚开辟的r1关联起来，是为了把&apos;Alex&apos;,&apos;police&apos;,’AK47’这3个值跟刚开辟的r1关联起来，是为了把&apos;Alex&apos;,&apos;police&apos;,’AK47’这3个值跟刚开辟的r1关联起来，重要的事情说3次， 因为关联起来后，你就可以直接r1.name, r1.weapon 这样来调用啦。所以，为实现这种关联，在调用__init__方法时，就必须把r1这个变量也传进去，否则__init__不知道要把那3个参数跟谁关联呀。3. 明白了么哥？所以这个__init__(…)方法里的，self.name = name , self.role = role 等等的意思就是要把这几个值 存到r1的内存空间里。 如果还不明白的话，哥，去测试一下智商吧， 应该不会超过70，哈哈。 为了暴露自己的智商，此时你假装懂了，但又问， __init__(…)我懂了，但后面的那几个函数，噢 不对，后面那几个方法 为什么也还需要self参数么？ 不是在初始化角色的时候 ，就已经把角色的属性跟r1绑定好了么？ good question, 先来看一下上面类中的一个buy_gun的方法： 12def buy_gun(self,gun_name): print(“%s has just bought %s” %(self.name,gun_name) ) 上面这个方法通过类调用的话要写成如下： 12r1 = Role(&apos;Alex&apos;,&apos;police&apos;,&apos;AK47&apos;)r1.buy_gun(&quot;B21”) #python 会自动帮你转成 Role.buy_gun(r1,”B21&quot;) 执行结果 #Alex has just bought B21 依然没给self传值 ，但Python还是会自动的帮你把r1 赋值给self这个参数， 为什么呢？ 因为，你在buy_gun(..)方法中可能要访问r1的一些其它属性呀， 比如这里就访问 了r1的名字，怎么访问呢？你得告诉这个方法呀，于是就把r1传给了这个self参数，然后在buy_gun里调用 self.name 就相当于调用r1.name 啦，如果还想知道r1的生命值 有多少，直接写成self.life_value就可以了。 说白了就是在调用类中的一个方法时，你得告诉人家你是谁。 好啦， 总结一下2点： 上面的这个r1 = Role(‘Alex’,’police’,’AK47’)动作，叫做类的“实例化”， 就是把一个虚拟的抽象的类，通过这个动作，变成了一个具体的对象了， 这个对象就叫做实例 刚才定义的这个类体现了面向对象的第一个基本特性，封装，其实就是使用构造方法将内容封装到某个具体对象中，然后通过对象直接或者self间接获取被封装的内容 面向对象的特性：封装封装最好理解了。封装是面向对象的特征之一，是对象和类概念的主要特性。 封装，也就是把客观事物封装成抽象的类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的进行信息隐藏。 继承面向对象编程 (OOP) 语言的一个主要功能就是“继承”。继承是指这样一种能力：它可以使用现有类的所有功能，并在无需重新编写原来的类的情况下对这些功能进行扩展。 通过继承创建的新类称为“子类”或“派生类”。 被继承的类称为“基类”、“父类”或“超类”。 继承的过程，就是从一般到特殊的过程。 要实现继承，可以通过“继承”（Inheritance）和“组合”（Composition）来实现。 在某些 OOP 语言中，一个子类可以继承多个基类。但是一般情况下，一个子类只能有一个基类，要实现多重继承，可以通过多级继承来实现。 继承概念的实现方式主要有2类：实现继承、接口继承。 Ø 实现继承是指使用基类的属性和方法而无需额外编码的能力； Ø 接口继承是指仅使用属性和方法的名称、但是子类必须提供实现的能力(子类重构爹类方法)； 在考虑使用继承时，有一点需要注意，那就是两个类之间的关系应该是“属于”关系。例如，Employee 是一个人，Manager 也是一个人，因此这两个类都可以继承 Person 类。但是 Leg 类却不能继承 Person 类，因为腿并不是一个人。 抽象类仅定义将由子类创建的一般属性和方法。 OO开发范式大致为：划分对象→抽象类→将类组织成为层次化结构(继承和合成) →用类与实例进行设计和实现几个阶段。 继承示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#!_*_coding:utf-8_*_#__author__:&quot;Alex Li&quot; class SchoolMember(object): members = 0 #初始学校人数为0 def __init__(self,name,age): self.name = name self.age = age def tell(self): pass def enroll(self): &apos;&apos;&apos;注册&apos;&apos;&apos; SchoolMember.members +=1 print(&quot;\033[32;1mnew member [%s] is enrolled,now there are [%s] members.\033[0m &quot; %(self.name,SchoolMember.members)) def __del__(self): &apos;&apos;&apos;析构方法&apos;&apos;&apos; print(&quot;\033[31;1mmember [%s] is dead!\033[0m&quot; %self.name)class Teacher(SchoolMember): def __init__(self,name,age,course,salary): super(Teacher,self).__init__(name,age) self.course = course self.salary = salary self.enroll() def teaching(self): &apos;&apos;&apos;讲课方法&apos;&apos;&apos; print(&quot;Teacher [%s] is teaching [%s] for class [%s]&quot; %(self.name,self.course,&apos;s12&apos;)) def tell(self): &apos;&apos;&apos;自我介绍方法&apos;&apos;&apos; msg = &apos;&apos;&apos;Hi, my name is [%s], works for [%s] as a [%s] teacher !&apos;&apos;&apos; %(self.name,&apos;Oldboy&apos;, self.course) print(msg) class Student(SchoolMember): def __init__(self, name,age,grade,sid): super(Student,self).__init__(name,age) self.grade = grade self.sid = sid self.enroll() def tell(self): &apos;&apos;&apos;自我介绍方法&apos;&apos;&apos; msg = &apos;&apos;&apos;Hi, my name is [%s], I&apos;m studying [%s] in [%s]!&apos;&apos;&apos; %(self.name, self.grade,&apos;Oldboy&apos;) print(msg) if __name__ == &apos;__main__&apos;: t1 = Teacher(&quot;Alex&quot;,22,&apos;Python&apos;,20000) t2 = Teacher(&quot;TengLan&quot;,29,&apos;Linux&apos;,3000) s1 = Student(&quot;Qinghua&quot;, 24,&quot;Python S12&quot;,1483) s2 = Student(&quot;SanJiang&quot;, 26,&quot;Python S12&quot;,1484) t1.teaching() t2.teaching() t1.tell() 多态多态性（polymorphisn）是允许你将父对象设置成为和一个或更多的他的子对象相等的技术，赋值之后，父对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。简单的说，就是一句话：允许将子类类型的指针赋值给父类类型的指针。 那么，多态的作用是什么呢？我们知道，封装可以隐藏实现细节，使得代码模块化；继承可以扩展已存在的代码模块（类）；它们的目的都是为了——代码重用。而多态则是为了实现另一个目的——接口重用！多态的作用，就是为了类在继承和派生的时候，保证使用“家谱”中任一类的实例的某一属性时的正确调用。 Pyhon 很多语法都是支持多态的，比如 len(),sorted(), 你给len传字符串就返回字符串的长度，传列表就返回列表长度。 Python多态示例 123456789101112131415161718192021222324252627282930#_*_coding:utf-8_*_ class Animal(object): def __init__(self, name): # Constructor of the class self.name = name def talk(self): # Abstract method, defined by convention only raise NotImplementedError(&quot;Subclass must implement abstract method&quot;) class Cat(Animal): def talk(self): print(&apos;%s: 喵喵喵!&apos; %self.name) class Dog(Animal): def talk(self): print(&apos;%s: 汪！汪！汪！&apos; %self.name) def func(obj): #一个接口，多种形态 obj.talk() c1 = Cat(&apos;小晴&apos;)d1 = Dog(&apos;李磊&apos;) func(c1)func(d1) 领域模型好了，你现在会了面向对象的各种语法了， 那请看下本章最后的作业需求，我相信你可能是蒙蔽的， 很多同学都是学会了面向对象的语法，却依然写不出面向对象的程序，原因是什么呢？原因就是因为你还没掌握一门面向对象设计利器， 你说我读书少别骗我， 什么利器？ 答案就是:领域建模。 从领域模型开始,我们就开始了面向对象的分析和设计过程,可以说,领域模型是完成从需求分析到面向 对象设计的一座桥梁。 领域模型,顾名思义,就是需求所涉及的领域的一个建模,更通俗的讲法是业务模型。 参考百度百科(http://baike.baidu.cn/view/757895.htm ),领域模型定义如下: 从这个定义我们可以看出,领域模型有两个主要的作用: 发掘重要的业务领域概念 建立业务领域概念之间的关系 领域建模三字经领域模型如此重要,很多同学可能会认为领域建模很复杂,需要很高的技巧。然而事实上领域建模非常简 单,简单得有点难以让人相信,领域建模的方法概括一下就是“找名词”! 许多同学看到这个方法后估计都会笑出来:太假了吧,这么简单,找个初中生都会啊,那我们公司那些分 析师和设计师还有什么用哦? 分析师和设计师当然有用,后面我们会看到,即使是简单的找名词这样的操作,也涉及到分析和提炼,而 不是简单的摘取出来就可,这种情况下分析师和设计师的经验和技能就能够派上用场了。但领域模型分析 也确实相对简单,即使没有丰富的经验和高超的技巧,至少也能完成一个能用的领域模型。 虽然我们说“找名词”很简单,但一个关键的问题还没有说明:从哪里找? 如果你还记得领域模型是“需求到面向对象的桥梁”,那么你肯定一下子就能想到:从需求模型中找,具 体来说就是从用例中找。 归纳一下域建模的方法就是“从用例中找名词”。 当然,找到名词后,为了能够更加符合面向对象的要求和特点,我们还需要对这些名词进一步完善,这就 是接下来的步骤:加属性,连关系! 最后我们总结出领域建模的三字经方法:找名词、加属性、连关系。 找名词 who : 学员、讲师、管理员 用例： \1. 管理员 创建了 北京 和 上海 两个校区 \2. 管理员 创建了 Linux \ Python \ Go 3个课程 \3. 管理员 创建了 北京校区的Python 16期， Go开发第一期，和上海校区的Linux 36期 班级 \4. 管理员 创建了 北京校区的 学员 小晴 ，并将其 分配 在了 班级 python 16期 \5. 管理员 创建了 讲师 Alex , 并将其分配 给了 班级 python 16期 和全栈脱产5期 \6. 讲师 Alex 创建 了一条 python 16期的 上课纪录 Day6 \7. 讲师 Alex 为Day6这节课 所有的学员 批了作业 ，小晴得了A, 李磊得了C-, 严帅得了B \8. 学员小晴 在 python 16 的 day6里 提交了作业 \9. 学员李磊 查看了自己所报的所有课程 10 学员 李磊 在 查看了 自己在 py16期 的 成绩列表 ，然后自杀了 \11. 学员小晴 跟 讲师 Alex 表白了 名词列表： 管理员、校区、课程、班级、上课纪录、作业、成绩、讲师、学员 加属性 连关系 有了类,也有了属性,接下来自然就是找出它们的关系了。]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python字符编码详解]]></title>
    <url>%2F2019%2F08%2F21%2Fpython%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[python字符编码编码种类： ASCII 占1个字节，只支持英文 GB2312 占2个字节，支持6700+汉字 GBK GB2312的升级版，支持21000+汉字 Shift-JIS 日本字符 ks_c_5601-1987 韩国编码 TIS-620 泰国编码 由于每个国家都有自己的字符，所以其对应关系也涵盖了自己国家的字符，但是以上编码都存在局限性，即：仅涵盖本国字符，无其他国家字符的对应关系。应运而生出现了万国码，他涵盖了全球所有的文字和二进制的对应关系， Unicode 2-4字节 已经收录136690个字符，并还在一直不断扩张中… Unicode 起到了2个作用： 直接支持全球所有语言，每个国家都可以不用再使用自己之前的旧编码了，用unicode就可以了。(就跟英语是全球统一语言一样) unicode包含了跟全球所有国家编码的映射关系，为什么呢？后面再讲 Unicode解决了字符和二进制的对应关系，但是使用unicode表示一个字符，太浪费空间。例如：利用unicode表示“Python”需要12个字节才能表示，比原来ASCII表示增加了1倍。 由于计算机的内存比较大，并且字符串在内容中表示时也不会特别大，所以内容可以使用unicode来处理，但是存储和网络传输时一般数据都会非常多，那么增加1倍将是无法容忍的！！！ 为了解决存储和网络传输的问题，出现了Unicode Transformation Format，学术名UTF，即：对unicode中的进行转换，以便于在存储和网络传输时可以节省空间! UTF-8： 使用1、2、3、4个字节表示所有字符；优先使用1个字符、无法满足则使增加一个字节，最多4个字节。英文占1个字节、欧洲语系占2个、东亚占3个，其它及特殊字符占4个 UTF-16： 使用2、4个字节表示所有字符；优先使用2个字节，否则使用4个字节表示。 UTF-32： 使用4个字节表示所有字符； 总结：UTF 是为unicode编码 设计 的一种 在存储 和传输时节省空间的编码方案。 字符在硬盘上的存储无论以什么编码在内存里显示字符，存到硬盘上都是2进制。 1`ascii编码(美国)：`` ``l ``0b1101100`` ``o ``0b1101111`` ``v ``0b1110110`` ``e ``0b1100101``GBK编码(中国)：`` ``老 ``0b11000000` `0b11001111`` ``男 ``0b11000100` `0b11010000`` ``孩 ``0b10111010` `0b10100010` `Shift_JIS编码(日本)：`` ``私 ``0b10001110` `0b10000100`` ``は ``0b10000010` `0b11001101` `ks_c_5601``-``1987``编码(韩国)：`` ``나 ``0b10110011` `0b10101010`` ``는 ``0b10110100` `0b11000010`` ` `TIS``-``620``编码(泰国)：`` ``ฉัน ``0b10101001` `0b11010001` `0b10111001``...` 编码的转换虽然国际语言是英语 ，但大家在自己的国家依然说自已的语言，不过出了国， 你就得会英语编码也一样，虽然有了unicode and utf-8 ， 但是由于历史问题，各个国家依然在大量使用自己的编码，比如中国的windows,默认编码依然是gbk,而不是utf-8 基于此，如果中国的软件出口到美国，在美国人的电脑上就会显示乱码，因为他们没有gbk编码。若想让中国的软件可以正常的在 美国人的电脑上显示，只有以下2条路可走： 让美国人的电脑上都装上gbk编码 把你的软件编码以utf-8编码 第1种方法几乎不可能实现，第2种方法比较简单。 但是也只能是针对新开发的软件。 如果你之前开发的软件就是以gbk编码的，上百万行代码可能已经写出去了，重新编码成utf-8格式也会费很大力气。 so , 针对已经用gbk开发完毕的项目，以上2种方案都不能轻松的让项目在美国人电脑上正常显示，难道没有别的办法了么？有， 还记得我们讲unicode其中一个功能是其包含了跟全球所有国家编码的映射关系，意思就是，你写的是gbk的“路飞学城”,但是unicode能自动知道它在unicode中的“路飞学城”的编码是什么，如果这样的话，那是不是意味着，无论你以什么编码存储的数据 ，只要你的软件在把数据从硬盘读到内存里，转成unicode来显示，就可以了。由于所有的系统、编程语言都默认支持unicode，那你的gbk软件放到美国电脑 上，加载到内存里，变成了unicode,中文就可以正常展示啦。 这个表你自己也可以下载下来 unicode与gbk的映射表 http://www.unicode.org/charts/ Python3的执行过程在看实际代码的例子前，我们来聊聊，python3 执行代码的过程 解释器找到代码文件，把代码字符串按文件头定义的编码加载到内存，转成unicode 把代码字符串按照语法规则进行解释， 所有的变量字符都会以unicode编码声明 编码转换过程实际代码演示，在py3上 把你的代码以utf-8编写， 保存，然后在windows上执行， 1`s ``=` `'路飞学城'``print``(s)` so ,一切都很美好，到这里，我们关于编码的学习按说就可以结束了。 但是，如生活一样，美好的表面下，总是隐藏着不尽如人意，上面的utf-8编码之所以能在windows gbk的终端下显示正常，是因为到了内存里python解释器把utf-8转成了unicode , 但是这只是python3, 并不是所有的编程语言在内存里默认编码都是unicode,比如 万恶的python2 就不是， 它的默认编码是ASCII，想写中文，就必须声明文件头的coding为gbk or utf-8, 声明之后，python2解释器仅以文件头声明的编码去解释你的代码，加载到内存后，并不会主动帮你转为unicode,也就是说，你的文件编码是utf-8,加载到内存里，你的变量字符串就也是utf-8, 这意味着什么你知道么？。。。意味着，你以utf-8编码的文件，在windows是乱码。 乱是正常的，不乱才不正常，因为只有2种情况 ，你的windows上显示才不会乱 字符串以GBK格式显示 字符串是unicode编码 既然Python2并不会自动的把文件编码转为unicode存在内存里， 那就只能使出最后一招了，你自己人肉转。Py3 自动把文件编码转为unicode必定是调用了什么方法，这个方法就是，decode(解码) 和encode(编码) 1`UTF``-``8` `-``-``&gt; decode 解码 ``-``-``&gt; ``Unicode``Unicode` `-``-``&gt; encode 编码 ``-``-``&gt; GBK ``/` `UTF``-``8` `..` decode示例 encode 示例 记住下图规则 如何验证编码转对了呢？ 查看数据类型，python 2 里有专门的unicode 类型 查看unicode编码映射表 unicode字符是有专门的unicode类型来判断的，但是utf-8,gbk编码的字符都是str,你如果分辨出来的当前的字符串数据是何种编码的呢？ 有人说可以通过字节长度判断，因为utf-8一个中文占3字节，gbk一个占2字节 靠上面字节个数，虽然也能大体判断是什么类型，但总觉得不是很专业。 怎么才能精确的验证一个字符的编码呢，就是拿这些16进制的数跟编码表里去匹配。 “路飞学城”的unicode编码的映射位置是 u’\u8def\u98de\u5b66\u57ce’ ，‘\u8def’ 就是‘路’，到表里搜一下。“路飞学城”对应的GBK编码是’\xc2\xb7\xb7\xc9\xd1\xa7\xb3\xc7’ ，2个字节一个中文，”路” 的二进制 “\xc2\xb7”是4个16进制，正好2字节，拿它到unicode映射表里对一下， 发现是G0-4237，并不是\xc2\xb7呀。。。擦。演砸了吧。。 再查下“飞” \u98de ，对应的是G0-3749， 跟\xb7\xc9也对不上。 虽然对不上， 但好\xc2\xb7 和G0-4237中的第2位的2和第4位的7对上了，“飞”字也是一样，莫非巧合？ 把他们都转成2进制显示试试 123456789101112131415161718路C 28 4 2 1 8 4 2 1&lt;strong&gt;1 1 0 0 0 0 1 0&lt;/strong&gt; B 78 4 2 1 8 4 2 1&lt;strong&gt;1 0 1 1 0 1 1 1&lt;/strong&gt; 飞B 78 4 2 1 8 4 2 11 0 1 1 0 1 1 1 C 98 4 2 1 8 4 2 11 1 0 0 1 0 0 1 这个“路”还是跟G0-4237对不上呀，没错， 但如果你把路\xc2\xb7的每个二进制字节的左边第一个bit变成0试试呢， 我擦，加起来就真的是4237了呀。。难道又是巧合？？？ 必然不是，是因为，GBK的编码表示形式决定的。。因为GBK编码在设计初期就考虑到了要兼容ASCII,即如果是英文，就用一个字节表示，2个字节就是中文，但如何区别连在一起的2个字节是代表2个英文字母，还是一个中文汉字呢？ 中国人如此聪明，决定，2个字节连在一起，如果每个字节的第1位(也就是相当于128的那个2进制位)如果是1，就代表这是个中文，这个首位是128的字节被称为高字节。 也就是2个高字节连在一起，必然就是一个中文。 你怎么如此笃定？因为0-127已经表示了英文的绝大部分字符，128-255是ASCII的扩展表，表示的都是极特殊的字符，一般没什么用。所以中国人就直接拿来用了。 问：那为什么上面 “\xc2\xb7”的2进制要把128所在的位去掉才能与unicode编码表里的G0-4237匹配上呢？ 这只能说是unicode在映射表的表达上直接忽略了高字节，但真正映射的时候 ，肯定还是需要用高字节的哈。 Python bytes类型在python 2 上写字符串 12345&gt;&gt;&gt; s = &quot;路飞&quot;&gt;&gt;&gt; print s路飞&gt;&gt;&gt; s&apos;\xe8\xb7\xaf\xe9\xa3\x9e&apos; 虽说打印的是路飞，但直接调用变量s，看到的却是一个个的16进制表示的二进制字节，我们怎么称呼这样的数据呢？直接叫二进制么？也可以， 但相比于010101，这个数据串在表示形式上又把2进制转成了16进制来表示，这是为什么呢？ 哈，为的就是让人们看起来更可读。我们称之为bytes类型，即字节类型， 它把8个二进制一组称为一个byte,用16进制来表示。 说这个有什么意思呢？ 想告诉你一个事实， 就是，python2的字符串其实更应该称为字节串。 通过存储方式就能看出来， 但python2里还有一个类型是bytes呀，难道又叫bytes又叫字符串？ 嗯 ，是的，在python2里，bytes == str ， 其实就是一回事 除此之外呢， python2里还有个单独的类型是unicode , 把字符串解码后，就会变成unicode 123456&gt;&gt;&gt; s&apos;\xe8\xb7\xaf\xe9\xa3\x9e&apos; #utf-8&gt;&gt;&gt; s.decode(&apos;utf-8&apos;)u&apos;\u8def\u98de&apos; #unicode 在unicode编码表里对应的位置&gt;&gt;&gt; print(s.decode(&apos;utf-8&apos;))路飞 #unicode 格式的字符 由于Python创始人在开发初期认知的局限性，其并未预料到python能发展成一个全球流行的语言，导致其开发初期并没有把支持全球各国语言当做重要的事情来做，所以就轻佻的把ASCII当做了默认编码。 当后来大家对支持汉字、日文、法语等语言的呼声越来越高时，Python于是准备引入unicode,但若直接把默认编码改成unicode的话是不现实的， 因为很多软件就是基于之前的默认编码ASCII开发的，编码一换，那些软件的编码就都乱了。所以Python 2 就直接 搞了一个新的字符类型，就叫unicode类型，比如你想让你的中文在全球所有电脑上正常显示，在内存里就得把字符串存成unicode类型 12345678&gt;&gt;&gt; s = &quot;路飞&quot;&gt;&gt;&gt; s&apos;\xe8\xb7\xaf\xe9\xa3\x9e&apos;&gt;&gt;&gt; s2 = s.decode(&quot;utf-8&quot;)&gt;&gt;&gt; s2u&apos;\u8def\u98de&apos;&gt;&gt;&gt; type(s2)&lt;type &apos;unicode&apos;&gt; 时间来到2008年，python发展已近20年，创始人龟叔越来越觉得python里的好多东西已发展的不像他的初衷那样，开始变得臃肿、不简洁、且有些设计让人摸不到头脑，比如unicode 与str类型，str 与bytes类型的关系，这给很多python程序员造成了困扰。龟叔再也忍不了，像之前一样的修修补补已不能让Python变的更好，于是来了个大变革，Python3横空出世，不兼容python2,python3比python2做了非常多的改进，其中一个就是终于把字符串变成了unicode,文件默认编码变成了utf-8,这意味着，只要用python3,无论你的程序是以哪种编码开发的，都可以在全球各国电脑上正常显示，真是太棒啦！ PY3 除了把字符串的编码改成了unicode, 还把str 和bytes 做了明确区分， str 就是unicode格式的字符， bytes就是单纯二进制啦. 最后一个问题，为什么在py3里，把unicode编码后，字符串就变成了bytes格式？ 你直接给我直接打印成gbk的字符展示不好么？我想其实py3的设计真是煞费苦心，就是想通过这样的方式明确的告诉你，想在py3里看字符，必须得是unicode编码，其它编码一律按bytes格式展示。 最后提示一下，Python只要出现各种编码问题，无非是哪里的编码设置出错了常见编码错误的原因有： Python解释器的默认编码 Python源文件文件编码 Terminal使用的编码 操作系统的语言设置 掌握了编码之前的关系后，挨个排错就好啦]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PicGo+GitHub图床，自己生成图片链接]]></title>
    <url>%2F2019%2F08%2F21%2FPicGo%2BGitHub%E5%9B%BE%E5%BA%8A%EF%BC%8C%E8%87%AA%E5%B7%B1%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87%E9%93%BE%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[PicGo+GitHub图床，自己生成图片链接链接地址:https://juejin.im/entry/5c4ec5aaf265da614420689f]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python列表生成式、生成器、迭代器、装饰器、软件目录结构规范]]></title>
    <url>%2F2019%2F08%2F21%2Fpython%E5%88%97%E8%A1%A8%E7%94%9F%E6%88%90%E5%BC%8F%E3%80%81%E7%94%9F%E6%88%90%E5%99%A8%E3%80%81%E8%BF%AD%E4%BB%A3%E5%99%A8%E3%80%81%E8%A3%85%E9%A5%B0%E5%99%A8%E3%80%81%E8%BD%AF%E4%BB%B6%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[python基础五本节内容 迭代器&amp;生成器 装饰器 Json &amp; pickle 数据序列化 软件目录结构规范 1.列表生成式，迭代器&amp;生成器列表生成式 孩子，我现在有个需求，看列表[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],我要求你把列表里的每个值加1，你怎么实现？你可能会想到2种方式 123456789101112131415161718192021222324252627282930313233343536373839404142434445&gt;&gt;&gt; a[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; b = []&gt;&gt;&gt; for i in a:b.append(i+1)... &gt;&gt;&gt; b[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; a = b&gt;&gt;&gt; a[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]a = [1,3,4,6,7,7,8,9,11]for index,i in enumerate(a): a[index] +=1print(a)原值修改&gt;&gt;&gt; a[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; a = map(lambda x:x+1, a)&gt;&gt;&gt; a&lt;map object at 0x101d2c630&gt;&gt;&gt;&gt; for i in a:print(i)... 234567891011其实还有一种写法，如下 &gt;&gt;&gt; a = [i+1 for i in range(10)]&gt;&gt;&gt; a[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]这就叫做列表生成 生成器通过列表生成式，我们可以直接创建一个列表。但是，受到内存限制，列表容量肯定是有限的。而且，创建一个包含100万个元素的列表，不仅占用很大的存储空间，如果我们仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。 所以，如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。在Python中，这种一边循环一边计算的机制，称为生成器：generator。 要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator： 123456&gt;&gt;&gt; L = [x * x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt; 创建L和g的区别仅在于最外层的[]和()，L是一个list，而g是一个generator。 我们可以直接打印出list的每一个元素，但我们怎么打印出generator的每一个元素呢？ 如果要一个一个打印出来，可以通过next()函数获得generator的下一个返回值： 123456789101112131415161718192021222324&gt;&gt;&gt; next(g)0&gt;&gt;&gt; next(g)1&gt;&gt;&gt; next(g)4&gt;&gt;&gt; next(g)9&gt;&gt;&gt; next(g)16&gt;&gt;&gt; next(g)25&gt;&gt;&gt; next(g)36&gt;&gt;&gt; next(g)49&gt;&gt;&gt; next(g)64&gt;&gt;&gt; next(g)81&gt;&gt;&gt; next(g)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration 我们讲过，generator保存的是算法，每次调用next(g)，就计算出g的下一个元素的值，直到计算到最后一个元素，没有更多的元素时，抛出StopIteration的错误。 当然，上面这种不断调用next(g)实在是太变态了，正确的方法是使用for循环，因为generator也是可迭代对象： 1234567891011121314&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; for n in g:... print(n)...0149162536496481 所以，我们创建了一个generator后，基本上永远不会调用next()，而是通过for循环来迭代它，并且不需要关心StopIteration的错误。 generator非常强大。如果推算的算法比较复杂，用类似列表生成式的for循环无法实现的时候，还可以用函数来实现。 比如，著名的斐波拉契数列（Fibonacci），除第一个和第二个数外，任意一个数都可由前两个数相加得到： 1, 1, 2, 3, 5, 8, 13, 21, 34, … 斐波拉契数列用列表生成式写不出来，但是，用函数把它打印出来却很容易： 1234567def fib(max): n, a, b = 0, 0, 1 while n &lt; max: print(b) a, b = b, a + b n = n + 1 return &apos;done&apos; 注意，赋值语句： 1a, b = b, a + b 相当于： 123t = (b, a + b) # t是一个tuplea = t[0]b = t[1] 但不必显式写出临时变量t就可以赋值。 上面的函数可以输出斐波那契数列的前N个数： 123456789101112&gt;&gt;&gt; fib(10)11235813213455done 仔细观察，可以看出，fib函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似generator。 也就是说，上面的函数和generator仅一步之遥。要把fib函数变成generator，只需要把print(b)改为yield b就可以了： 1234567891011def fib(max): n,a,b = 0,0,1 while n &lt; max: #print(b) yield b a,b = b,a+b n += 1 return &apos;done&apos; 这就是定义generator的另一种方法。如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator： 123&gt;&gt;&gt; f = fib(6)&gt;&gt;&gt; f&lt;generator object fib at 0x104feaaa0&gt; 这里，最难理解的就是generator和函数的执行流程不一样。函数是顺序执行，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。 12345678910111213141516171819202122data = fib(10)print(data)print(data.__next__())print(data.__next__())print(&quot;干点别的事&quot;)print(data.__next__())print(data.__next__())print(data.__next__())print(data.__next__())print(data.__next__())#输出&lt;generator object fib at 0x101be02b0&gt;11干点别的事235813 在上面fib的例子，我们在循环过程中不断调用yield，就会不断中断。当然要给循环设置一个条件来退出循环，不然就会产生一个无限数列出来。 同样的，把函数改成generator后，我们基本上从来不会用next()来获取下一个返回值，而是直接使用for循环来迭代： 123456789&gt;&gt;&gt; for n in fib(6):... print(n)...112358 但是用for循环调用generator时，发现拿不到generator的return语句的返回值。如果想要拿到返回值，必须捕获StopIteration错误，返回值包含在StopIteration的value中： 12345678910111213141516&gt;&gt;&gt; g = fib(6)&gt;&gt;&gt; while True:... try:... x = next(g)... print(&apos;g:&apos;, x)... except StopIteration as e:... print(&apos;Generator return value:&apos;, e.value)... break...g: 1g: 1g: 2g: 3g: 5g: 8Generator return value: done 关于如何捕获错误，后面的错误处理还会详细讲解。 还可通过yield实现在单线程的情况下实现并发运算的效果 12345678910111213141516171819202122232425#_*_coding:utf-8_*___author__ = &apos;Alex Li&apos;import timedef consumer(name): print(&quot;%s 准备吃包子啦!&quot; %name) while True: baozi = yield print(&quot;包子[%s]来了,被[%s]吃了!&quot; %(baozi,name))def producer(name): c = consumer(&apos;A&apos;) c2 = consumer(&apos;B&apos;) c.__next__() c2.__next__() print(&quot;老子开始准备做包子啦!&quot;) for i in range(10): time.sleep(1) print(&quot;做了2个包子!&quot;) c.send(i) c2.send(i)producer(&quot;alex&quot;) 迭代器我们已经知道，可以直接作用于for循环的数据类型有以下几种： 一类是集合数据类型，如list、tuple、dict、set、str等； 一类是generator，包括生成器和带yield的generator function。 这些可以直接作用于for循环的对象统称为可迭代对象：Iterable。 可以使用isinstance()判断一个对象是否是Iterable对象： 1234567891011&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance([], Iterable)True&gt;&gt;&gt; isinstance(&#123;&#125;, Iterable)True&gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterable)True&gt;&gt;&gt; isinstance((x for x in range(10)), Iterable)True&gt;&gt;&gt; isinstance(100, Iterable)False 而生成器不但可以作用于for循环，还可以被next()函数不断调用并返回下一个值，直到最后抛出StopIteration错误表示无法继续返回下一个值了。 *可以被next()函数调用并不断返回下一个值的对象称为迭代器：Iterator。 可以使用isinstance()判断一个对象是否是Iterator对象： 123456789&gt;&gt;&gt; from collections import Iterator&gt;&gt;&gt; isinstance((x for x in range(10)), Iterator)True&gt;&gt;&gt; isinstance([], Iterator)False&gt;&gt;&gt; isinstance(&#123;&#125;, Iterator)False&gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterator)False 生成器都是Iterator对象，但list、dict、str虽然是Iterable，却不是Iterator。 把list、dict、str等Iterable变成Iterator可以使用iter()函数： 1234&gt;&gt;&gt; isinstance(iter([]), Iterator)True&gt;&gt;&gt; isinstance(iter(&apos;abc&apos;), Iterator)True 你可能会问，为什么list、dict、str等数据类型不是Iterator？ 这是因为Python的Iterator对象表示的是一个数据流，Iterator对象可以被next()函数调用并不断返回下一个数据，直到没有数据时抛出StopIteration错误。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据，所以Iterator的计算是惰性的，只有在需要返回下一个数据时它才会计算。 Iterator甚至可以表示一个无限大的数据流，例如全体自然数。而使用list是永远不可能存储全体自然数的。 小结凡是可作用于for循环的对象都是Iterable类型； 凡是可作用于next()函数的对象都是Iterator类型，它们表示一个惰性计算的序列； 集合数据类型如list、dict、str等是Iterable但不是Iterator，不过可以通过iter()函数获得一个Iterator对象。 Python的for循环本质上就是通过不断调用next()函数实现的，例如： 12for x in [1, 2, 3, 4, 5]: pass 实际上完全等价于： 12345678910# 首先获得Iterator对象:it = iter([1, 2, 3, 4, 5])# 循环:while True: try: # 获得下一个值: x = next(it) except StopIteration: # 遇到StopIteration就退出循环 break 2.装饰器你是一家视频网站的后端开发工程师，你们网站有以下几个版块 1234567891011def home(): print(&quot;---首页----&quot;) def america(): print(&quot;----欧美专区----&quot;) def japan(): print(&quot;----日韩专区----&quot;) def henan(): print(&quot;----河南专区----&quot;) 视频刚上线初期，为了吸引用户，你们采取了免费政策，所有视频免费观看，迅速吸引了一大批用户，免费一段时间后，每天巨大的带宽费用公司承受不了了，所以准备对比较受欢迎的几个版块收费，其中包括“欧美” 和 “河南”专区，你拿到这个需求后，想了想，想收费得先让其进行用户认证，认证通过后，再判定这个用户是否是VIP付费会员就可以了，是VIP就让看，不是VIP就不让看就行了呗。 你觉得这个需求很是简单，因为要对多个版块进行认证，那应该把认证功能提取出来单独写个模块，然后每个版块里调用 就可以了，与是你轻轻的就实现了下面的功能 。 1234567891011121314151617181920212223242526272829303132333435363738394041#_*_coding:utf-8_*_ user_status = False #用户登录了就把这个改成True def login(): _username = &quot;alex&quot; #假装这是DB里存的用户信息 _password = &quot;abc!23&quot; #假装这是DB里存的用户信息 global user_status if user_status == False: username = input(&quot;user:&quot;) password = input(&quot;pasword:&quot;) if username == _username and password == _password: print(&quot;welcome login....&quot;) user_status = True else: print(&quot;wrong username or password!&quot;) else: print(&quot;用户已登录，验证通过...&quot;) def home(): print(&quot;---首页----&quot;) def america(): login() #执行前加上验证 print(&quot;----欧美专区----&quot;) def japan(): print(&quot;----日韩专区----&quot;) def henan(): login() #执行前加上验证 print(&quot;----河南专区----&quot;) home()america()henan() 此时你信心满满的把这个代码提交给你的TEAM LEADER审核，没成想，没过5分钟，代码就被打回来了， TEAM LEADER给你反馈是，我现在有很多模块需要加认证模块，你的代码虽然实现了功能，但是需要更改需要加认证的各个模块的代码，这直接违反了软件开发中的一个原则“开放-封闭”原则，简单来说，它规定已经实现的功能代码不允许被修改，但可以被扩展，即： 封闭：已实现的功能代码块不应该被修改 开放：对现有功能的扩展开放 这个原则你还是第一次听说，我擦，再次感受了自己这个野生程序员与正规军的差距，BUT ANYWAY,老大要求的这个怎么实现呢？如何在不改原有功能代码的情况下加上认证功能呢？你一时想不出思路，只好带着这个问题回家继续憋，媳妇不在家，去隔壁老王家串门了，你正好落的清静，一不小心就想到了解决方案，不改源代码可以呀， 你师从沙河金角大王时，记得他教过你，高阶函数，就是把一个函数当做一个参数传给另外一个函数，当时大王说，有一天，你会用到它的，没想到这时这个知识点突然从脑子 里蹦出来了，我只需要写个认证方法，每次调用 需要验证的功能 时，直接 把这个功能 的函数名当做一个参数 传给 我的验证模块不就行了么，哈哈，机智如我，如是你啪啪啪改写了之前的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344#_*_coding:utf-8_*_ user_status = False #用户登录了就把这个改成True def login(func): #把要执行的模块从这里传进来 _username = &quot;alex&quot; #假装这是DB里存的用户信息 _password = &quot;abc!23&quot; #假装这是DB里存的用户信息 global user_status if user_status == False: username = input(&quot;user:&quot;) password = input(&quot;pasword:&quot;) if username == _username and password == _password: print(&quot;welcome login....&quot;) user_status = True else: print(&quot;wrong username or password!&quot;) if user_status == True: func() # 看这里看这里，只要验证通过了，就调用相应功能 def home(): print(&quot;---首页----&quot;) def america(): #login() #执行前加上验证 print(&quot;----欧美专区----&quot;) def japan(): print(&quot;----日韩专区----&quot;) def henan(): #login() #执行前加上验证 print(&quot;----河南专区----&quot;) home()login(america) #需要验证就调用 login，把需要验证的功能 当做一个参数传给login# home()# america()login(henan) 你很开心，终于实现了老板的要求，不改变原功能代码的前提下，给功能加上了验证，此时，媳妇回来了，后面还跟着老王，你两家关系 非常 好，老王经常来串门，老王也是码农，你跟他分享了你写的代码，兴奋的等他看完 夸奖你NB,没成想，老王看后，并没有夸你，抱起你的儿子，笑笑说，你这个代码还是改改吧， 要不然会被开除的，WHAT? 会开除，明明实现了功能 呀， 老王讲，没错，你功能 是实现了，但是你又犯了一个大忌，什么大忌？ 你改变了调用方式呀， 想一想，现在没每个需要认证的模块，都必须调用你的login()方法，并把自己的函数名传给你，人家之前可不是这么调用 的， 试想，如果 有100个模块需要认证，那这100个模块都得更改调用方式，这么多模块肯定不止是一个人写的，让每个人再去修改调用方式 才能加上认证，你会被骂死的。。。。 你觉得老王说的对，但问题是，如何即不改变原功能代码，又不改变原有调用方式，还能加上认证呢？ 你苦思了一会，还是想不出，老王在逗你的儿子玩，你说，老王呀，快给我点思路 ，实在想不出来，老王背对着你问， 老王：学过匿名函数没有？ 你：学过学过，就是lambda嘛 老王：那lambda与正常函数的区别是什么？ 你：最直接的区别是，正常函数定义时需要写名字，但lambda不需要 老王：没错，那lambda定好后，为了多次调用 ，可否也给它命个名？ 你：可以呀，可以写成plus = lambda x:x+1类似这样，以后再调用plus就可以了，但这样不就失去了lambda的意义了，明明人家叫匿名函数呀，你起了名字有什么用呢？ 老王：我不是要跟你讨论它的意义 ，我想通过这个让你明白一个事实 说着，老王拿起你儿子的画板，在上面写了以下代码 1234def plus(n): return n+1 plus2 = lambda x:x+1 老王： 上面这两种写法是不是代表 同样的意思？ 你：是的 老王：我给lambda x:x+1 起了个名字叫plus2，是不是相当于def plus2(x) ? 你：我擦，你别说，还真是，但老王呀，你想说明什么呢？ 老王： 没啥，只想告诉你，给函数赋值变量名就像def func_name 是一样的效果，如下面的plus(n)函数，你调用时可以用plus名，还可以再起个其它名字，如 123calc = plus calc(n) 你明白我想传达什么意思了么？ 你：。。。。。。。。。。。这。。。。。。嗯 。。。。。不太。。。。明白 。。 老王：。。。。这。。。。。呵呵。。。。。。好吧。。。。，那我在给你点一下，你之前写的下面这段调用 认证的代码 12345home()login(america) #需要验证就调用 login，把需要验证的功能 当做一个参数传给login# home()# america()login(henan) 你之所改变了调用方式，是因为用户每次调用时需要执行login(henan)，类似的。其实稍一改就可以了呀 123home()america = login(america)henan = login(henan) 这样你，其它人调用henan时，其实相当于调用了login(henan), 通过login里的验证后，就会自动调用henan功能。 你：我擦，还真是唉。。。，老王，还是你nb。。。不过，等等， 我这样写了好，那用户调用时，应该是下面这个样子 123456home()america = login(america) #你在这里相当于把america这个函数替换了henan = login(henan) #那用户调用时依然写america() 但问题在于，还不等用户调用 ，你的america = login(america)就会先自己把america执行了呀。。。。，你应该等我用户调用 的时候 再执行才对呀，不信我试给你看。。。 老王：哈哈，你说的没错，这样搞会出现这个问题？ 但你想想有没有解决办法 呢？ 你：我擦，你指的思路呀，大哥。。。我哪知道 下一步怎么走。。。 老王：算了，估计你也想不出来。。。 学过嵌套函数没有？ 你：yes,然后呢？ 老王：想实现一开始你写的america = login(america)不触发你函数的执行，只需要在这个login里面再定义一层函数，第一次调用america = login(america)只调用到外层login，这个login虽然会执行，但不会触发认证了，因为认证的所有代码被封装在login里层的新定义 的函数里了，login只返回 里层函数的函数名，这样下次再执行america()时， 就会调用里层函数啦。。。 你：。。。。。。什么？ 什么个意思，我蒙逼了。。。 老王：还是给你看代码吧。。 123456789101112131415161718192021def login(func): #把要执行的模块从这里传进来 def inner():#再定义一层函数 _username = &quot;alex&quot; #假装这是DB里存的用户信息 _password = &quot;abc!23&quot; #假装这是DB里存的用户信息 global user_status if user_status == False: username = input(&quot;user:&quot;) password = input(&quot;pasword:&quot;) if username == _username and password == _password: print(&quot;welcome login....&quot;) user_status = True else: print(&quot;wrong username or password!&quot;) if user_status == True: func() # 看这里看这里，只要验证通过了，就调用相应功能 return inner #用户调用login时，只会返回inner的内存地址，下次再调用时加上()才会执行inner函数 此时你仔细着了老王写的代码 ，感觉老王真不是一般人呀，连这种奇淫巧技都能想出来。。。，心中默默感谢上天赐你一个大牛邻居。 你: 老王呀，你这个姿势很nb呀，你独创的？ 此时你媳妇噗嗤的笑出声来，你也不知道 她笑个球。。。 老王：呵呵， 这不是我独创的呀当然 ，这是开发中一个常用的玩法，叫语法糖，官方名称“装饰器”，其实上面的写法，还可以更简单 可以把下面代码去掉 1america = login(america) #你在这里相当于把america这个函数替换了 只在你要装饰的函数上面加上下面代码 123456789101112@logindef america(): #login() #执行前加上验证 print(&quot;----欧美专区----&quot;) def japan(): print(&quot;----日韩专区----&quot;) @logindef henan(): #login() #执行前加上验证 print(&quot;----河南专区----&quot;) 效果是一样的。 你开心的玩着老王教你的新姿势 ，玩着玩着就手贱给你的“河南专区”版块 加了个参数，然后，结果 出错了。。。 你：老王，老王，怎么传个参数就不行了呢？ 老王：那必然呀，你调用henan时，其实是相当于调用的login，你的henan第一次调用时henan = login(henan)， login就返回了inner的内存地址，第2次用户自己调用henan(“3p”),实际上相当于调用的时inner,但你的inner定义时并没有设置参数，但你给他传了个参数，所以自然就报错了呀 你：但是我的 版块需要传参数呀，你不让我传不行呀。。。 老王：没说不让你传，稍做改动便可。。 老王：你再试试就好了 。 你： 果然好使，大神就是大神呀。 。。 不过，如果有多个参数呢？ 老王：。。。。老弟，你不要什么都让我教你吧，非固定参数你没学过么？ args,*kwargs… 你：噢 。。。还能这么搞?,nb,我再试试。 你身陷这种新玩法中无法自拔，竟没注意到老王已经离开，你媳妇告诉你说为了不打扰你加班，今晚带孩子去跟她姐妹住 ，你觉得媳妇真体贴，最终，你终于搞定了所有需求，完全遵循开放-封闭原则，最终代码如下 。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#_*_coding:utf-8_*_ user_status = False #用户登录了就把这个改成True def login(func): #把要执行的模块从这里传进来 def inner(*args,**kwargs):#再定义一层函数 _username = &quot;alex&quot; #假装这是DB里存的用户信息 _password = &quot;abc!23&quot; #假装这是DB里存的用户信息 global user_status if user_status == False: username = input(&quot;user:&quot;) password = input(&quot;pasword:&quot;) if username == _username and password == _password: print(&quot;welcome login....&quot;) user_status = True else: print(&quot;wrong username or password!&quot;) if user_status == True: func(*args,**kwargs) # 看这里看这里，只要验证通过了，就调用相应功能 return inner #用户调用login时，只会返回inner的内存地址，下次再调用时加上()才会执行inner函数 def home(): print(&quot;---首页----&quot;) @logindef america(): #login() #执行前加上验证 print(&quot;----欧美专区----&quot;) def japan(): print(&quot;----日韩专区----&quot;) # @logindef henan(style): &apos;&apos;&apos; :param style: 喜欢看什么类型的，就传进来 :return: &apos;&apos;&apos; #login() #执行前加上验证 print(&quot;----河南专区----&quot;) home()# america = login(america) #你在这里相当于把america这个函数替换了henan = login(henan) # #那用户调用时依然写america() henan(&quot;3p&quot;) 此时，你已累的不行了，洗洗就抓紧睡了，半夜，上厕所，隐隐听到隔壁老王家有微弱的女人的声音传来，你会心一笑，老王这家伙，不声不响找了女朋友也不带给我看看，改天一定要见下真人。。。。 第二2天早上，产品经理又提了新的需求，要允许用户选择用qq\weibo\weixin认证，此时的你，已深谙装饰器各种装逼技巧，轻松的就实现了新的需求。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#_*_coding:utf-8_*_user_status = False #用户登录了就把这个改成Truedef login(auth_type): #把要执行的模块从这里传进来 def auth(func): def inner(*args,**kwargs):#再定义一层函数 if auth_type == &quot;qq&quot;: _username = &quot;alex&quot; #假装这是DB里存的用户信息 _password = &quot;abc!23&quot; #假装这是DB里存的用户信息 global user_status if user_status == False: username = input(&quot;user:&quot;) password = input(&quot;pasword:&quot;) if username == _username and password == _password: print(&quot;welcome login....&quot;) user_status = True else: print(&quot;wrong username or password!&quot;) if user_status == True: return func(*args,**kwargs) # 看这里看这里，只要验证通过了，就调用相应功能 else: print(&quot;only support qq &quot;) return inner #用户调用login时，只会返回inner的内存地址，下次再调用时加上()才会执行inner函数 return authdef home(): print(&quot;---首页----&quot;)@login(&apos;qq&apos;)def america(): #login() #执行前加上验证 print(&quot;----欧美专区----&quot;)def japan(): print(&quot;----日韩专区----&quot;)@login(&apos;weibo&apos;)def henan(style): &apos;&apos;&apos; :param style: 喜欢看什么类型的，就传进来 :return: &apos;&apos;&apos; #login() #执行前加上验证 print(&quot;----河南专区----&quot;)home()# america = login(america) #你在这里相当于把america这个函数替换了#henan = login(henan)# #那用户调用时依然写america()# henan(&quot;3p&quot;) ###软件目录结构规范为什么要设计好目录结构?“设计项目目录结构”，就和”代码编码风格”一样，属于个人风格问题。对于这种风格上的规范，一直都存在两种态度: 一类同学认为，这种个人风格问题”无关紧要”。理由是能让程序work就好，风格问题根本不是问题。另一类同学认为，规范化能更好的控制程序结构，让程序具有更高的可读性。我是比较偏向于后者的，因为我是前一类同学思想行为下的直接受害者。我曾经维护过一个非常不好读的项目，其实现的逻辑并不复杂，但是却耗费了我非常长的时间去理解它想表达的意思。从此我个人对于提高项目可读性、可维护性的要求就很高了。”项目目录结构”其实也是属于”可读性和可维护性”的范畴，我们设计一个层次清晰的目录结构，就是为了达到以下两点: 可读性高: 不熟悉这个项目的代码的人，一眼就能看懂目录结构，知道程序启动脚本是哪个，测试目录在哪儿，配置文件在哪儿等等。从而非常快速的了解这个项目。可维护性高: 定义好组织规则后，维护者就能很明确地知道，新增的哪个文件和代码应该放在什么目录之下。这个好处是，随着时间的推移，代码/配置的规模增加，项目结构不会混乱，仍然能够组织良好。所以，我认为，保持一个层次清晰的目录结构是有必要的。更何况组织一个良好的工程目录，其实是一件很简单的事儿。 目录组织方式关于如何组织一个较好的Python工程目录结构，已经有一些得到了共识的目录结构。在Stackoverflow的这个问题上https://stackoverflow.com/questions/193161/what-is-the-best-project-structure-for-a-python-application，能看到大家对Python目录结构的讨论。 这里面说的已经很好了，我也不打算重新造轮子列举各种不同的方式，这里面我说一下我的理解和体会。 假设你的项目名为foo, 我比较建议的最方便快捷目录结构这样就足够了: 12345678910111213141516171819Foo/|-- bin/| |-- foo||-- foo/| |-- tests/| | |-- __init__.py| | |-- test_main.py| || |-- __init__.py| |-- main.py||-- docs/| |-- conf.py| |-- abc.rst||-- setup.py|-- requirements.txt|-- README 简要解释一下: bin/: 存放项目的一些可执行文件，当然你可以起名script/之类的也行。 foo/: 存放项目的所有源代码。(1) 源代码中的所有模块、包都应该放在此目录。不要置于顶层目录。(2) 其子目录tests/存放单元测试代码； (3) 程序的入口最好命名为main.py。 docs/: 存放一些文档。 setup.py: 安装、部署、打包的脚本。 requirements.txt: 存放软件依赖的外部Python包列表。 README: 项目说明文件。 除此之外，有一些方案给出了更加多的内容。比如LICENSE.txt,ChangeLog.txt文件等，我没有列在这里，因为这些东西主要是项目开源的时候需要用到。如果你想写一个开源软件，目录该如何组织，可以参考https://www.jeffknupp.com/blog/2013/08/16/open-sourcing-a-python-project-the-right-way/ 关于README的内容 这个我觉得是每个项目都应该有的一个文件，目的是能简要描述该项目的信息，让读者快速了解这个项目。 它需要说明以下几个事项: 软件定位，软件的基本功能。 运行代码的方法: 安装环境、启动命令等。 简要的使用说明。 代码目录结构说明，更详细点可以说明软件的基本原理。 常见问题说明。 我觉得有以上几点是比较好的一个README。在软件开发初期，由于开发过程中以上内容可能不明确或者发生变化，并不是一定要在一开始就将所有信息都补全。但是在项目完结的时候，是需要撰写这样的一个文档的。 可以参考Redis源码中Readme的写法，这里面简洁但是清晰的描述了Redis功能和源码结构。https://github.com/antirez/redis#what-is-redis 关于requirements.txt和setup.py setup.py 一般来说，用setup.py来管理代码的打包、安装、部署问题。业界标准的写法是用Python流行的打包工具setuptools来管理这些事情。这种方式普遍应用于开源项目中。不过这里的核心思想不是用标准化的工具来解决这些问题，而是说，一个项目一定要有一个安装部署工具，能快速便捷的在一台新机器上将环境装好、代码部署好和将程序运行起来。 这个我是踩过坑的。 我刚开始接触Python写项目的时候，安装环境、部署代码、运行程序这个过程全是手动完成，遇到过以下问题: 安装环境时经常忘了最近又添加了一个新的Python包，结果一到线上运行，程序就出错了。 Python包的版本依赖问题，有时候我们程序中使用的是一个版本的Python包，但是官方的已经是最新的包了，通过手动安装就可能装错了。 如果依赖的包很多的话，一个一个安装这些依赖是很费时的事情。 新同学开始写项目的时候，将程序跑起来非常麻烦，因为可能经常忘了要怎么安装各种依赖。 setup.py可以将这些事情自动化起来，提高效率、减少出错的概率。”复杂的东西自动化，能自动化的东西一定要自动化。”是一个非常好的习惯。 setuptools的文档比较庞大，刚接触的话，可能不太好找到切入点。学习技术的方式就是看他人是怎么用的，可以参考一下Python的一个Web框架，flask是如何写的: setup.py 当然，简单点自己写个安装脚本（deploy.sh）替代setup.py也未尝不可。 requirements.txt 这个文件存在的目的是: 方便开发者维护软件的包依赖。将开发过程中新增的包添加进这个列表中，避免在setup.py安装依赖时漏掉软件包。 方便读者明确项目使用了哪些Python包。 这个文件的格式是每一行包含一个包依赖的说明，通常是flask&gt;=0.10这种格式，要求是这个格式能被pip识别，这样就可以简单的通过 pip install -r requirements.txt来把所有Python包依赖都装好了。具体格式说明： https://pip.readthedocs.io/en/1.1/requirements.html 关于配置文件的使用方法 注意，在上面的目录结构中，没有将conf.py放在源码目录下，而是放在docs/目录下。 很多项目对配置文件的使用做法是: 配置文件写在一个或多个python文件中，比如此处的conf.py。 项目中哪个模块用到这个配置文件就直接通过import conf这种形式来在代码中使用配置。 这种做法我不太赞同: 这让单元测试变得困难（因为模块内部依赖了外部配置） 另一方面配置文件作为用户控制程序的接口，应当可以由用户自由指定该文件的路径。 程序组件可复用性太差，因为这种贯穿所有模块的代码硬编码方式，使得大部分模块都依赖conf.py这个文件。 所以，我认为配置的使用，更好的方式是， 模块的配置都是可以灵活配置的，不受外部配置文件的影响。 程序的配置也是可以灵活控制的。 能够佐证这个思想的是，用过nginx和mysql的同学都知道，nginx、mysql这些程序都可以自由的指定用户配置。 所以，不应当在代码中直接import conf来使用配置文件。上面目录结构中的conf.py，是给出的一个配置样例，不是在写死在程序中直接引用的配置文件。可以通过给main.py启动参数指定配置路径的方式来让程序读取配置内容。当然，这里的conf.py你可以换个类似的名字，比如settings.py。或者你也可以使用其他格式的内容来编写配置文件，比如settings.yaml之类的。]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常用模块]]></title>
    <url>%2F2019%2F08%2F20%2Fpython%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[python常用模块学习本节大纲： 模块介绍 time &amp;datetime模块 random os sys shutil json &amp; picle shelve xml处理 yaml处理 configparser hashlib subprocess logging模块 re正则表达式 模块，用一砣代码实现了某个功能的代码集合。 类似于函数式编程和面向过程编程，函数式编程则完成一个功能，其他代码用来调用即可，提供了代码的重用性和代码间的耦合。而对于一个复杂的功能来，可能需要多个函数才能完成（函数又可以在不同的.py文件中），n个 .py 文件组成的代码集合就称为模块。 如：os 是系统相关的模块；file是文件操作相关的模块 模块分为三种： 自定义模块 内置标准模块（又称标准库） 开源模块 time &amp; datetime模块在Python中，通常有这几种方式来表示时间：1）时间戳 2）格式化的时间字符串 3）元组（struct_time）共九个元素。由于Python的time模块实现主要调用C库，所以各个平台可能有所不同。 UTC（Coordinated Universal Time）即格林威治天文时间，为世界标准时间。中国北京为UTC+8。DST（Daylight Saving Time）即夏令时。 时间戳（timestamp）的方式：通常来说，时间戳是指格林威治时间1970年01月01日00时00分00秒(北京时间1970年01月01日08时00分00秒)起至现在的总秒数。 我们运行“type(time.time())”，返回的是float类型。 返回时间戳方式的函数主要有time()，clock()等。 元组（struct_time）方式：struct_time元组共有9个元素，返回struct_time的函数主要有gmtime()，localtime()，strptime()。 下面列出这种方式元组中的几个元素： 123456789101112131415161718192021222324252627282930313233341.以元组方式返回本地当前时间&gt;&gt;&gt; time.localtime()time.struct_time(tm_year=2017, tm_mon=5, tm_mday=8, tm_hour=16, tm_min=13, tm_sec=34, tm_wday=0, tm_yday=128, tm_isdst=0)2.以元组方式返回格林威治时间&gt;&gt;&gt; time.gmtime() time.struct_time(tm_year=2017, tm_mon=5, tm_mday=8, tm_hour=8, tm_min=13, tm_sec=38, tm_wday=0, tm_yday=128, tm_isdst=0)3.将元组时间转换为时间戳&gt;&gt;&gt; x = time.localtime()&gt;&gt;&gt; time.mktime(x)1494232890.04.将元组时间转换为字符串格式时间&gt;&gt;&gt; x = time.localtime()&gt;&gt;&gt; time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;,x)&apos;2017-05-08 16:57:38&apos;5.将字符串格式时间转换为元组格式时间&gt;&gt;&gt; time.strptime(&apos;2017-05-08 17:03:12&apos;,&apos;%Y-%m-%d %H:%M:%S&apos;)time.struct_time(tm_year=2017, tm_mon=5, tm_mday=8, tm_hour=17, tm_min=3, tm_sec=12, tm_wday=0, tm_yday=128, tm_isdst=-1)6.元组格式时间转换为字符串格式时间&gt;&gt;&gt; time.asctime()&apos;Tue May 9 15:23:21 2017&apos;&gt;&gt;&gt; x = time.localtime()&gt;&gt;&gt; time.asctime(x)&apos;Tue May 9 15:23:39 2017&apos;7.时间戳转换成字符串格式时间&gt;&gt;&gt; time.ctime()&apos;Tue May 9 16:07:24 2017&apos;&gt;&gt;&gt; time.ctime(987867475)&apos;Sat Apr 21 23:37:55 2001&apos; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596格式参照:字符串功能%a本地（locale）简化星期名称%A本地完整星期名称%b本地简化月份名称%B本地完整月份名称%c本地相应的日期和时间表示%d一个月中的第几天（01 - 31）%H一天中的第几个小时（24小时制，00 - 23）%I第几个小时（12小时制，01 - 12）%j一年中的第几天（001 - 366）%m月份（01 - 12）%M分钟数（00 - 59）%p本地am或者pm的相应符%S秒（01 - 61）%w一个星期中的第几天（0 - 6，0是星期天）%W和%U基本相同，不同的是%W以星期一为一个星期的开始。%x本地相应日期%X本地相应时间%y去掉世纪的年份（00 - 99）%Y完整的年份%Z时区的名字（如果不存在为空字符）%%%’字符%U一年中的周数。（00 - 53，周日是一个周的开始。）第一个星期天之前的所有天数都放在第0周 12345678910111213141516171819202122232425datetime模块import datetime1.返回当前时间&gt;&gt;&gt; datetime.datetime.now()datetime.datetime(2017, 5, 9, 17, 7, 0, 514481)2.时间戳转换成日期&gt;&gt;&gt; datetime.date.fromtimestamp(1178766678)datetime.date(2007, 5, 10)3.当前时间+3天&gt;&gt;&gt; datetime.datetime.now() + datetime.timedelta(+3)datetime.datetime(2017, 5, 12, 17, 12, 42, 124379)4.当前时间-3天&gt;&gt;&gt; datetime.datetime.now() + datetime.timedelta(-3)datetime.datetime(2017, 5, 6, 17, 13, 18, 474406)5.当前时间+3小时&gt;&gt;&gt; datetime.datetime.now() + datetime.timedelta(hours=3)datetime.datetime(2017, 5, 9, 20, 13, 55, 678310)6.当前时间+30分钟&gt;&gt;&gt; datetime.datetime.now() + datetime.timedelta(minutes=30)datetime.datetime(2017, 5, 9, 17, 44, 40, 392370) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748491 #_*_coding:utf-8_*_2 __author__ = &apos;Alex Li&apos;3 4 import time5 6 7 # print(time.clock()) #返回处理器时间,3.3开始已废弃 , 改成了time.process_time()测量处理器运算时间,不包括sleep时间,不稳定,mac上测不出来8 # print(time.altzone) #返回与utc时间的时间差,以秒计算\9 # print(time.asctime()) #返回时间格式&quot;Fri Aug 19 11:14:16 2016&quot;,10 # print(time.localtime()) #返回本地时间 的struct time对象格式11 # print(time.gmtime(time.time()-800000)) #返回utc时间的struc时间对象格式12 13 # print(time.asctime(time.localtime())) #返回时间格式&quot;Fri Aug 19 11:14:16 2016&quot;,14 #print(time.ctime()) #返回Fri Aug 19 12:38:29 2016 格式, 同上15 16 17 18 # 日期字符串 转成 时间戳19 # string_2_struct = time.strptime(&quot;2016/05/22&quot;,&quot;%Y/%m/%d&quot;) #将 日期字符串 转成 struct时间对象格式20 # print(string_2_struct)21 # #22 # struct_2_stamp = time.mktime(string_2_struct) #将struct时间对象转成时间戳23 # print(struct_2_stamp)24 25 26 27 #将时间戳转为字符串格式28 # print(time.gmtime(time.time()-86640)) #将utc时间戳转换成struct_time格式29 # print(time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;,time.gmtime()) ) #将utc struct_time格式转成指定的字符串格式30 31 32 33 34 35 #时间加减36 import datetime37 38 # print(datetime.datetime.now()) #返回 2016-08-19 12:47:03.94192539 #print(datetime.date.fromtimestamp(time.time()) ) # 时间戳直接转成日期格式 2016-08-1940 # print(datetime.datetime.now() )41 # print(datetime.datetime.now() + datetime.timedelta(3)) #当前时间+3天42 # print(datetime.datetime.now() + datetime.timedelta(-3)) #当前时间-3天43 # print(datetime.datetime.now() + datetime.timedelta(hours=3)) #当前时间+3小时44 # print(datetime.datetime.now() + datetime.timedelta(minutes=30)) #当前时间+30分45 46 47 #48 # c_time = datetime.datetime.now()49 # print(c_time.replace(minute=3,hour=2)) #时间替换 总结:格式化的字符串(format string)转化成元组(struct_time)用strptime,元组(struct_time)转换成格式化的字符串(format string)用strftime 元组(struct_time)转换成时间戳(timestamp)用mktime，时间戳(timestamp)转成元组(struct_time)用localtime,gmtime 元组(struct_time)转换成%a %b %d %H %D %Y用asctime，时间戳(timestamp)转换成%a %b %d %H %D %Y用ctime。 random模块随机数 1234import randomprint random.random() 返回一个大于0小于1的数print random.randint(1,2)print random.randrange(1,10) os模块提供对操作系统进行调用的接口 1234567891011121314151617181920212223242526272829os.getcwd() 获取当前工作目录，即当前python脚本工作的目录路径os.chdir(&quot;dirname&quot;) 改变当前脚本工作目录；相当于shell下cdos.curdir 返回当前目录: (&apos;.&apos;)os.pardir 获取当前目录的父目录字符串名：(&apos;..&apos;)os.makedirs(&apos;dirname1/dirname2&apos;) 可生成多层递归目录os.removedirs(&apos;dirname1&apos;) 若目录为空，则删除，并递归到上一级目录，如若也为空，则删除，依此类推os.mkdir(&apos;dirname&apos;) 生成单级目录；相当于shell中mkdir dirnameos.rmdir(&apos;dirname&apos;) 删除单级空目录，若目录不为空则无法删除，报错；相当于shell中rmdir dirnameos.listdir(&apos;dirname&apos;) 列出指定目录下的所有文件和子目录，包括隐藏文件，并以列表方式打印os.remove() 删除一个文件os.rename(&quot;oldname&quot;,&quot;newname&quot;) 重命名文件/目录os.stat(&apos;path/filename&apos;) 获取文件/目录信息os.sep 输出操作系统特定的路径分隔符，win下为&quot;\\&quot;,Linux下为&quot;/&quot;os.linesep 输出当前平台使用的行终止符，win下为&quot;\t\n&quot;,Linux下为&quot;\n&quot;os.pathsep 输出用于分割文件路径的字符串os.name 输出字符串指示当前使用平台。win-&gt;&apos;nt&apos;; Linux-&gt;&apos;posix&apos;os.system(&quot;bash command&quot;) 运行shell命令，直接显示os.environ 获取系统环境变量os.path.abspath(path) 返回path规范化的绝对路径os.path.split(path) 将path分割成目录和文件名二元组返回os.path.dirname(path) 返回path的目录。其实就是os.path.split(path)的第一个元素os.path.basename(path) 返回path最后的文件名。如何path以／或\结尾，那么就会返回空值。即os.path.split(path)的第二个元素os.path.exists(path) 如果path存在，返回True；如果path不存在，返回Falseos.path.isabs(path) 如果path是绝对路径，返回Trueos.path.isfile(path) 如果path是一个存在的文件，返回True。否则返回Falseos.path.isdir(path) 如果path是一个存在的目录，则返回True。否则返回Falseos.path.join(path1[, path2[, ...]]) 将多个路径组合后返回，第一个绝对路径之前的参数将被忽略 会从第一个以”/”开头的参数开始拼接，之前的参数全部丢弃 以上一种情况为先。在上一种情况确保情况下，若出现”./”开头的参数，会从”./”开头的参数的上一个参数开始拼接os.path.getatime(path) 返回path所指向的文件或者目录的最后存取时间os.path.getmtime(path) 返回path所指向的文件或者目录的最后修改时间 更多:https://docs.python.org/2/library/os.html?highlight=os#module-os sys模块12345678sys.argv 命令行参数List，第一个元素是程序本身路径sys.exit(n) 退出程序，正常退出时exit(0)sys.version 获取Python解释程序的版本信息sys.maxint 最大的Int值sys.path 返回模块的搜索路径，初始化时使用PYTHONPATH环境变量的值sys.platform 返回操作系统平台名称sys.stdout.write(&apos;please:&apos;)val = sys.stdin.readline()[:-1] shutil模块shutil – High-level file operations 是一种高层次的文件操作工具 类似于高级API，而且主要强大之处在于其对文件的复制与删除操作更是比较支持好。 shutil:高级的文件、文件夹、压缩包处理模块 shutil被定义为python中的一个高级的文件操作模块，拥有比os模块中更强大的函数。 1、shutil.copyfileobj(fsrc, fdst[, length])（copyfileobj方法只会拷贝文件内容） 将文件内容拷贝到另一个文件中 123import shutilshutil.copyfileobj(open(&apos;old.xml&apos;,&apos;r&apos;),open(&apos;new.xml&apos;,&apos;w&apos;))shutil.copyfile(src,dst) （copyfile只拷贝文件内容） 2、拷贝文件 123shutil.copyfile(&apos;f1.log&apos;, &apos;f2.log&apos;)shutil.copy(src, dst) 拷贝文件和权限shutil.copy(&apos;f1.log&apos;, &apos;f2.log&apos;) 3、shutil.copy2(src, dst) 拷贝文件和状态信息 1shutil.copy2(&apos;f1.log&apos;, &apos;f2.log&apos;) 4、shutil.copymode(src, dst) （前提是dst文件存在，不然报错） 仅拷贝权限。内容、组、用户均不变 1shutil.copymode(&apos;f1.log&apos;, &apos;f2.log&apos;) 5、shutil.copystat(src, dst) 仅拷贝状态的信息，即文件属性，包括：mode bits, atime, mtime, flags 1shutil.copystat(&apos;f1.log&apos;, &apos;f2.log&apos;) 6、shutil.ignore_patterns(*patterns) （忽略哪个文件，有选择性的拷贝） 7、shutil.copytree(src, dst, symlinks=False, ignore=None) 递归的去拷贝文件夹 12shutil.copytree(&apos;folder1&apos;, &apos;folder2&apos;, ignore=shutil.ignore_patterns(&apos;*.pyc&apos;, &apos;tmp*&apos;))shutil.copytree(&apos;f1&apos;, &apos;f2&apos;, symlinks=True, ignore=shutil.ignore_patterns(&apos;*.pyc&apos;, &apos;tmp*&apos;)) 8、shutil.rmtree(path[, ignore_errors[, onerror]]) 递归的去删除文件 1shutil.rmtree(&apos;folder1&apos;) 9、shutil.move(src, dst) 递归的去移动文件，它类似mv命令，其实就是重命名。 1shutil.move(&apos;folder1&apos;, &apos;folder3&apos;) 10、shutil.make_archive(base_name, format,…) 12345678910111213创建压缩包并返回文件路径，例如：zip、tar ● base_name： 压缩包的文件名，也可以是压缩包的路径。只是文件名时，则保存至当前目录，否则保存至指定路径， 如： tar_name =&gt;保存至当前路径 如：/Users/a6/tar_name =&gt;保存至/Users/a6/ ● format： 压缩包种类，“zip”, “tar”, “bztar”，“gztar” ● root_dir： 要压缩的文件夹路径（默认当前目录） ● owner： 用户，默认当前用户 ● group： 组，默认当前组 ● logger： 用于记录日志，通常是logging.Logger对象#将 /Users/a6/Downloads/test 下的文件打包放置当前程序目录import shutilret = shutil.make_archive(&quot;tar_name&quot;, &apos;gztar&apos;, root_dir=&apos;/Users/a6/Downloads/test&apos;)#将 /Users/a6/Downloads/test 下的文件打包放置 /Users/a6/目录import shutilret = shutil.make_archive(&quot;/Users/a6/tar_name&quot;, &apos;gztar&apos;, root_dir=&apos;/Users/a6/Downloads/test&apos;) 11、shutil 对压缩包的处理是调用 ZipFile 和 TarFile 两个模块来进行的，详细： 1234567891011121314151617181920import zipfile# 压缩z = zipfile.ZipFile(&apos;laxi.zip&apos;, &apos;w&apos;)z.write(&apos;a.log&apos;)z.write(&apos;data.data&apos;)z.close()# 解压z = zipfile.ZipFile(&apos;laxi.zip&apos;, &apos;r&apos;)z.extractall()z.close()import tarfile# 压缩tar = tarfile.open(&apos;your.tar&apos;,&apos;w&apos;)tar.add(&apos;/Users/wupeiqi/PycharmProjects/bbs2.log&apos;, arcname=&apos;bbs2.log&apos;)tar.add(&apos;/Users/wupeiqi/PycharmProjects/cmdb.log&apos;, arcname=&apos;cmdb.log&apos;)tar.close()# 解压tar = tarfile.open(&apos;your.tar&apos;,&apos;r&apos;)tar.extractall() # 可设置解压地址tar.close() 备注：zipfile压缩不会保留文件的状态信息，而tarfile会保留文件的状态信息json &amp; pickle 模块 用于序列化的两个模块 序列化 在程序运行的过程中，所有的变量都是在内存中，比如，定义一个dict： 12345678910111213d = dict(name=&apos;Bob&apos;, age=20, score=88)``` 可以随时修改变量，比如把name改成Bill，但是一旦程序结束，变量所占用的内存就被操作系统全部回收。如果没有把修改后的Bill存储到磁盘上，下次重新运行程序，变量又被初始化为Bob。我们把变量从内存中变成可存储或传输的过程称之为序列化，在python中叫pickling,在其他语言中也被称之为serialization,marshalling,flattening等等，都是一个意思。序列化之后，就可以把序列化后的内容写入磁盘，或者通过网络传输到别的机器上。反过来，把变量内容从序列化的对象重新读到内存里称之为反序列化，即unpickling。python提供了pickle模块来实现序列化。首先，我们尝试把一个对象序列化写入文件: import pickled = dict(name=’Bob’, age=20, score=88)pickle.dumps(d)b’\x80\x03}q\x00(X\x03\x00\x00\x00ageq\x01K\x14X\x05\x00\x00\x00scoreq\x02KXX\x04\x00\x00\x00nameq\x03X\x03\x00\x00\x00Bobq\x04u.’ 1pickle.dumps()方法把任意对象序列化成一个bytes，然后，就可以把这个bytes写入文件。或者用另一个方法pickle.dump()直接把对象序列化后写入一个file-like Object： f = open(‘dump.txt’, ‘wb’)pickle.dump(d, f)f.close() 123看看写入的dump.txt文件，一堆乱七八糟的内容，这些都是Python保存的对象内部信息。当我们要把对象从磁盘读到内存时，可以先把内容读到一个bytes，然后用pickle.loads()方法反序列化出对象，也可以直接用pickle.load()方法从一个file-like Object中直接反序列化出对象。我们打开另一个Python命令行来反序列化刚才保存的对象： f = open(‘dump.txt’, ‘rb’)d = pickle.load(f)f.close()d{‘age’: 20, ‘score’: 88, ‘name’: ‘Bob’} 12345678910变量的内容又回来了！当然，这个变量和原来的变量是完全不相干的对象，它们只是内容相同而已。Pickle的问题和所有其他编程语言特有的序列化问题一样，就是它只能用于Python，并且可能不同版本的Python彼此都不兼容，因此，只能用Pickle保存那些不重要的数据，不能成功地反序列化也没关系。### JSON如果我们要在不同的编程语言之间传递对象，就必须把对象序列化为标准格式，比如XML，但更好的方法是序列化为JSON，因为JSON表示出来就是一个字符串，可以被所有语言读取，也可以方便地存储到磁盘或者通过网络传输。JSON不仅是标准格式，并且比XML更快，而且可以直接在Web页面中读取，非常方便。JSON表示的对象就是标准的JavaScript语言的对象，JSON和Python内置的数据类型对应如下： JSON类型 Python类型{} dict[] list“string” str1234.56 int或floattrue/false True/Falsenull None 1Python内置的json模块提供了非常完善的Python对象到JSON格式的转换。我们先看看如何把Python对象变成一个JSON： import jsond = dict(name=’Bob’, age=20, score=88)json.dumps(d)‘{“age”: 20, “score”: 88, “name”: “Bob”}’ 123dumps()方法返回一个str，内容就是标准的JSON。类似的，dump()方法可以直接把JSON写入一个file-like Object。要把JSON反序列化为Python对象，用loads()或者对应的load()方法，前者把JSON的字符串反序列化，后者从file-like Object中读取字符串并反序列化： json_str = ‘{“age”: 20, “score”: 88, “name”: “Bob”}’json.loads(json_str){‘age’: 20, ‘score’: 88, ‘name’: ‘Bob’} 123由于JSON标准规定JSON编码是UTF-8，所以我们总是能正确地在Python的str与JSON的字符串之间转换。### JSON进阶Python的dict对象可以直接序列化为JSON的&#123;&#125;，不过，很多时候，我们更喜欢用class表示对象，比如定义Student类，然后序列化： import json class Student(object): def init(self, name, age, score): self.name = name self.age = age self.score = score s = Student(‘Bob’, 20, 88)print(json.dumps(s)) 1运行代码，毫不留情地得到一个TypeError： Traceback (most recent call last): …TypeError: &lt;main.Student object at 0x10603cc50&gt; is not JSON serializable 1234567891011错误的原因是Student对象不是一个可序列化为JSON的对象。如果连class的实例对象都无法序列化为JSON，这肯定不合理！别急，我们仔细看看dumps()方法的参数列表，可以发现，除了第一个必须的obj参数外，dumps()方法还提供了一大堆的可选参数：https://docs.python.org/3/library/json.html#json.dumps这些可选参数就是让我们来定制JSON序列化。前面的代码之所以无法把Student类实例序列化为JSON，是因为默认情况下，dumps()方法不知道如何将Student实例变为一个JSON的&#123;&#125;对象。可选参数default就是把任意一个对象变成一个可序列为JSON的对象，我们只需要为Student专门写一个转换函数，再把函数传进去即可： def student2dict(std): return { ‘name’: std.name, ‘age’: std.age, ‘score’: std.score } 1这样，Student实例首先被student2dict()函数转换成dict，然后再被顺利序列化为JSON： print(json.dumps(s, default=student2dict)){“age”: 20, “name”: “Bob”, “score”: 88} 1不过，下次如果遇到一个Teacher类的实例，照样无法序列化为JSON。我们可以偷个懒，把任意class的实例变为dict： print(json.dumps(s, default=lambda obj: obj.dict)) 123因为通常class的实例都有一个__dict__属性，它就是一个dict，用来存储实例变量。也有少数例外，比如定义了__slots__的class。同样的道理，如果我们要把JSON反序列化为一个Student对象实例，loads()方法首先转换出一个dict对象，然后，我们传入的object_hook函数负责把dict转换为Student实例： def dict2student(d): return Student(d[‘name’], d[‘age’], d[‘score’]) 1运行结果如下： json_str = ‘{“age”: 20, “score”: 88, “name”: “Bob”}’print(json.loads(json_str, object_hook=dict2student))&lt;main.Student object at 0x10cd3c190&gt; 12345678打印出的是反序列化的Student实例对象。 ### 小结Python语言特定的序列化模块是pickle，但如果要把序列化搞得更通用、更符合Web标准，就可以使用json模块。json模块的dumps()和loads()函数是定义得非常好的接口的典范。当我们使用时，只需要传入一个必须的参数。但是，当默认的序列化或反序列机制不满足我们的要求时，我们又可以传入更多的参数来定制序列化或反序列化的规则，既做到了接口简单易用，又做到了充分的扩展性和灵活性。### shelve 模块shelve类似于一个key-value数据库，可以很方便的用来保存Python的内存对象，其内部使用pickle来序列化数据，简单来说，使用者可以将一个列表、字典、或者用户自定义的类实例保存到shelve中，下次需要用的时候直接取出来，就是一个Python内存对象，不需要像传统数据库一样，先取出数据，然后用这些数据重新构造一遍所需要的对象。下面是简单示例： import shelve d = shelve.open(‘shelve_test’) #打开一个文件 class Test(object): def init(self,n): self.n = n t = Test(123)t2 = Test(123334) name = [“alex”,”rain”,”test”]d[“test”] = name #持久化列表d[“t1”] = t #持久化类d[“t2”] = t2 d.close() 1234### xml处理模块xml是实现不同语言或程序之间进行数据交换的协议，跟json差不多，但json使用起来更简单，不过，古时候，在json还没诞生的黑暗年代，大家只能选择用xml呀，至今很多传统公司如金融行业的很多系统的接口还主要是xml。xml的格式如下，就是通过&lt;&gt;节点来区别数据结构的: 2 2008 141100 5 2011 59900 69 2011 13600 1xml协议在各个语言里的都 是支持的，在python中可以用以下模块操作xml import xml.etree.ElementTree as ET tree = ET.parse(“xmltest.xml”)root = tree.getroot()print(root.tag) #遍历xml文档for child in root: print(child.tag, child.attrib) for i in child: print(i.tag,i.text) #只遍历year 节点for node in root.iter(‘year’): print(node.tag,node.text) 1修改和删除xml文档内容 import xml.etree.ElementTree as ET tree = ET.parse(“xmltest.xml”)root = tree.getroot() #修改for node in root.iter(‘year’): new_year = int(node.text) + 1 node.text = str(new_year) node.set(“updated”,”yes”) tree.write(“xmltest.xml”) #删除nodefor country in root.findall(‘country’): rank = int(country.find(‘rank’).text) if rank &gt; 50: root.remove(country) tree.write(‘output.xml’) 1自己创建xml文档 import xml.etree.ElementTree as ET new_xml = ET.Element(“namelist”)name = ET.SubElement(new_xml,”name”,attrib={“enrolled”:”yes”})age = ET.SubElement(name,”age”,attrib={“checked”:”no”})sex = ET.SubElement(name,”sex”)sex.text = ‘33’name2 = ET.SubElement(new_xml,”name”,attrib={“enrolled”:”no”})age = ET.SubElement(name2,”age”)age.text = ‘19’ et = ET.ElementTree(new_xml) #生成文档对象et.write(“test.xml”, encoding=”utf-8”,xml_declaration=True) ET.dump(new_xml) #打印生成的格式 123456### PyYAML模块Python也可以很容易的处理ymal文档格式，只不过需要安装一个模块，参考文档：http://pyyaml.org/wiki/PyYAMLDocumentation ### ConfigParser模块用于生成和修改常见配置文档，当前模块的名称在 python 3.x 版本中变更为 configparser。来看一个好多软件的常见文档格式如下 [DEFAULT]ServerAliveInterval = 45Compression = yesCompressionLevel = 9ForwardX11 = yes [bitbucket.org]User = hg [topsecret.server.com]Port = 50022ForwardX11 = no 1如果想用python生成一个这样的文档怎么做呢？ import configparser config = configparser.ConfigParser()config[“DEFAULT”] = {‘ServerAliveInterval’: ‘45’, ‘Compression’: ‘yes’, ‘CompressionLevel’: ‘9’} config[‘bitbucket.org’] = {}config[‘bitbucket.org’][‘User’] = ‘hg’config[‘topsecret.server.com’] = {}topsecret = config[‘topsecret.server.com’]topsecret[‘Host Port’] = ‘50022’ # mutates the parsertopsecret[‘ForwardX11’] = ‘no’ # same hereconfig[‘DEFAULT’][‘ForwardX11’] = ‘yes’with open(‘example.ini’, ‘w’) as configfile: config.write(configfile) 1写完了还可以再读出来哈。 import configparserconfig = configparser.ConfigParser()config.sections()[]config.read(‘example.ini’)[‘example.ini’]config.sections()[‘bitbucket.org’, ‘topsecret.server.com’]‘bitbucket.org’ in configTrue‘bytebong.com’ in configFalseconfig[‘bitbucket.org’][‘User’]‘hg’config[‘DEFAULT’][‘Compression’]‘yes’topsecret = config[‘topsecret.server.com’]topsecret[‘ForwardX11’]‘no’topsecret[‘Port’]‘50022’for key in config[‘bitbucket.org’]: print(key)…usercompressionlevelserveraliveintervalcompressionforwardx11config[‘bitbucket.org’][‘ForwardX11’]‘yes’ 1configparser增删改查语法 [section1]k1 = v1k2:v2 [section2]k1 = v1 import ConfigParser config = ConfigParser.ConfigParser()config.read(‘i.cfg’) ########## 读#secs = config.sections() #print secs #options = config.options(‘group2’) #print options #item_list = config.items(‘group2’) #print item_list #val = config.get(‘group1’,’key’) #val = config.getint(‘group1’,’key’) ########## 改写#sec = config.remove_section(‘group1’) #config.write(open(‘i.cfg’, “w”)) #sec = config.has_section(‘wupeiqi’) #sec = config.add_section(‘wupeiqi’) #config.write(open(‘i.cfg’, “w”)) #config.set(‘group2’,’k1’,11111) #config.write(open(‘i.cfg’, “w”)) #config.remove_option(‘group2’,’age’) #config.write(open(‘i.cfg’, “w”)) 123### hashlib模块 用于加密相关的操作，3.x里代替了md5模块和sha模块，主要提供 SHA1, SHA224, SHA256, SHA384, SHA512 ，MD5 算法 import hashlib m = hashlib.md5()m.update(b”Hello”)m.update(b”It’s me”)print(m.digest())m.update(b”It’s been a long time since last time we …”) print(m.digest()) #2进制格式hashprint(len(m.hexdigest())) #16进制格式hash‘’’def digest(self, args, *kwargs): # real signature unknown “”” Return the digest value as a string of binary data. “”” pass def hexdigest(self, args, *kwargs): # real signature unknown “”” Return the digest value as a string of hexadecimal digits. “”” pass ‘’’import hashlib ######## md5hash = hashlib.md5()hash.update(‘admin’)print(hash.hexdigest()) ######## sha1hash = hashlib.sha1()hash.update(‘admin’)print(hash.hexdigest()) ######## sha256hash = hashlib.sha256()hash.update(‘admin’)print(hash.hexdigest()) ######## sha384hash = hashlib.sha384()hash.update(‘admin’)print(hash.hexdigest()) ######## sha512hash = hashlib.sha512()hash.update(‘admin’)print(hash.hexdigest()) 12345还不够吊？python 还有一个 hmac 模块，它内部对我们创建 key 和 内容 再进行处理然后再加密散列消息鉴别码，简称HMAC，是一种基于消息鉴别码MAC（Message Authentication Code）的鉴别机制。使用HMAC时,消息通讯的双方，通过验证消息中加入的鉴别密钥K来鉴别消息的真伪；一般用于网络通信中消息加密，前提是双方先要约定好key,就像接头暗号一样，然后消息发送把用key把消息加密，接收方用key ＋ 消息明文再加密，拿加密后的值 跟 发送者的相对比是否相等，这样就能验证消息的真实性，及发送者的合法性了。 import hmach = hmac.new(b’天王盖地虎’, b’宝塔镇河妖’)print h.hexdigest()更多关于md5,sha1,sha256等介绍的文章看这里https://www.tbs-certificates.co.uk/FAQ/en/sha256.html 123### re模块 常用正表达式符号 ‘.’ 默认匹配除\n之外的任意一个字符，若指定flag DOTALL,则匹配任意字符，包括换行‘^’ 匹配字符开头，若指定flags MULTILINE,这种也可以匹配上(r”^a”,”\nabc\neee”,flags=re.MULTILINE)‘$’ 匹配字符结尾，或e.search(“foo$”,”bfoo\nsdfsf”,flags=re.MULTILINE).group()也可以‘‘ 匹配*号前的字符0次或多次，re.findall(“ab“,”cabb3abcbbac”) 结果为[‘abb’, ‘ab’, ‘a’]‘+’ 匹配前一个字符1次或多次，re.findall(“ab+”,”ab+cd+abb+bba”) 结果[‘ab’, ‘abb’]‘?’ 匹配前一个字符1次或0次‘{m}’ 匹配前一个字符m次‘{n,m}’ 匹配前一个字符n到m次，re.findall(“ab{1,3}”,”abb abc abbcbbb”) 结果’abb’, ‘ab’, ‘abb’]‘|’ 匹配|左或|右的字符，re.search(“abc|ABC”,”ABCBabcCD”).group() 结果’ABC’‘(…)’ 分组匹配，re.search(“(abc){2}a(123|456)c”, “abcabca456c”).group() 结果 abcabca456c ‘\A’ 只从字符开头匹配，re.search(“\Aabc”,”alexabc”) 是匹配不到的‘\Z’ 匹配字符结尾，同$‘\d’ 匹配数字0-9‘\D’ 匹配非数字‘\w’ 匹配[A-Za-z0-9]‘\W’ 匹配非[A-Za-z0-9]‘s’ 匹配空白字符、\t、\n、\r , re.search(“\s+”,”ab\tc1\n3”).group() 结果 ‘\t’ ‘(?P…)’ 分组匹配 re.search(“(?P[0-9]{4})(?P[0-9]{2})(?P[0-9]{4})”,”371481199306143242”).groupdict(“city”) 结果{‘province’: ‘3714’, ‘city’: ‘81’, ‘birthday’: ‘1993’} 1最常用的匹配语法 re.match 从头开始匹配re.search 匹配包含re.findall 把所有匹配到的字符放到以列表中的元素返回re.splitall 以匹配到的字符当做列表分隔符re.sub 匹配字符并替换 1234反斜杠的困扰与大多数编程语言相同，正则表达式里使用&quot;\&quot;作为转义字符，这就可能造成反斜杠困扰。假如你需要匹配文本中的字符&quot;\&quot;，那么使用编程语言表示的正则表达式里将需要4个反斜杠&quot;\\\\&quot;：前两个和后两个分别用于在编程语言里转义成反斜杠，转换成两个反斜杠后再在正则表达式里转义成一个反斜杠。Python里的原生字符串很好地解决了这个问题，这个例子中的正则表达式可以使用r&quot;\\&quot;表示。同样，匹配一个数字的&quot;\\d&quot;可以写成r&quot;\d&quot;。有了原生字符串，你再也不用担心是不是漏写了反斜杠，写出来的表达式也更直观。仅需轻轻知道的几个匹配模式 re.I(re.IGNORECASE): 忽略大小写（括号内是完整写法，下同）M(MULTILINE): 多行模式，改变’^’和’$’的行为（参见上图）S(DOTALL): 点任意匹配模式，改变’.’的行为]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sys模块]]></title>
    <url>%2F2019%2F08%2F20%2Fsys%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[### 一、sys模块概述Python 的 sys 模块提供访问解释器使用或维护的变量，和与解释器进行交互的函数。通俗来讲，sys 模块为程序与 python 解释器的交互，提供了一系列的函数和变量，用于操控 Python 运行时的环境。 sys 模块是 Python 中内置的模块，所以不需要再单独安装，只需在使用前 import sys 即可。 sys模块是与python解释器交互的一个接口, sys模块负责程序与python解释器的交互，提供了一系列的函数和变量，用于操控python的运行时环境。 我们可以通过 dir() 方法查看模块中可用的方法。结果如下： 123&gt;&gt;&gt; import sys&gt;&gt;&gt; dir(sys)[&apos;__displayhook__&apos;, &apos;__doc__&apos;, &apos;__excepthook__&apos;, &apos;__interactivehook__&apos;, &apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;, &apos;__spec__&apos;, &apos;__stderr__&apos;, &apos;__stdin__&apos;, &apos;__stdout__&apos;, &apos;_clear_type_cache&apos;, &apos;_current_frames&apos;, &apos;_debugmallocstats&apos;, &apos;_enablelegacywindowsfsencoding&apos;, &apos;_getframe&apos;, &apos;_git&apos;, &apos;_home&apos;, &apos;_xoptions&apos;, &apos;api_version&apos;, &apos;argv&apos;, &apos;base_exec_prefix&apos;, &apos;base_prefix&apos;, &apos;builtin_module_names&apos;, &apos;byteorder&apos;, &apos;call_tracing&apos;, &apos;callstats&apos;, &apos;copyright&apos;, &apos;displayhook&apos;, &apos;dllhandle&apos;, &apos;dont_write_bytecode&apos;, &apos;exc_info&apos;, &apos;excepthook&apos;, &apos;exec_prefix&apos;, &apos;executable&apos;, &apos;exit&apos;, &apos;flags&apos;, &apos;float_info&apos;, &apos;float_repr_style&apos;, &apos;get_asyncgen_hooks&apos;, &apos;get_coroutine_wrapper&apos;, &apos;getallocatedblocks&apos;, &apos;getcheckinterval&apos;, &apos;getdefaultencoding&apos;, &apos;getfilesystemencodeerrors&apos;, &apos;getfilesystemencoding&apos;, &apos;getprofile&apos;, &apos;getrecursionlimit&apos;, &apos;getrefcount&apos;, &apos;getsizeof&apos;, &apos;getswitchinterval&apos;, &apos;gettrace&apos;, &apos;getwindowsversion&apos;, &apos;hash_info&apos;, &apos;hexversion&apos;, &apos;implementation&apos;, &apos;int_info&apos;, &apos;intern&apos;, &apos;is_finalizing&apos;, &apos;last_traceback&apos;, &apos;last_type&apos;, &apos;last_value&apos;, &apos;maxsize&apos;, &apos;maxunicode&apos;, &apos;meta_path&apos;, &apos;modules&apos;, &apos;path&apos;, &apos;path_hooks&apos;, &apos;path_importer_cache&apos;, &apos;platform&apos;, &apos;prefix&apos;, &apos;ps1&apos;, &apos;ps2&apos;, &apos;set_asyncgen_hooks&apos;, &apos;set_coroutine_wrapper&apos;, &apos;setcheckinterval&apos;, &apos;setprofile&apos;, &apos;setrecursionlimit&apos;, &apos;setswitchinterval&apos;, &apos;settrace&apos;, &apos;stderr&apos;, &apos;stdin&apos;, &apos;stdout&apos;, &apos;thread_info&apos;, &apos;version&apos;, &apos;version_info&apos;, &apos;warnoptions&apos;, &apos;winver&apos;] 二、sys模块的常见用法1、sys.argv — 实现从程序外部向程序传递参数 sys.argv 变量是一个包含了命令行参数的字符串列表，利用命令行向程序传递参数。其中，脚本的名称总是 sys.argv 列表的第一个参数。 12345678910111213141516import sysprint(sys.argv[0]) #sys.argv[0]表示代码本身的文件路径print(&quot;命令行参数如下：&quot;)for i in sys.argv: print(i)命令行：D:\st13\python\1.20\lx.py Welcome to Xian运行结果：D:\st13\python\1.20\lx.py #sys.argv[0]命令行参数如下：D:\st13\python\1.20\lx.pyWelcometoXian 2、sys.path 获取指定模块搜索路径的目录名列表，列表中的第一项为当前的工作目录。跟linux中的PATH变量一样的，如果想直接执行命令，必须放在PATH提供的路径或自己添加一个路径进去。这里主要是import导入的时候就是从这里面去找对就的模块名在不在这里面，没有就不能导入。 12345import sysprint(sys.path)运行结果：[&apos;D:\\st13\\python\\1.20&apos;, &apos;C:\\Python36\\python36.zip&apos;, &apos;C:\\Python36\\DLLs&apos;, &apos;C:\\Python36\\lib&apos;, &apos;C:\\Python36&apos;, &apos;C:\\Python36\\lib\\site-packages&apos;] 3、sys.exit([arg]) 一般情况下执行到主程序末尾，解释器自动退出，但是如果需要中途退出程序，可以调用 sys.exit() 函数，带有一个可选的整数参数返回给调用它的程序，表示你可以在主程序中捕获对 sys.exit() 的调用。（0是正常退出，其他为异常）当然也可以用字符串参数，表示错误不成功的报错信息。 下面的例子，首先打印 ‘Hello’，执行完 sys.exit(1)，执行 except 语句，将 ‘中途退出’ 作为参数传递给函数 exitfunc()，然后将 ‘中途退出’ 打印出来，程序正常退出，不执行后面的 print(“Welcome”) 语句。 1234567891011121314import sysdef exitfunc(value): print(value) sys.exit(0)print(&quot;Hello&quot;)try: sys.exit(1)except SystemExit as value: exitfunc(&apos;中途退出&apos;) print(&quot;Welcome&quot;)运行结果：Hello中途退出 4、sys.version — 获取Python版本的信息。 123&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.version&apos;3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)]&apos; 5、sys.platform — 获取当前的系统平台，返回操作系统的名称。 123&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.platform&apos;win32&apos; 6、sys.modules、sys.modules.keys()、sys.modules.values() sys.modules 是一个全局字典，该字典是 Python 启动后就加载在内存中。每当程序员导入新的模块，sys.modules 将自动记录该模块。当第二次再导入该模块时，Python 会直接到字典中查找，从而加快了程序运行的速度。它拥有字典所拥有的一切方法。 1234567891011&gt;&gt;&gt;import sys#返回系统导入的模块字段&gt;&gt;&gt; sys.modules[&apos;os&apos;]&lt;module &apos;os&apos; from &apos;C:\\Python36\\lib\\os.py&apos;&gt;#返回所有已经导入的模块列表&gt;&gt;&gt; sys.modules.keys()dict_keys([&apos;builtins&apos;, &apos;sys&apos;, &apos;_frozen_importlib&apos;, &apos;_imp&apos;, &apos;_warnings&apos;, &apos;_thread&apos;,&apos;_weakref&apos;, &apos;_frozen_importlib_external&apos;, &apos;_io&apos;, &apos;marshal&apos;,&apos;nt&apos;,&apos;winreg&apos;,&apos;zipimport&apos;, &apos;encodings&apos;, &apos;codecs&apos;, &apos;_codecs&apos;, &apos;encodings.aliases&apos;, &apos;encodings.utf_8&apos;, &apos;_signal&apos;, &apos;__main__&apos;, &apos;encodings.latin_1&apos;, &apos;io&apos;, &apos;abc&apos;, &apos;_weakrefset&apos;, &apos;site&apos;, &apos;os&apos;, &apos;errno&apos;, &apos;stat&apos;, &apos;_stat&apos;, &apos;ntpath&apos;, &apos;genericpath&apos;,&apos;os.path&apos;, &apos;_collections_abc&apos;, &apos;_sitebuiltins&apos;, &apos;sysconfig&apos;, &apos;_bootlocale&apos;, &apos;_locale&apos;, &apos;encodings.gbk&apos;, &apos;_codecs_cn&apos;, &apos;_multibytecodec&apos;, &apos;types&apos;, &apos;functools&apos;, &apos;_functools&apos;, &apos;collections&apos;, &apos;operator&apos;, &apos;_operator&apos;, &apos;keyword&apos;, &apos;heapq&apos;, &apos;_heapq&apos;, &apos;itertools&apos;, &apos;reprlib&apos;, &apos;_collections&apos;, &apos;weakref&apos;, &apos;collections.abc&apos;, &apos;importlib&apos;,&apos;importlib._bootstrap&apos;, &apos;importlib._bootstrap_external&apos;, &apos;warnings&apos;,&apos;importlib.util&apos;,&apos;importlib.abc&apos;, &apos;importlib.machinery&apos;, &apos;contextlib&apos;, &apos;zope&apos;,&apos;atexit&apos;])#返回所有的模块，包块模块的路径&gt;&gt;&gt; sys.modules.values()dict_values([&lt;module &apos;builtins&apos; (built-in)&gt;, &lt;module &apos;sys&apos; (built-in)&gt;, &lt;module &apos;importlib._bootstrap&apos; (frozen)&gt;, &lt;module &apos;_imp&apos; (built-in)&gt;, &lt;module &apos;_warnings&apos; (built-in)&gt;, &lt;module &apos;_thread&apos; (built-in)&gt;, &lt;module &apos;_weakref&apos; (built-in)&gt;, &lt;module &apos;importlib._bootstrap_external&apos; (frozen)&gt;, &lt;module &apos;io&apos; (built-in)&gt;, &lt;module &apos;marshal&apos; (built-in)&gt;, &lt;module &apos;nt&apos; (built-in)&gt;, &lt;module &apos;winreg&apos; (built-in)&gt;, &lt;module &apos;zipimport&apos; (built-in)&gt;, &lt;module &apos;encodings&apos; from &apos;C:\\Python36\\lib\\encodings\\__init__.py&apos;&gt;, &lt;module &apos;codecs&apos; from &apos;C:\\Python36\\lib\\codecs.py&apos;&gt;, &lt;module &apos;_codecs&apos; (built-in)&gt;, &lt;module &apos;encodings.aliases&apos; from &apos;C:\\Python36\\lib\\encodings\\aliases.py&apos;&gt;, &lt;module &apos;encodings.utf_8&apos; from &apos;C:\\Python36\\lib\\encodings\\utf_8.py&apos;&gt;, &lt;module &apos;_signal&apos; (built-in)&gt;, &lt;module &apos;__main__&apos; (built-in)&gt;, &lt;module &apos;encodings.latin_1&apos; from &apos;C:\\Python36\\lib\\encodings\\latin_1.py&apos;&gt;, &lt;module &apos;io&apos; from &apos;C:\\Python36\\lib\\io.py&apos;&gt;, &lt;module &apos;abc&apos; from &apos;C:\\Python36\\lib\\abc.py&apos;&gt;, &lt;module &apos;_weakrefset&apos; from &apos;C:\\Python36\\lib\\_weakrefset.py&apos;&gt;, &lt;module &apos;site&apos; from &apos;C:\\Python36\\lib\\site.py&apos;&gt;, &lt;module &apos;os&apos; from &apos;C:\\Python36\\lib\\os.py&apos;&gt;, &lt;module &apos;errno&apos; (built-in)&gt;, &lt;module &apos;stat&apos; from &apos;C:\\Python36\\lib\\stat.py&apos;&gt;, &lt;module &apos;_stat&apos; (built-in)&gt;, &lt;module &apos;ntpath&apos; from &apos;C:\\Python36\\lib\\ntpath.py&apos;&gt;, &lt;module &apos;genericpath&apos; from &apos;C:\\Python36\\lib\\genericpath.py&apos;&gt;, &lt;module &apos;ntpath&apos; from &apos;C:\\Python36\\lib\\ntpath.py&apos;&gt;, &lt;module &apos;_collections_abc&apos; from &apos;C:\\Python36\\lib\\_collections_abc.py&apos;&gt;, &lt;module &apos;_sitebuiltins&apos; from &apos;C:\\Python36\\lib\\_sitebuiltins.py&apos;&gt;, &lt;module &apos;sysconfig&apos; from &apos;C:\\Python36\\lib\\sysconfig.py&apos;&gt;, &lt;module &apos;_bootlocale&apos; from &apos;C:\\Python36\\lib\\_bootlocale.py&apos;&gt;, &lt;module &apos;_locale&apos; (built-in)&gt;, &lt;module &apos;encodings.gbk&apos; from &apos;C:\\Python36\\lib\\encodings\\gbk.py&apos;&gt;, &lt;module &apos;_codecs_cn&apos; (built-in)&gt;, &lt;module &apos;_multibytecodec&apos; (built-in)&gt;, &lt;module &apos;types&apos; from &apos;C:\\Python36\\lib\\types.py&apos;&gt;, &lt;module &apos;functools&apos; from &apos;C:\\Python36\\lib\\functools.py&apos;&gt;, &lt;module &apos;_functools&apos; (built-in)&gt;, &lt;module &apos;collections&apos; from &apos;C:\\Python36\\lib\\collections\\__init__.py&apos;&gt;, &lt;module &apos;operator&apos; from &apos;C:\\Python36\\lib\\operator.py&apos;&gt;, &lt;module &apos;_operator&apos; (built-in)&gt;, &lt;module &apos;keyword&apos; from &apos;C:\\Python36\\lib\\keyword.py&apos;&gt;, &lt;module &apos;heapq&apos; from &apos;C:\\Python36\\lib\\heapq.py&apos;&gt;, &lt;module &apos;_heapq&apos; (built-in)&gt;, &lt;module &apos;itertools&apos; (built-in)&gt;, &lt;module &apos;reprlib&apos; from &apos;C:\\Python36\\lib\\reprlib.py&apos;&gt;, &lt;module &apos;_collections&apos; (built-in)&gt;, &lt;module &apos;weakref&apos; from &apos;C:\\Python36\\lib\\weakref.py&apos;&gt;, &lt;module &apos;collections.abc&apos; from &apos;C:\\Python36\\lib\\collections\\abc.py&apos;&gt;, &lt;module &apos;importlib&apos; from &apos;C:\\Python36\\lib\\importlib\\__init__.py&apos;&gt;, &lt;module &apos;importlib._bootstrap&apos; (frozen)&gt;, &lt;module &apos;importlib._bootstrap_external&apos; (frozen)&gt;, &lt;module &apos;warnings&apos; from &apos;C:\\Python36\\lib\\warnings.py&apos;&gt;, &lt;module &apos;importlib.util&apos; from &apos;C:\\Python36\\lib\\importlib\\util.py&apos;&gt;, &lt;module &apos;importlib.abc&apos; from &apos;C:\\Python36\\lib\\importlib\\abc.py&apos;&gt;, &lt;module &apos;importlib.machinery&apos; from &apos;C:\\Python36\\lib\\importlib\\machinery.py&apos;&gt;, &lt;module &apos;contextlib&apos; from &apos;C:\\Python36\\lib\\contextlib.py&apos;&gt;, &lt;module &apos;zope&apos; (namespace)&gt;, &lt;module &apos;atexit&apos; (built-in)&gt;]) 7、sys.stdout、sys.stdin、sys.stderr sys.stdin：标准输入；stdout：标准输出；stderr：错误输出 stdin，stdout 以及 stderr 变量包含与标准I/O 流对应的流对象。如果需要更好地控制输出，而 print 不能满足你的要求，它们就是你所需要的。你也可以替换它们，这时候你就可以重定向输出和输入到其它设备( device )， 或者以非标准的方式处理它们。 1234567&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.stdin&lt;_io.TextIOWrapper name=&apos;&lt;stdin&gt;&apos; mode=&apos;r&apos; encoding=&apos;utf-8&apos;&gt;&gt;&gt;&gt; sys.stdout&lt;_io.TextIOWrapper name=&apos;&lt;stdout&gt;&apos; mode=&apos;w&apos; encoding=&apos;utf-8&apos;&gt;&gt;&gt;&gt; sys.stderr&lt;_io.TextIOWrapper name=&apos;&lt;stderr&gt;&apos; mode=&apos;w&apos; encoding=&apos;utf-8&apos;&gt; 8、sys.getdefaultencoding() 、 sys.getfilesystemencoding() 1234567sys.getdefaultencoding() ： 获取解释器默认编码。sys.getfilesystemencoding() ：获取内存数据存到文件里的默认编码。&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getdefaultencoding()&apos;utf-8&apos;&gt;&gt;&gt; sys.getfilesystemencoding()&apos;utf-8&apos; 二、sys模块的应用在下面的例子中，应用了 sys 模块中的 argv 实现了从程序外部向程序传递函数，以此来实现不同参数个数的 ping 命令。 123456789101112131415161718192021222324import os#import sys 这种方法导入sys模块，需使用sys.argvfrom sys import argv #这种方法导入sys模块，可直接使用argvdef ping(net,start=1,end=85,n=1,w=3): for i in range(start,end+1): ip=net+&quot;.&quot;+str(i) command=&quot;ping %s -n %d -w %d&quot;%(ip,n,w) print(ip,(&quot;通&quot;,&quot;不通&quot;)[os.system(command)]) #os.system(command)：运行command命令if len(argv) not in [2,4,6]: print(&quot;参数输入错误！&quot;) print(&quot;运行示例：&quot;) print(&quot;note1.py 121.194.14&quot;) print(&quot;note1.py 121.194.14 80 90&quot;) print(&quot;note1.py 121.194.14 80 90 3 1&quot;) print(&quot;语法：note1.py net startip endip count timeout&quot;)elif len(argv)==2: net=argv[1] ping(net)elif len(argv)==4: net=argv[1] ping(net,start=int(argv[2]),end=int(argv[3]))else: net=argv[1] ping(net,start=int(argv[2]),end=int(argv[3]),n=int(argv[4]),w=int(argv[5]))]]></content>
      <categories>
        <category>python模块</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中os和sys模块的区别与常用方法总结]]></title>
    <url>%2F2019%2F08%2F20%2Fpython%E4%B8%ADos%E5%92%8Csys%E6%A8%A1%E5%9D%97%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[python中os和sys模块的区别与常用方法总结官方解释： os： This module provides a portable way of using operating system dependent functionality. 翻译：提供一种方便的使用操作系统函数的方法。 sys：This module provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. 翻译：提供访问由解释器使用或维护的变量和在与解释器交互使用到的函数。 os模块 Python os模块包含普遍的操作系统功能。如果你希望你的程序能够与平台无关的话，这个模块是尤为重要的。如果我们要操作文件、目录，可以在命令行下面输入操作系统提供的各种命令来完成。比如dir、cp等命令。其实操作系统提供的命令只是简单地调用了操作系统提供的接口函数，Python内置的os模块也可以直接调用操作系统提供的接口函数。 123456789101112131415161718192021os 常用方法总结如下：os.remove() #删除文件 os.rename() #重命名文件 os.walk() #生成目录树下的所有文件名 os.chdir() #改变目录 os.mkdir/makedirs() #创建目录/多层目录 os.rmdir/removedirs #删除目录/多层目录 os.listdir() #列出指定目录的文件 os.getcwd() #取得当前工作目录 os.chmod() #改变目录权限 os.path.basename() #去掉目录路径，返回文件名 os.path.dirname() #去掉文件名，返回目录路径 os.path.join() #将分离的各部分组合成一个路径名 os.path.split() #返回(dirname(),basename())元组 os.path.splitext() #返回filename,extension)元组 os.path.getatime\ctime\mtime #分别返回最近访问、创建、修改时间 os.path.getsize() #返回文件大小 os.path.exists() #是否存在 os.path.isabs() #是否为绝对路径 os.path.isdir() #是否为目录 os.path.isfile() #是否为文件 更多关于os模块的介绍大家可以参考这篇文章：//www.jb51.net/article/57995.htm sys 常用方法总结如下： sys模块包括了一组非常实用的服务，内含很多函数方法和变量，用来处理Python运行时配置以及资源，从而可以与前当程序之外的系统环境交互，如：Python解释器。导入sys模块 首先，打开终端模拟器进入Python解释器或者打开IDE编辑器创建一个新的.py后缀名的Python程序文件。 下面，以解释器中的操作举例： 12&gt;&gt;&gt; import sys #导入sys模块&gt;&gt;&gt; dir(sys) #dir()方法查看模块中可用的方法 注意：如果是在编辑器，一定要注意要事先声明代码的编码方式，否则中文会乱码。 常用方法: 123456789101112131415161718192021222324252627sys.argv #命令行参数List，第一个元素是程序本身路径 sys.modules.keys() #返回所有已经导入的模块列表 sys.exc_info() #获取当前正在处理的异常类,exc_type、exc_value、exc_traceback当前处理的异常详细信息 sys.exit(n) #程序，正常退出时exit(0) sys.hexversion #获取Python解释程序的版本值，16进制格式如：0x020403F0 sys.version #获取Python解释程序的版本信息 sys.maxint #最大的Int值 sys.maxunicode #最大的Unicode值 sys.modules #返回系统导入的模块字段，key是模块名，value是模块 sys.path #返回模块的搜索路径，初始化时使用PYTHONPATH环境变量的值 sys.platform #返回操作系统平台名称 sys.stdout #标准输出 sys.stdin #标准输入 sys.stderr #错误输出 sys.exc_clear() #用来清除当前线程所出现的当前的或最近的错误信息 sys.exec_prefix #返回平台独立的python文件安装的位置 sys.byteorder #本地字节规则的指示器，big-endian平台的值是&apos;big&apos;,little-endian平台的值是&apos;little&apos; sys.copyright #记录python版权相关的东西 sys.api_version #解释器的C的API版本 sys.version_info #获取Python解释器的版本信息 sys.getwindowsversion #获取Windows的版本sys.getdefaultencoding #返回当前你所用的默认的字符编码格式sys.getfilesystemencoding #返回将Unicode文件名转换成系统文件名的编码的名字sys.setdefaultencoding(name) #用来设置当前默认的字符编码sys.builtin_module_names #Python解释器导入的模块列表 sys.executable #Python解释程序路径 sys.stdin.readline #从标准输入读一行，sys.stdout.write(&quot;a&quot;) 屏幕输出a]]></content>
      <categories>
        <category>python模块</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python paramiko模块]]></title>
    <url>%2F2019%2F08%2F19%2Fpython%20paramiko%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[python模块之paramiko模块paramiko是用python语言写的一个模块，遵循SSH2协议，支持以加密和认证的方式，进行远程服务器的连接。paramiko支持Linux, Solaris, BSD, MacOS X, Windows等平台通过SSH从一个平台连接到另外一个平台。利用该模块，可以方便的进行ssh连接和sftp协议进行sftp文件传输。 paramiko包里一共有种连接方式（两个类）：SSHClient和Transport，每种连接方式都支持口令认证和证书认证。 paramiko也有一些其它的方法和属性。本文只介绍简单、常用的。 paramiko不是python基本模块，需是基于pycrypto模块，所以需要安装pycrypto，再安装paramiko 12345pip3 install pycrypto # windows下多半是会报错的，原因是因为需要安装c++，并且设置变量。pip3 install paramiko # 实际上直接安装paramiko,自动安装依赖的包。我使用的pip版本是pip 9.0.3# 成功的时候会提示安装了下面这些包：pycparser, cffi, pynacl, bcrypt, pyasn1, asn1crypto, cryptography, paramiko paramiko—-&gt;SSHClient类——–&gt;口令认证 证书认证 paramiko—-&gt;Transport类——–&gt;口令认证 证书认证 一、SSHClient:用来远程执行命令方法/属性名 123456789101112131415161718192021222324252627282930313233343536371.connect()参数/参数类型:hostname(str:主机ip 必须参数) port(int:端口 必须参数)username(str:用户名 用户名和pkey密钥连接方式必须存在一个)password(str:密码)pkey(pkey:密钥)timeout(float:超时时间 可选)allow_agent(bool:当为flase时，禁用连接到ssh代理 可行)look_for_keys(bool:flase时，禁用在~/.ssh中搜索秘钥文件 可选)compress(bool:true时打开压缩 可选)2.exec_command()远程执行命令，popen远程版command(str:远程执行的命令，如果有多个命令需要操作时，需要通过分号进行分割)bufsize(int:缓冲区的大小默认为-1，无限制)3.load_system_host_keys()加载本地公秘钥校验文件，默认为~/.ssh/known_hostsfilename(str:远程主机公钥记录文件，linux系统下默认路径~/.ssh/known_hosts)4.set_missing_host_key_policy()连接主机没有本地主机秘钥或者HostKeys对象时策略，目前支持三种：AutoAddPolicy,RejectPolicy,WarningPolicyAutoAddPolicy:自动添加主机名以及主机密钥RejectPolicy（默认）：自动拒绝未知的主机名和秘钥WarningPolicy： 用于记录一个未知主机秘钥的Python警告示例：ssh = paramiko.SSHClient()ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())ssh.connect(.................)stdin.wirte(str) str 输入str型的字符确认继续。应该类似bat 中的pause 二、Transport：用来上传下载远程主机的文件 12345678910111213141516171819202122232425262728293031方法/属性名 作用 参数 示例Transport((主机名,端口号))建立远程主机加密码管道对象主机名：ip或者主机名，str型端口号：指定端口，int型sf = paramiko.Transport((&quot;192.168.1.1&quot;,22))connect(username,password)建立远程连接username：用户名password：密码sf.connect(username = &quot;root&quot;,password=&quot;areyouok1&quot;)SFTPClient.from_transport(加密管道)建立一个客户端对象，通过ssh transport操作远程文件加密管道：之前创建的sf对象sftp = paramiko.SFTPClient.from_transport(sf)get(远程文件,本地文件) 从远程下载指定文件存到本地 远程文件名本地文件名sftp.get(remotepath,localpath)put(本地文件,远程文件) 从本地上传指定文件到远程路径 本地文件名远程文件名sftp.put(localpath,remotepath)listdir(远程路径) 列出远程指定路径的文件夹 远程路径名 sftp.listdir(&quot;..&quot;) 案例： 123456789101112131415161718密码登陆方式代码如下:#!/usr/bin/env python# coding:utf-8import paramiko# 创建SSH对象ssh = paramiko.SSHClient()# 访问未知主机时候的策略，允许连接不在know_hosts文件中的主机ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())# 连接服务器ssh.connect(hostname=&apos;172.16.1.101&apos;, port=22, username=&apos;wgw&apos;, password=&apos;123456&apos;)# 执行命令stdin, stdout, stderr = ssh.exec_command(&apos;df&apos;)# 获取命令结果result = stdout.read().decode#打印远程命令的执行结果print(result)# 关闭连接ssh.close() 唯一要注意的就是遇到未知主机的时候，ssh的处理策略。这个要设置好。不然know_hosts文件里没有的主机在第一次登陆的时候会被拒绝掉。 使用SSH秘钥登陆的代码如下： 123456789101112131415161718#!/usr/bin/env python# coding:utf-8import paramiko#指定用于登录其他主机的私钥文件的存储位置 private_key = paramiko.RSAKey.from_private_key_file(&apos;/home/auto/.ssh/id_rsa&apos;)# 创建SSH对象ssh = paramiko.SSHClient()# 允许连接不在know_hosts文件中的主机ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())# 连接服务器的时候通过pkey关键字替代password关键字ssh.connect(hostname=&apos;172.16.1.101&apos;, port=22, username=&apos;wgw&apos;, pkey=private_key)# 执行命令stdin, stdout, stderr = ssh.exec_command(&apos;df&apos;)# 获取命令结果result = stdout.read().decode()print(result)# 关闭连接ssh.close() 通过秘钥登陆的用户和密码登陆大同小异，只需定义私钥文件的位置然后引用pkey关键字就好了。 使用密码和SFTP命令上传和下载文件的代码如下： 123456789import paramikotransport = paramiko.Transport((&apos;hostname&apos;,22))transport.connect(username=&apos;wupeiqi&apos;,password=&apos;123&apos;)sftp = paramiko.SFTPClient.from_transport(transport)# 将location.py 上传至服务器 /tmp/test.pysftp.put(&apos;/tmp/location.py&apos;, &apos;/tmp/test.py&apos;)# 将remove_path 下载到本地 local_pathsftp.get(&apos;remove_path&apos;, &apos;local_path&apos;) transport.close() 使用秘钥和SFTP命令上传和下载文件的代码如下： 12345678910111213import paramikoprivate_key = paramiko.RSAKey.from_private_key_file(&apos;/home/auto/.ssh/id_rsa&apos;)#创建transport对象 transport = paramiko.Transport((&apos;hostname&apos;, 22))#通过connect方法创建远程连接transport.connect(username=&apos;wgw&apos;, pkey=private_key )#调用SFTPClient填写刚才创建的transport对象来创建一个sftp对象 sftp = paramiko.SFTPClient.from_transport(transport)# 将location.py 上传至服务器 /tmp/test.pysftp.put(&apos;/tmp/location.py&apos;, &apos;/tmp/test.py&apos;)# 将remove_path 下载到本地 local_pathsftp.get(&apos;remove_path&apos;, &apos;local_path&apos;)transport.close() 代码不难，但是从代码里可以到实现ssh和sftp的时候。创建连接时使用的方法不一样。 SSH是使用SSHClient()里面的connect方法创建的 12ssh = paramiko.SSHClient()ssh.connect(hostname=&apos;172.16.1.101&apos;, port=22, username=&apos;wgw&apos;, password=&apos;123456&apos;) SFTP使用的Transport()里面的connetc方法创建的 12transport = paramiko.Transport((&apos;172.16.1.101&apos;,22))transport.connect(username=&apos;wgw&apos;,password=&apos;123456&apos;) 两个都是connect方法有什么区别和联系呢？其实paramiko.SSHClient().connect()这个方法的内部实现调用的就是Transport().connect()这个方法。所以可以认为Transport()是paramiko里面创建连接的通用方法。那我们通过Transport方法来改写一下SSH的功能 1234567891011import paramiko#调用Transport方法与实现SFTP的连接功能一样，创建连接transport = paramiko.Transport((&apos;172.16.1.101&apos;, 22))transport.connect(username=&apos;wgw&apos;, password=&apos;123456&apos;)#这个方法还是必须要加上的还需要调用exec_command（）等方法使用ssh = paramiko.SSHClient()#用这一句代替ssh.connect()方法ssh._transport = transportstdin, stdout, stderr = ssh.exec_command(&apos;df&apos;)print stdout.read()transport.close() 这样如果我们要实现一个可以远程操作主机的程序，那么程序的远程执行命令和传输文件等功能就可以写到一个类里面了。示意代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243import paramikoclass my_paramiko(object): def __init__(self,ip,port): self.ip=ip self.port=int(port) #定义登录被管理主机时使用的用户 self.manager=&apos;admin&apos; #指定登录各个主机时所使用的秘钥文件位置 self.key=paramiko.RSAKey.from_private_key_file(&apos;/home/wgw/.ssh/id_rsa&apos;) def connect(self): &quot;&quot;&quot; 这个方法用于调用paramiko创建连接。 &quot;&quot;&quot; #创建连接实例 transport = paramiko.Transport((self.ip,self.port)) transport.connect(username=self.manager, pkey=self.key) self.__transport=transport def cmd(self,user_cmd): &quot;&quot;&quot; 这个方法用于实现在远程主机上执行命令 &quot;&quot;&quot; #调用连接方法连接服务器 self.connect() #调用paramiko的SSH方法实现远程执行命令 ssh = paramiko.SSHClient() ssh._transport=self.__transport ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy()) #远程执行命令 ssh.exec_command(user_cmd) #执行命令后管理远程连接 ssh.close() def excute_upload(self,local_file,remote_file): &quot;&quot;&quot; 这个方法用于实现，上传文件到远程的功能。要求用户输入本地文件路径和远程文件 路径。 &quot;&quot;&quot; #调用连接方法连接服务器 self.connect() #调用paramiko的sftp方法实现远程上传命令 sftp = paramiko.SFTPClient.from_transport(self.__transport) #调用sftp.put方法把本地文件上传到服务器上。 sftp.put(local_file,remote_file)]]></content>
      <categories>
        <category>python模块</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh下know_hosts的作用]]></title>
    <url>%2F2019%2F08%2F19%2Fssh%E4%B8%8Bknow_hosts%E7%9A%84%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[ssh下know_hosts的作用在平时工作中，有时候需要ssh登陆到别的linux主机上去，但有时候ssh登陆会被禁止，并弹出如下类似提示: 1@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!Someone could be eavesdropping on you right now (man-in-the-middle attack)!It is also possible that the RSA host key has just been changed.The fingerprint for the RSA key sent by the remote host is36:68:a6:e6:43:34:6b:82:d7:f4:df:1f:c2:e7:37:cc.Please contact your system administrator.Add correct host key in /u/xlian008/.ssh/known_hosts to get rid of this message.Offending key in /u/xlian008/.ssh/known_hosts:2RSA host key for 135.1.35.130 has changed and you have requested strict checking.Host key verification failed. 这是因为ssh会把你访问过计算机的公钥(public key)都记录在~/.ssh/known_hosts。当下次访问想再计算机时，openssh会核对公钥。如果公钥不同。openssh会发出警告，避免你受到dns hijack之类的攻击。上面就是种情况。 原因:一台主机上有多个Linux系统，会经常切换，那么这些系统使用同一ip，登录过一次后就会把ssh信息记录在本地的~/.ssh/known_hsots文件中，切换该系统后再用ssh访问这台主机就会出现冲突警告，需要手动删除修改known_hsots里面的内容。 有以下两个解决方案： 手动删除修改known_hsots里面的内容； 修改配置文件“~/.ssh/config”，加上这两行，重启服务器。 StrictHostKeyChecking no UserKnownHostsFile /dev/null 优缺点： 需要每次手动删除文件内容，一些自动化脚本的无法运行（在SSH登陆时失败），但是安全性高； SSH登陆时会忽略known_hsots的访问，但是安全性低；]]></content>
      <categories>
        <category>linux基础</category>
      </categories>
      <tags>
        <tag>linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python模块学习]]></title>
    <url>%2F2019%2F08%2F19%2Fpython%E6%A8%A1%E5%9D%97%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[python常用模块模块，用一砣代码实现了某个功能的代码集合 类似于函数式编程和面向过程编程，函数式编程则完成一个功能，其他代码用来调用即可，提供了代码的重用性和代码间的耦合。而对于一个复杂的功能来，可能需要多个函数才能完成(函数又可以在不同的.py文件中)，n个.py文件组成的代码集合就称为模块。 如os是系统相关的模块，file是文件操作相关的模块 模块分为三种: 自定义模块 内置模块 开源模块 2.导入模块python之所以应用越来越广泛，在一定程度上也依赖于其为程序员提供了大量的模块以供使用，如果想要使用模块，则需要导入，导入模块有以下几种方法: 1234import modulefrom module.xx.xx import xxfrom module.xx.xx import xx as renamefrom module.xx.xx import * 导入模块其实就是告诉python解释器去解释那个py文件 导入一个py文件，解释器解释该py文件 导入一个包，解释器解释该包下的init.py文件 那么问题来了，导入模块时是根据那个路径作为基准来进行的呢？即:sys.path 1234import sysprint(sys.path)结果：[&apos;/Users/wupeiqi/PycharmProjects/calculator/p1/pp1&apos;, &apos;/usr/local/lib/python2.7/site-packages/setuptools-15.2-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/MySQL_python-1.2.4b4-py2.7-macosx-10.10-x86_64.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/xlutils-1.7.1-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/xlwt-1.0.0-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/xlrd-0.9.3-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/tornado-4.1-py2.7-macosx-10.10-x86_64.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/backports.ssl_match_hostname-3.4.0.2-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/certifi-2015.4.28-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/pyOpenSSL-0.15.1-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/six-1.9.0-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/cryptography-0.9.1-py2.7-macosx-10.10-x86_64.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/cffi-1.1.1-py2.7-macosx-10.10-x86_64.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/ipaddress-1.0.7-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/enum34-1.0.4-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/pyasn1-0.1.7-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/idna-2.0-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/pycparser-2.13-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/Django-1.7.8-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/paramiko-1.10.1-py2.7.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/gevent-1.0.2-py2.7-macosx-10.10-x86_64.egg&apos;, &apos;/usr/local/lib/python2.7/site-packages/greenlet-0.4.7-py2.7-macosx-10.10-x86_64.egg&apos;, &apos;/Users/wupeiqi/PycharmProjects/calculator&apos;, &apos;/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/python27.zip&apos;, &apos;/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/python2.7&apos;, &apos;/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-darwin&apos;, &apos;/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac&apos;, &apos;/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac/lib-scriptpackages&apos;, &apos;/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk&apos;, &apos;/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-old&apos;, &apos;/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload&apos;, &apos;/usr/local/lib/python2.7/site-packages&apos;, &apos;/Library/Python/2.7/site-packages&apos;] 如果sys.path路径列表没有你想要的路径，可以通过sys.path.append(‘路径’)添加。 通过os模块可以获取各种目录，例如： 12345import sysimport ospre_path = os.path.abspath(&apos;../&apos;)sys.path.append(pre_path) 开源模块一、下载安装下载安装有两种方式: 123yum pipapt-get 12345下载源码解压源码进入目录编译源码 python setup.py build安装源码 python setup.py install 注：在使用源码安装时，需要使用到gcc编译和python开发环境，所以，需要先执行： 1234yum install gccyum install python-devel或apt-get python-dev 安装成功后，模块会自动安装到 sys.path 中的某个目录中，如： 1/usr/lib/python2.7/site-packages/ 二、导入模块同自定义模块中导入的方式 三、模块 paramikoparamiko是一个用于做远程控制的模块，使用该模块可以对远程服务器进行命令或文件操作，值得一说的是，fabric和ansible内部的远程管理就是使用的paramiko来现实。 1、下载安装 1pip3 install paramiko 或 12345678910111213141516171819# pycrypto，由于 paramiko 模块内部依赖pycrypto，所以先下载安装pycrypto # 下载安装 pycryptowget http://files.cnblogs.com/files/wupeiqi/pycrypto-2.6.1.tar.gztar -xvf pycrypto-2.6.1.tar.gzcd pycrypto-2.6.1python setup.py buildpython setup.py install # 进入python环境，导入Crypto检查是否安装成功 # 下载安装 paramikowget http://files.cnblogs.com/files/wupeiqi/paramiko-1.10.1.tar.gztar -xvf paramiko-1.10.1.tar.gzcd paramiko-1.10.1python setup.py buildpython setup.py install # 进入python环境，导入paramiko检查是否安装成功 2、使用模块123456789101112执行命令 - 通过用户名和密码连接服务器#!/usr/bin/env python#coding:utf-8import paramikossh = paramiko.SSHClient()ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())ssh.connect(&apos;192.168.1.108&apos;, 22, &apos;alex&apos;, &apos;123&apos;)stdin, stdout, stderr = ssh.exec_command(&apos;df&apos;)print stdout.read()ssh.close(); 12345678910111213执行命令 - 过密钥链接服务器import paramikoprivate_key_path = &apos;/home/auto/.ssh/id_rsa&apos;key = paramiko.RSAKey.from_private_key_file(private_key_path)ssh = paramiko.SSHClient()ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())ssh.connect(&apos;主机名 &apos;, 端口, &apos;用户名&apos;, key)stdin, stdout, stderr = ssh.exec_command(&apos;df&apos;)print stdout.read()ssh.close() 12345678910111213141516171819上传或者下载文件 - 通过用户名和密码import os,sysimport paramikot = paramiko.Transport((&apos;182.92.219.86&apos;,22))t.connect(username=&apos;wupeiqi&apos;,password=&apos;123&apos;)sftp = paramiko.SFTPClient.from_transport(t)sftp.put(&apos;/tmp/test.py&apos;,&apos;/tmp/test.py&apos;) t.close()import os,sysimport paramikot = paramiko.Transport((&apos;182.92.219.86&apos;,22))t.connect(username=&apos;wupeiqi&apos;,password=&apos;123&apos;)sftp = paramiko.SFTPClient.from_transport(t)sftp.get(&apos;/tmp/test.py&apos;,&apos;/tmp/test2.py&apos;)t.close() 1234567891011121314151617181920212223242526上传或下载文件 - 通过密钥import paramikopravie_key_path = &apos;/home/auto/.ssh/id_rsa&apos;key = paramiko.RSAKey.from_private_key_file(pravie_key_path)t = paramiko.Transport((&apos;182.92.219.86&apos;,22))t.connect(username=&apos;wupeiqi&apos;,pkey=key)sftp = paramiko.SFTPClient.from_transport(t)sftp.put(&apos;/tmp/test3.py&apos;,&apos;/tmp/test3.py&apos;) t.close()import paramikopravie_key_path = &apos;/home/auto/.ssh/id_rsa&apos;key = paramiko.RSAKey.from_private_key_file(pravie_key_path)t = paramiko.Transport((&apos;182.92.219.86&apos;,22))t.connect(username=&apos;wupeiqi&apos;,pkey=key)sftp = paramiko.SFTPClient.from_transport(t)sftp.get(&apos;/tmp/test3.py&apos;,&apos;/tmp/test4.py&apos;) t.close() 内置模块一、os用于提供系统级别的操作 1234567891011121314151617181920212223242526272829os.getcwd() 获取当前工作目录，即当前python脚本工作的目录路径os.chdir(&quot;dirname&quot;) 改变当前脚本工作目录；相当于shell下cdos.curdir 返回当前目录: (&apos;.&apos;)os.pardir 获取当前目录的父目录字符串名：(&apos;..&apos;)os.makedirs(&apos;dirname1/dirname2&apos;) 可生成多层递归目录os.removedirs(&apos;dirname1&apos;) 若目录为空，则删除，并递归到上一级目录，如若也为空，则删除，依此类推os.mkdir(&apos;dirname&apos;) 生成单级目录；相当于shell中mkdir dirnameos.rmdir(&apos;dirname&apos;) 删除单级空目录，若目录不为空则无法删除，报错；相当于shell中rmdir dirnameos.listdir(&apos;dirname&apos;) 列出指定目录下的所有文件和子目录，包括隐藏文件，并以列表方式打印os.remove() 删除一个文件os.rename(&quot;oldname&quot;,&quot;newname&quot;) 重命名文件/目录os.stat(&apos;path/filename&apos;) 获取文件/目录信息os.sep 输出操作系统特定的路径分隔符，win下为&quot;\\&quot;,Linux下为&quot;/&quot;os.linesep 输出当前平台使用的行终止符，win下为&quot;\t\n&quot;,Linux下为&quot;\n&quot;os.pathsep 输出用于分割文件路径的字符串os.name 输出字符串指示当前使用平台。win-&gt;&apos;nt&apos;; Linux-&gt;&apos;posix&apos;os.system(&quot;bash command&quot;) 运行shell命令，直接显示os.environ 获取系统环境变量os.path.abspath(path) 返回path规范化的绝对路径os.path.split(path) 将path分割成目录和文件名二元组返回os.path.dirname(path) 返回path的目录。其实就是os.path.split(path)的第一个元素os.path.basename(path) 返回path最后的文件名。如何path以／或\结尾，那么就会返回空值。即os.path.split(path)的第二个元素os.path.exists(path) 如果path存在，返回True；如果path不存在，返回Falseos.path.isabs(path) 如果path是绝对路径，返回Trueos.path.isfile(path) 如果path是一个存在的文件，返回True。否则返回Falseos.path.isdir(path) 如果path是一个存在的目录，则返回True。否则返回Falseos.path.join(path1[, path2[, ...]]) 将多个路径组合后返回，第一个绝对路径之前的参数将被忽略os.path.getatime(path) 返回path所指向的文件或者目录的最后存取时间os.path.getmtime(path) 返回path所指向的文件或者目录的最后修改时间 二、sys用于提供对解释器相关的操作 12345678sys.argv 命令行参数List，第一个元素是程序本身路径sys.exit(n) 退出程序，正常退出时exit(0)sys.version 获取Python解释程序的版本信息sys.maxint 最大的Int值sys.path 返回模块的搜索路径，初始化时使用PYTHONPATH环境变量的值sys.platform 返回操作系统平台名称sys.stdout.write(&apos;please:&apos;)val = sys.stdin.readline()[:-1] 三、hashlib用于加密相关的操作，代替了md5模块和sha模块，主要提供 SHA1, SHA224, SHA256, SHA384, SHA512 ，MD5 算法 12345md5废弃import md5hash = md5.new()hash.update(&apos;admin&apos;)print hash.hexdigest() 123456sha废弃import shahash = sha.new()hash.update(&apos;admin&apos;)print hash.hexdigest() 1234567891011121314151617181920212223242526272829303132import hashlib # ######## md5 ######## hash = hashlib.md5()hash.update(&apos;admin&apos;)print hash.hexdigest() # ######## sha1 ######## hash = hashlib.sha1()hash.update(&apos;admin&apos;)print hash.hexdigest() # ######## sha256 ######## hash = hashlib.sha256()hash.update(&apos;admin&apos;)print hash.hexdigest() # ######## sha384 ######## hash = hashlib.sha384()hash.update(&apos;admin&apos;)print hash.hexdigest() # ######## sha512 ######## hash = hashlib.sha512()hash.update(&apos;admin&apos;)print hash.hexdigest() 以上加密算法虽然依然非常厉害，但时候存在缺陷，即：通过撞库可以反解。所以，有必要对加密算法中添加自定义key再来做加密。 1234567import hashlib # ######## md5 ######## hash = hashlib.md5(&apos;898oaFs09f&apos;)hash.update(&apos;admin&apos;)print hash.hexdigest() 还不够吊？python 还有一个 hmac 模块，它内部对我们创建 key 和 内容 再进行处理然后再加密 12345import hmach = hmac.new(&apos;wueiqi&apos;)h.update(&apos;hellowo&apos;)print h.hexdigest()不能再牛逼了！！！ 四、json 和 pickle用于序列化的两个模块 json，用于字符串 和 python数据类型间进行转换 pickle，用于python特有的类型 和 python的数据类型间进行转换 Json模块提供了四个功能：dumps、dump、loads、load pickle模块提供了四个功能：dumps、dump、loads、load 五、执行系统命令可以执行shell命令的相关模块和函数有： 12345os.systemos.spawn*os.popen* --废弃popen2.* --废弃commands.* --废弃，3.x中被移除 12345import commandsresult = commands.getoutput(&apos;cmd&apos;)result = commands.getstatus(&apos;cmd&apos;)result = commands.getstatusoutput(&apos;cmd&apos;) 以上执行shell命令的相关的模块和函数的功能均在 subprocess 模块中实现，并提供了更丰富的功能。 call 执行命令，返回状态码 12ret = subprocess.call([&quot;ls&quot;, &quot;-l&quot;], shell=False)ret = subprocess.call(&quot;ls -l&quot;, shell=True) shell = True ，允许 shell 命令是字符串形式 check_call 执行命令，如果执行状态码是 0 ，则返回0，否则抛异常 12subprocess.check_call([&quot;ls&quot;, &quot;-l&quot;])subprocess.check_call(&quot;exit 1&quot;, shell=True) check_output 执行命令，如果状态码是 0 ，则返回执行结果，否则抛异常 12subprocess.check_output([&quot;echo&quot;, &quot;Hello World!&quot;])subprocess.check_output(&quot;exit 1&quot;, shell=True) subprocess.Popen(…) 用于执行复杂的系统命令 参数： 123456789101112args：shell命令，可以是字符串或者序列类型（如：list，元组）bufsize：指定缓冲。0 无缓冲,1 行缓冲,其他 缓冲区大小,负值 系统缓冲stdin, stdout, stderr：分别表示程序的标准输入、输出、错误句柄preexec_fn：只在Unix平台下有效，用于指定一个可执行对象（callable object），它将在子进程运行之前被调用close_sfs：在windows平台下，如果close_fds被设置为True，则新创建的子进程将不会继承父进程的输入、输出、错误管道。所以不能将close_fds设置为True同时重定向子进程的标准输入、输出与错误(stdin, stdout, stderr)。shell：同上cwd：用于设置子进程的当前目录env：用于指定子进程的环境变量。如果env = None，子进程的环境变量将从父进程中继承。universal_newlines：不同系统的换行符不同，True -&gt; 同意使用 \nstartupinfo与createionflags只在windows下有效将被传递给底层的CreateProcess()函数，用于设置子进程的一些属性，如：主窗口的外观，进程的优先级等等 123import subprocessret1 = subprocess.Popen([&quot;mkdir&quot;,&quot;t1&quot;])ret2 = subprocess.Popen(&quot;mkdir t2&quot;, shell=True) 终端输入的命令分为两种： 输入即可得到输出，如：ifconfig 输入进行某环境，依赖再输入，如：python 123import subprocessobj = subprocess.Popen(&quot;mkdir t3&quot;, shell=True, cwd=&apos;/home/dev&apos;,) 12345678910111213141516import subprocessobj = subprocess.Popen([&quot;python&quot;], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)obj.stdin.write(&apos;print 1 \n &apos;)obj.stdin.write(&apos;print 2 \n &apos;)obj.stdin.write(&apos;print 3 \n &apos;)obj.stdin.write(&apos;print 4 \n &apos;)obj.stdin.close()cmd_out = obj.stdout.read()obj.stdout.close()cmd_error = obj.stderr.read()obj.stderr.close()print cmd_outprint cmd_error 12345678910import subprocessobj = subprocess.Popen([&quot;python&quot;], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)obj.stdin.write(&apos;print 1 \n &apos;)obj.stdin.write(&apos;print 2 \n &apos;)obj.stdin.write(&apos;print 3 \n &apos;)obj.stdin.write(&apos;print 4 \n &apos;)out_error_list = obj.communicate()print out_error_list 12345import subprocessobj = subprocess.Popen([&quot;python&quot;], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)out_error_list = obj.communicate(&apos;print &quot;hello&quot;&apos;)print out_error_list 六、shutil 高级的 文件、文件夹、压缩包 处理模块 shutil.copyfileobj(fsrc, fdst[, length]) 将文件内容拷贝到另一个文件中，可以部分内容 1234567def copyfileobj(fsrc, fdst, length=16*1024): &quot;&quot;&quot;copy data from file-like object fsrc to file-like object fdst&quot;&quot;&quot; while 1: buf = fsrc.read(length) if not buf: break fdst.write(buf) shutil.copyfile(src, dst) 拷贝文件 12345678910111213141516171819def copyfile(src, dst): &quot;&quot;&quot;Copy data from src to dst&quot;&quot;&quot; if _samefile(src, dst): raise Error(&quot;`%s` and `%s` are the same file&quot; % (src, dst)) for fn in [src, dst]: try: st = os.stat(fn) except OSError: # File most likely does not exist pass else: # XXX What about other special files? (sockets, devices...) if stat.S_ISFIFO(st.st_mode): raise SpecialFileError(&quot;`%s` is a named pipe&quot; % fn) with open(src, &apos;rb&apos;) as fsrc: with open(dst, &apos;wb&apos;) as fdst: copyfileobj(fsrc, fdst) shutil.copymode(src, dst) 仅拷贝权限。内容、组、用户均不变 123456def copymode(src, dst): &quot;&quot;&quot;Copy mode bits from src to dst&quot;&quot;&quot; if hasattr(os, &apos;chmod&apos;): st = os.stat(src) mode = stat.S_IMODE(st.st_mode) os.chmod(dst, mode) shutil.copystat(src, dst) 拷贝状态的信息，包括：mode bits, atime, mtime, flags 1234567891011121314151617def copystat(src, dst): &quot;&quot;&quot;Copy all stat info (mode bits, atime, mtime, flags) from src to dst&quot;&quot;&quot; st = os.stat(src) mode = stat.S_IMODE(st.st_mode) if hasattr(os, &apos;utime&apos;): os.utime(dst, (st.st_atime, st.st_mtime)) if hasattr(os, &apos;chmod&apos;): os.chmod(dst, mode) if hasattr(os, &apos;chflags&apos;) and hasattr(st, &apos;st_flags&apos;): try: os.chflags(dst, st.st_flags) except OSError, why: for err in &apos;EOPNOTSUPP&apos;, &apos;ENOTSUP&apos;: if hasattr(errno, err) and why.errno == getattr(errno, err): break else: raise shutil.copy(src, dst) 拷贝文件和权限 12345678910def copy(src, dst): &quot;&quot;&quot;Copy data and mode bits (&quot;cp src dst&quot;). The destination may be a directory. &quot;&quot;&quot; if os.path.isdir(dst): dst = os.path.join(dst, os.path.basename(src)) copyfile(src, dst) copymode(src, dst) shutil.copy2(src, dst) 拷贝文件和状态信息 12345678910def copy2(src, dst): &quot;&quot;&quot;Copy data and all stat info (&quot;cp -p src dst&quot;). The destination may be a directory. &quot;&quot;&quot; if os.path.isdir(dst): dst = os.path.join(dst, os.path.basename(src)) copyfile(src, dst) copystat(src, dst) shutil.ignore_patterns(*patterns) shutil.copytree(src, dst, symlinks=False, ignore=None) 递归的去拷贝文件 例如：copytree(source, destination, ignore=ignore_patterns(‘.pyc’, ‘tmp‘)) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576def ignore_patterns(*patterns): &quot;&quot;&quot;Function that can be used as copytree() ignore parameter. Patterns is a sequence of glob-style patterns that are used to exclude files&quot;&quot;&quot; def _ignore_patterns(path, names): ignored_names = [] for pattern in patterns: ignored_names.extend(fnmatch.filter(names, pattern)) return set(ignored_names) return _ignore_patternsdef copytree(src, dst, symlinks=False, ignore=None): &quot;&quot;&quot;Recursively copy a directory tree using copy2(). The destination directory must not already exist. If exception(s) occur, an Error is raised with a list of reasons. If the optional symlinks flag is true, symbolic links in the source tree result in symbolic links in the destination tree; if it is false, the contents of the files pointed to by symbolic links are copied. The optional ignore argument is a callable. If given, it is called with the `src` parameter, which is the directory being visited by copytree(), and `names` which is the list of `src` contents, as returned by os.listdir(): callable(src, names) -&gt; ignored_names Since copytree() is called recursively, the callable will be called once for each directory that is copied. It returns a list of names relative to the `src` directory that should not be copied. XXX Consider this example code rather than the ultimate tool. &quot;&quot;&quot; names = os.listdir(src) if ignore is not None: ignored_names = ignore(src, names) else: ignored_names = set() os.makedirs(dst) errors = [] for name in names: if name in ignored_names: continue srcname = os.path.join(src, name) dstname = os.path.join(dst, name) try: if symlinks and os.path.islink(srcname): linkto = os.readlink(srcname) os.symlink(linkto, dstname) elif os.path.isdir(srcname): copytree(srcname, dstname, symlinks, ignore) else: # Will raise a SpecialFileError for unsupported file types copy2(srcname, dstname) # catch the Error from the recursive copytree so that we can # continue with other files except Error, err: errors.extend(err.args[0]) except EnvironmentError, why: errors.append((srcname, dstname, str(why))) try: copystat(src, dst) except OSError, why: if WindowsError is not None and isinstance(why, WindowsError): # Copying file access times may fail on Windows pass else: errors.append((src, dst, str(why))) if errors: raise Error, errors shutil.rmtree(path[, ignore_errors[, onerror]]) 递归的去删除文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def rmtree(path, ignore_errors=False, onerror=None): &quot;&quot;&quot;Recursively delete a directory tree. If ignore_errors is set, errors are ignored; otherwise, if onerror is set, it is called to handle the error with arguments (func, path, exc_info) where func is os.listdir, os.remove, or os.rmdir; path is the argument to that function that caused it to fail; and exc_info is a tuple returned by sys.exc_info(). If ignore_errors is false and onerror is None, an exception is raised. &quot;&quot;&quot; if ignore_errors: def onerror(*args): pass elif onerror is None: def onerror(*args): raise try: if os.path.islink(path): # symlinks to directories are forbidden, see bug #1669 raise OSError(&quot;Cannot call rmtree on a symbolic link&quot;) except OSError: onerror(os.path.islink, path, sys.exc_info()) # can&apos;t continue even if onerror hook returns return names = [] try: names = os.listdir(path) except os.error, err: onerror(os.listdir, path, sys.exc_info()) for name in names: fullname = os.path.join(path, name) try: mode = os.lstat(fullname).st_mode except os.error: mode = 0 if stat.S_ISDIR(mode): rmtree(fullname, ignore_errors, onerror) else: try: os.remove(fullname) except os.error, err: onerror(os.remove, fullname, sys.exc_info()) try: os.rmdir(path) except os.error: onerror(os.rmdir, path, sys.exc_info()) shutil.move(src, dst) 递归的去移动文件 123456789101112131415161718192021222324252627282930313233343536373839def move(src, dst): &quot;&quot;&quot;Recursively move a file or directory to another location. This is similar to the Unix &quot;mv&quot; command. If the destination is a directory or a symlink to a directory, the source is moved inside the directory. The destination path must not already exist. If the destination already exists but is not a directory, it may be overwritten depending on os.rename() semantics. If the destination is on our current filesystem, then rename() is used. Otherwise, src is copied to the destination and then removed. A lot more could be done here... A look at a mv.c shows a lot of the issues this implementation glosses over. &quot;&quot;&quot; real_dst = dst if os.path.isdir(dst): if _samefile(src, dst): # We might be on a case insensitive filesystem, # perform the rename anyway. os.rename(src, dst) return real_dst = os.path.join(dst, _basename(src)) if os.path.exists(real_dst): raise Error, &quot;Destination path &apos;%s&apos; already exists&quot; % real_dst try: os.rename(src, real_dst) except OSError: if os.path.isdir(src): if _destinsrc(src, dst): raise Error, &quot;Cannot move a directory &apos;%s&apos; into itself &apos;%s&apos;.&quot; % (src, dst) copytree(src, real_dst, symlinks=True) rmtree(src) else: copy2(src, real_dst) os.unlink(src) shutil.make_archive(base_name, format,…) 创建压缩包并返回文件路径，例如：zip、tar 123456789101112131415161718base_name： 压缩包的文件名，也可以是压缩包的路径。只是文件名时，则保存至当前目录，否则保存至指定路径，如：www =&gt;保存至当前路径如：/Users/wupeiqi/www =&gt;保存至/Users/wupeiqi/format： 压缩包种类，“zip”, “tar”, “bztar”，“gztar”root_dir： 要压缩的文件夹路径（默认当前目录）owner： 用户，默认当前用户group： 组，默认当前组logger： 用于记录日志，通常是logging.Logger对象#将 /Users/wupeiqi/Downloads/test 下的文件打包放置当前程序目录 import shutilret = shutil.make_archive(&quot;wwwwwwwwww&quot;, &apos;gztar&apos;, root_dir=&apos;/Users/wupeiqi/Downloads/test&apos;) #将 /Users/wupeiqi/Downloads/test 下的文件打包放置 /Users/wupeiqi/目录import shutilret = shutil.make_archive(&quot;/Users/wupeiqi/wwwwwwwwww&quot;, &apos;gztar&apos;, root_dir=&apos;/Users/wupeiqi/Downloads/test&apos;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253def make_archive(base_name, format, root_dir=None, base_dir=None, verbose=0, dry_run=0, owner=None, group=None, logger=None): &quot;&quot;&quot;Create an archive file (eg. zip or tar). &apos;base_name&apos; is the name of the file to create, minus any format-specific extension; &apos;format&apos; is the archive format: one of &quot;zip&quot;, &quot;tar&quot;, &quot;bztar&quot; or &quot;gztar&quot;. &apos;root_dir&apos; is a directory that will be the root directory of the archive; ie. we typically chdir into &apos;root_dir&apos; before creating the archive. &apos;base_dir&apos; is the directory where we start archiving from; ie. &apos;base_dir&apos; will be the common prefix of all files and directories in the archive. &apos;root_dir&apos; and &apos;base_dir&apos; both default to the current directory. Returns the name of the archive file. &apos;owner&apos; and &apos;group&apos; are used when creating a tar archive. By default, uses the current owner and group. &quot;&quot;&quot; save_cwd = os.getcwd() if root_dir is not None: if logger is not None: logger.debug(&quot;changing into &apos;%s&apos;&quot;, root_dir) base_name = os.path.abspath(base_name) if not dry_run: os.chdir(root_dir) if base_dir is None: base_dir = os.curdir kwargs = &#123;&apos;dry_run&apos;: dry_run, &apos;logger&apos;: logger&#125; try: format_info = _ARCHIVE_FORMATS[format] except KeyError: raise ValueError, &quot;unknown archive format &apos;%s&apos;&quot; % format func = format_info[0] for arg, val in format_info[1]: kwargs[arg] = val if format != &apos;zip&apos;: kwargs[&apos;owner&apos;] = owner kwargs[&apos;group&apos;] = group try: filename = func(base_name, base_dir, **kwargs) finally: if root_dir is not None: if logger is not None: logger.debug(&quot;changing back to &apos;%s&apos;&quot;, save_cwd) os.chdir(save_cwd) return filename shutil 对压缩包的处理是调用 ZipFile 和 TarFile 两个模块来进行的，详细： 12345678910111213zipfile 压缩解压import zipfile# 压缩z = zipfile.ZipFile(&apos;laxi.zip&apos;, &apos;w&apos;)z.write(&apos;a.log&apos;)z.write(&apos;data.data&apos;)z.close()# 解压z = zipfile.ZipFile(&apos;laxi.zip&apos;, &apos;r&apos;)z.extractall()z.close() 12345678910111213tarfile 压缩解压import tarfile# 压缩tar = tarfile.open(&apos;your.tar&apos;,&apos;w&apos;)tar.add(&apos;/Users/wupeiqi/PycharmProjects/bbs2.zip&apos;, arcname=&apos;bbs2.zip&apos;)tar.add(&apos;/Users/wupeiqi/PycharmProjects/cmdb.zip&apos;, arcname=&apos;cmdb.zip&apos;)tar.close()# 解压tar = tarfile.open(&apos;your.tar&apos;,&apos;r&apos;)tar.extractall() # 可设置解压地址tar.close() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663ZipFileclass ZipFile(object): &quot;&quot;&quot; Class with methods to open, read, write, close, list zip files. z = ZipFile(file, mode=&quot;r&quot;, compression=ZIP_STORED, allowZip64=False) file: Either the path to the file, or a file-like object. If it is a path, the file will be opened and closed by ZipFile. mode: The mode can be either read &quot;r&quot;, write &quot;w&quot; or append &quot;a&quot;. compression: ZIP_STORED (no compression) or ZIP_DEFLATED (requires zlib). allowZip64: if True ZipFile will create files with ZIP64 extensions when needed, otherwise it will raise an exception when this would be necessary. &quot;&quot;&quot; fp = None # Set here since __del__ checks it def __init__(self, file, mode=&quot;r&quot;, compression=ZIP_STORED, allowZip64=False): &quot;&quot;&quot;Open the ZIP file with mode read &quot;r&quot;, write &quot;w&quot; or append &quot;a&quot;.&quot;&quot;&quot; if mode not in (&quot;r&quot;, &quot;w&quot;, &quot;a&quot;): raise RuntimeError(&apos;ZipFile() requires mode &quot;r&quot;, &quot;w&quot;, or &quot;a&quot;&apos;) if compression == ZIP_STORED: pass elif compression == ZIP_DEFLATED: if not zlib: raise RuntimeError,\ &quot;Compression requires the (missing) zlib module&quot; else: raise RuntimeError, &quot;That compression method is not supported&quot; self._allowZip64 = allowZip64 self._didModify = False self.debug = 0 # Level of printing: 0 through 3 self.NameToInfo = &#123;&#125; # Find file info given name self.filelist = [] # List of ZipInfo instances for archive self.compression = compression # Method of compression self.mode = key = mode.replace(&apos;b&apos;, &apos;&apos;)[0] self.pwd = None self._comment = &apos;&apos; # Check if we were passed a file-like object if isinstance(file, basestring): self._filePassed = 0 self.filename = file modeDict = &#123;&apos;r&apos; : &apos;rb&apos;, &apos;w&apos;: &apos;wb&apos;, &apos;a&apos; : &apos;r+b&apos;&#125; try: self.fp = open(file, modeDict[mode]) except IOError: if mode == &apos;a&apos;: mode = key = &apos;w&apos; self.fp = open(file, modeDict[mode]) else: raise else: self._filePassed = 1 self.fp = file self.filename = getattr(file, &apos;name&apos;, None) try: if key == &apos;r&apos;: self._RealGetContents() elif key == &apos;w&apos;: # set the modified flag so central directory gets written # even if no files are added to the archive self._didModify = True elif key == &apos;a&apos;: try: # See if file is a zip file self._RealGetContents() # seek to start of directory and overwrite self.fp.seek(self.start_dir, 0) except BadZipfile: # file is not a zip file, just append self.fp.seek(0, 2) # set the modified flag so central directory gets written # even if no files are added to the archive self._didModify = True else: raise RuntimeError(&apos;Mode must be &quot;r&quot;, &quot;w&quot; or &quot;a&quot;&apos;) except: fp = self.fp self.fp = None if not self._filePassed: fp.close() raise def __enter__(self): return self def __exit__(self, type, value, traceback): self.close() def _RealGetContents(self): &quot;&quot;&quot;Read in the table of contents for the ZIP file.&quot;&quot;&quot; fp = self.fp try: endrec = _EndRecData(fp) except IOError: raise BadZipfile(&quot;File is not a zip file&quot;) if not endrec: raise BadZipfile, &quot;File is not a zip file&quot; if self.debug &gt; 1: print endrec size_cd = endrec[_ECD_SIZE] # bytes in central directory offset_cd = endrec[_ECD_OFFSET] # offset of central directory self._comment = endrec[_ECD_COMMENT] # archive comment # &quot;concat&quot; is zero, unless zip was concatenated to another file concat = endrec[_ECD_LOCATION] - size_cd - offset_cd if endrec[_ECD_SIGNATURE] == stringEndArchive64: # If Zip64 extension structures are present, account for them concat -= (sizeEndCentDir64 + sizeEndCentDir64Locator) if self.debug &gt; 2: inferred = concat + offset_cd print &quot;given, inferred, offset&quot;, offset_cd, inferred, concat # self.start_dir: Position of start of central directory self.start_dir = offset_cd + concat fp.seek(self.start_dir, 0) data = fp.read(size_cd) fp = cStringIO.StringIO(data) total = 0 while total &lt; size_cd: centdir = fp.read(sizeCentralDir) if len(centdir) != sizeCentralDir: raise BadZipfile(&quot;Truncated central directory&quot;) centdir = struct.unpack(structCentralDir, centdir) if centdir[_CD_SIGNATURE] != stringCentralDir: raise BadZipfile(&quot;Bad magic number for central directory&quot;) if self.debug &gt; 2: print centdir filename = fp.read(centdir[_CD_FILENAME_LENGTH]) # Create ZipInfo instance to store file information x = ZipInfo(filename) x.extra = fp.read(centdir[_CD_EXTRA_FIELD_LENGTH]) x.comment = fp.read(centdir[_CD_COMMENT_LENGTH]) x.header_offset = centdir[_CD_LOCAL_HEADER_OFFSET] (x.create_version, x.create_system, x.extract_version, x.reserved, x.flag_bits, x.compress_type, t, d, x.CRC, x.compress_size, x.file_size) = centdir[1:12] x.volume, x.internal_attr, x.external_attr = centdir[15:18] # Convert date/time code to (year, month, day, hour, min, sec) x._raw_time = t x.date_time = ( (d&gt;&gt;9)+1980, (d&gt;&gt;5)&amp;0xF, d&amp;0x1F, t&gt;&gt;11, (t&gt;&gt;5)&amp;0x3F, (t&amp;0x1F) * 2 ) x._decodeExtra() x.header_offset = x.header_offset + concat x.filename = x._decodeFilename() self.filelist.append(x) self.NameToInfo[x.filename] = x # update total bytes read from central directory total = (total + sizeCentralDir + centdir[_CD_FILENAME_LENGTH] + centdir[_CD_EXTRA_FIELD_LENGTH] + centdir[_CD_COMMENT_LENGTH]) if self.debug &gt; 2: print &quot;total&quot;, total def namelist(self): &quot;&quot;&quot;Return a list of file names in the archive.&quot;&quot;&quot; l = [] for data in self.filelist: l.append(data.filename) return l def infolist(self): &quot;&quot;&quot;Return a list of class ZipInfo instances for files in the archive.&quot;&quot;&quot; return self.filelist def printdir(self): &quot;&quot;&quot;Print a table of contents for the zip file.&quot;&quot;&quot; print &quot;%-46s %19s %12s&quot; % (&quot;File Name&quot;, &quot;Modified &quot;, &quot;Size&quot;) for zinfo in self.filelist: date = &quot;%d-%02d-%02d %02d:%02d:%02d&quot; % zinfo.date_time[:6] print &quot;%-46s %s %12d&quot; % (zinfo.filename, date, zinfo.file_size) def testzip(self): &quot;&quot;&quot;Read all the files and check the CRC.&quot;&quot;&quot; chunk_size = 2 ** 20 for zinfo in self.filelist: try: # Read by chunks, to avoid an OverflowError or a # MemoryError with very large embedded files. with self.open(zinfo.filename, &quot;r&quot;) as f: while f.read(chunk_size): # Check CRC-32 pass except BadZipfile: return zinfo.filename def getinfo(self, name): &quot;&quot;&quot;Return the instance of ZipInfo given &apos;name&apos;.&quot;&quot;&quot; info = self.NameToInfo.get(name) if info is None: raise KeyError( &apos;There is no item named %r in the archive&apos; % name) return info def setpassword(self, pwd): &quot;&quot;&quot;Set default password for encrypted files.&quot;&quot;&quot; self.pwd = pwd @property def comment(self): &quot;&quot;&quot;The comment text associated with the ZIP file.&quot;&quot;&quot; return self._comment @comment.setter def comment(self, comment): # check for valid comment length if len(comment) &gt; ZIP_MAX_COMMENT: import warnings warnings.warn(&apos;Archive comment is too long; truncating to %d bytes&apos; % ZIP_MAX_COMMENT, stacklevel=2) comment = comment[:ZIP_MAX_COMMENT] self._comment = comment self._didModify = True def read(self, name, pwd=None): &quot;&quot;&quot;Return file bytes (as a string) for name.&quot;&quot;&quot; return self.open(name, &quot;r&quot;, pwd).read() def open(self, name, mode=&quot;r&quot;, pwd=None): &quot;&quot;&quot;Return file-like object for &apos;name&apos;.&quot;&quot;&quot; if mode not in (&quot;r&quot;, &quot;U&quot;, &quot;rU&quot;): raise RuntimeError, &apos;open() requires mode &quot;r&quot;, &quot;U&quot;, or &quot;rU&quot;&apos; if not self.fp: raise RuntimeError, \ &quot;Attempt to read ZIP archive that was already closed&quot; # Only open a new file for instances where we were not # given a file object in the constructor if self._filePassed: zef_file = self.fp should_close = False else: zef_file = open(self.filename, &apos;rb&apos;) should_close = True try: # Make sure we have an info object if isinstance(name, ZipInfo): # &apos;name&apos; is already an info object zinfo = name else: # Get info object for name zinfo = self.getinfo(name) zef_file.seek(zinfo.header_offset, 0) # Skip the file header: fheader = zef_file.read(sizeFileHeader) if len(fheader) != sizeFileHeader: raise BadZipfile(&quot;Truncated file header&quot;) fheader = struct.unpack(structFileHeader, fheader) if fheader[_FH_SIGNATURE] != stringFileHeader: raise BadZipfile(&quot;Bad magic number for file header&quot;) fname = zef_file.read(fheader[_FH_FILENAME_LENGTH]) if fheader[_FH_EXTRA_FIELD_LENGTH]: zef_file.read(fheader[_FH_EXTRA_FIELD_LENGTH]) if fname != zinfo.orig_filename: raise BadZipfile, \ &apos;File name in directory &quot;%s&quot; and header &quot;%s&quot; differ.&apos; % ( zinfo.orig_filename, fname) # check for encrypted flag &amp; handle password is_encrypted = zinfo.flag_bits &amp; 0x1 zd = None if is_encrypted: if not pwd: pwd = self.pwd if not pwd: raise RuntimeError, &quot;File %s is encrypted, &quot; \ &quot;password required for extraction&quot; % name zd = _ZipDecrypter(pwd) # The first 12 bytes in the cypher stream is an encryption header # used to strengthen the algorithm. The first 11 bytes are # completely random, while the 12th contains the MSB of the CRC, # or the MSB of the file time depending on the header type # and is used to check the correctness of the password. bytes = zef_file.read(12) h = map(zd, bytes[0:12]) if zinfo.flag_bits &amp; 0x8: # compare against the file type from extended local headers check_byte = (zinfo._raw_time &gt;&gt; 8) &amp; 0xff else: # compare against the CRC otherwise check_byte = (zinfo.CRC &gt;&gt; 24) &amp; 0xff if ord(h[11]) != check_byte: raise RuntimeError(&quot;Bad password for file&quot;, name) return ZipExtFile(zef_file, mode, zinfo, zd, close_fileobj=should_close) except: if should_close: zef_file.close() raise def extract(self, member, path=None, pwd=None): &quot;&quot;&quot;Extract a member from the archive to the current working directory, using its full name. Its file information is extracted as accurately as possible. `member&apos; may be a filename or a ZipInfo object. You can specify a different directory using `path&apos;. &quot;&quot;&quot; if not isinstance(member, ZipInfo): member = self.getinfo(member) if path is None: path = os.getcwd() return self._extract_member(member, path, pwd) def extractall(self, path=None, members=None, pwd=None): &quot;&quot;&quot;Extract all members from the archive to the current working directory. `path&apos; specifies a different directory to extract to. `members&apos; is optional and must be a subset of the list returned by namelist(). &quot;&quot;&quot; if members is None: members = self.namelist() for zipinfo in members: self.extract(zipinfo, path, pwd) def _extract_member(self, member, targetpath, pwd): &quot;&quot;&quot;Extract the ZipInfo object &apos;member&apos; to a physical file on the path targetpath. &quot;&quot;&quot; # build the destination pathname, replacing # forward slashes to platform specific separators. arcname = member.filename.replace(&apos;/&apos;, os.path.sep) if os.path.altsep: arcname = arcname.replace(os.path.altsep, os.path.sep) # interpret absolute pathname as relative, remove drive letter or # UNC path, redundant separators, &quot;.&quot; and &quot;..&quot; components. arcname = os.path.splitdrive(arcname)[1] arcname = os.path.sep.join(x for x in arcname.split(os.path.sep) if x not in (&apos;&apos;, os.path.curdir, os.path.pardir)) if os.path.sep == &apos;\\&apos;: # filter illegal characters on Windows illegal = &apos;:&lt;&gt;|&quot;?*&apos; if isinstance(arcname, unicode): table = &#123;ord(c): ord(&apos;_&apos;) for c in illegal&#125; else: table = string.maketrans(illegal, &apos;_&apos; * len(illegal)) arcname = arcname.translate(table) # remove trailing dots arcname = (x.rstrip(&apos;.&apos;) for x in arcname.split(os.path.sep)) arcname = os.path.sep.join(x for x in arcname if x) targetpath = os.path.join(targetpath, arcname) targetpath = os.path.normpath(targetpath) # Create all upper directories if necessary. upperdirs = os.path.dirname(targetpath) if upperdirs and not os.path.exists(upperdirs): os.makedirs(upperdirs) if member.filename[-1] == &apos;/&apos;: if not os.path.isdir(targetpath): os.mkdir(targetpath) return targetpath with self.open(member, pwd=pwd) as source, \ file(targetpath, &quot;wb&quot;) as target: shutil.copyfileobj(source, target) return targetpath def _writecheck(self, zinfo): &quot;&quot;&quot;Check for errors before writing a file to the archive.&quot;&quot;&quot; if zinfo.filename in self.NameToInfo: import warnings warnings.warn(&apos;Duplicate name: %r&apos; % zinfo.filename, stacklevel=3) if self.mode not in (&quot;w&quot;, &quot;a&quot;): raise RuntimeError, &apos;write() requires mode &quot;w&quot; or &quot;a&quot;&apos; if not self.fp: raise RuntimeError, \ &quot;Attempt to write ZIP archive that was already closed&quot; if zinfo.compress_type == ZIP_DEFLATED and not zlib: raise RuntimeError, \ &quot;Compression requires the (missing) zlib module&quot; if zinfo.compress_type not in (ZIP_STORED, ZIP_DEFLATED): raise RuntimeError, \ &quot;That compression method is not supported&quot; if not self._allowZip64: requires_zip64 = None if len(self.filelist) &gt;= ZIP_FILECOUNT_LIMIT: requires_zip64 = &quot;Files count&quot; elif zinfo.file_size &gt; ZIP64_LIMIT: requires_zip64 = &quot;Filesize&quot; elif zinfo.header_offset &gt; ZIP64_LIMIT: requires_zip64 = &quot;Zipfile size&quot; if requires_zip64: raise LargeZipFile(requires_zip64 + &quot; would require ZIP64 extensions&quot;) def write(self, filename, arcname=None, compress_type=None): &quot;&quot;&quot;Put the bytes from filename into the archive under the name arcname.&quot;&quot;&quot; if not self.fp: raise RuntimeError( &quot;Attempt to write to ZIP archive that was already closed&quot;) st = os.stat(filename) isdir = stat.S_ISDIR(st.st_mode) mtime = time.localtime(st.st_mtime) date_time = mtime[0:6] # Create ZipInfo instance to store file information if arcname is None: arcname = filename arcname = os.path.normpath(os.path.splitdrive(arcname)[1]) while arcname[0] in (os.sep, os.altsep): arcname = arcname[1:] if isdir: arcname += &apos;/&apos; zinfo = ZipInfo(arcname, date_time) zinfo.external_attr = (st[0] &amp; 0xFFFF) &lt;&lt; 16L # Unix attributes if compress_type is None: zinfo.compress_type = self.compression else: zinfo.compress_type = compress_type zinfo.file_size = st.st_size zinfo.flag_bits = 0x00 zinfo.header_offset = self.fp.tell() # Start of header bytes self._writecheck(zinfo) self._didModify = True if isdir: zinfo.file_size = 0 zinfo.compress_size = 0 zinfo.CRC = 0 zinfo.external_attr |= 0x10 # MS-DOS directory flag self.filelist.append(zinfo) self.NameToInfo[zinfo.filename] = zinfo self.fp.write(zinfo.FileHeader(False)) return with open(filename, &quot;rb&quot;) as fp: # Must overwrite CRC and sizes with correct data later zinfo.CRC = CRC = 0 zinfo.compress_size = compress_size = 0 # Compressed size can be larger than uncompressed size zip64 = self._allowZip64 and \ zinfo.file_size * 1.05 &gt; ZIP64_LIMIT self.fp.write(zinfo.FileHeader(zip64)) if zinfo.compress_type == ZIP_DEFLATED: cmpr = zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION, zlib.DEFLATED, -15) else: cmpr = None file_size = 0 while 1: buf = fp.read(1024 * 8) if not buf: break file_size = file_size + len(buf) CRC = crc32(buf, CRC) &amp; 0xffffffff if cmpr: buf = cmpr.compress(buf) compress_size = compress_size + len(buf) self.fp.write(buf) if cmpr: buf = cmpr.flush() compress_size = compress_size + len(buf) self.fp.write(buf) zinfo.compress_size = compress_size else: zinfo.compress_size = file_size zinfo.CRC = CRC zinfo.file_size = file_size if not zip64 and self._allowZip64: if file_size &gt; ZIP64_LIMIT: raise RuntimeError(&apos;File size has increased during compressing&apos;) if compress_size &gt; ZIP64_LIMIT: raise RuntimeError(&apos;Compressed size larger than uncompressed size&apos;) # Seek backwards and write file header (which will now include # correct CRC and file sizes) position = self.fp.tell() # Preserve current position in file self.fp.seek(zinfo.header_offset, 0) self.fp.write(zinfo.FileHeader(zip64)) self.fp.seek(position, 0) self.filelist.append(zinfo) self.NameToInfo[zinfo.filename] = zinfo def writestr(self, zinfo_or_arcname, bytes, compress_type=None): &quot;&quot;&quot;Write a file into the archive. The contents is the string &apos;bytes&apos;. &apos;zinfo_or_arcname&apos; is either a ZipInfo instance or the name of the file in the archive.&quot;&quot;&quot; if not isinstance(zinfo_or_arcname, ZipInfo): zinfo = ZipInfo(filename=zinfo_or_arcname, date_time=time.localtime(time.time())[:6]) zinfo.compress_type = self.compression if zinfo.filename[-1] == &apos;/&apos;: zinfo.external_attr = 0o40775 &lt;&lt; 16 # drwxrwxr-x zinfo.external_attr |= 0x10 # MS-DOS directory flag else: zinfo.external_attr = 0o600 &lt;&lt; 16 # ?rw------- else: zinfo = zinfo_or_arcname if not self.fp: raise RuntimeError( &quot;Attempt to write to ZIP archive that was already closed&quot;) if compress_type is not None: zinfo.compress_type = compress_type zinfo.file_size = len(bytes) # Uncompressed size zinfo.header_offset = self.fp.tell() # Start of header bytes self._writecheck(zinfo) self._didModify = True zinfo.CRC = crc32(bytes) &amp; 0xffffffff # CRC-32 checksum if zinfo.compress_type == ZIP_DEFLATED: co = zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION, zlib.DEFLATED, -15) bytes = co.compress(bytes) + co.flush() zinfo.compress_size = len(bytes) # Compressed size else: zinfo.compress_size = zinfo.file_size zip64 = zinfo.file_size &gt; ZIP64_LIMIT or \ zinfo.compress_size &gt; ZIP64_LIMIT if zip64 and not self._allowZip64: raise LargeZipFile(&quot;Filesize would require ZIP64 extensions&quot;) self.fp.write(zinfo.FileHeader(zip64)) self.fp.write(bytes) if zinfo.flag_bits &amp; 0x08: # Write CRC and file sizes after the file data fmt = &apos;&lt;LQQ&apos; if zip64 else &apos;&lt;LLL&apos; self.fp.write(struct.pack(fmt, zinfo.CRC, zinfo.compress_size, zinfo.file_size)) self.fp.flush() self.filelist.append(zinfo) self.NameToInfo[zinfo.filename] = zinfo def __del__(self): &quot;&quot;&quot;Call the &quot;close()&quot; method in case the user forgot.&quot;&quot;&quot; self.close() def close(self): &quot;&quot;&quot;Close the file, and for mode &quot;w&quot; and &quot;a&quot; write the ending records.&quot;&quot;&quot; if self.fp is None: return try: if self.mode in (&quot;w&quot;, &quot;a&quot;) and self._didModify: # write ending records pos1 = self.fp.tell() for zinfo in self.filelist: # write central directory dt = zinfo.date_time dosdate = (dt[0] - 1980) &lt;&lt; 9 | dt[1] &lt;&lt; 5 | dt[2] dostime = dt[3] &lt;&lt; 11 | dt[4] &lt;&lt; 5 | (dt[5] // 2) extra = [] if zinfo.file_size &gt; ZIP64_LIMIT \ or zinfo.compress_size &gt; ZIP64_LIMIT: extra.append(zinfo.file_size) extra.append(zinfo.compress_size) file_size = 0xffffffff compress_size = 0xffffffff else: file_size = zinfo.file_size compress_size = zinfo.compress_size if zinfo.header_offset &gt; ZIP64_LIMIT: extra.append(zinfo.header_offset) header_offset = 0xffffffffL else: header_offset = zinfo.header_offset extra_data = zinfo.extra if extra: # Append a ZIP64 field to the extra&apos;s extra_data = struct.pack( &apos;&lt;HH&apos; + &apos;Q&apos;*len(extra), 1, 8*len(extra), *extra) + extra_data extract_version = max(45, zinfo.extract_version) create_version = max(45, zinfo.create_version) else: extract_version = zinfo.extract_version create_version = zinfo.create_version try: filename, flag_bits = zinfo._encodeFilenameFlags() centdir = struct.pack(structCentralDir, stringCentralDir, create_version, zinfo.create_system, extract_version, zinfo.reserved, flag_bits, zinfo.compress_type, dostime, dosdate, zinfo.CRC, compress_size, file_size, len(filename), len(extra_data), len(zinfo.comment), 0, zinfo.internal_attr, zinfo.external_attr, header_offset) except DeprecationWarning: print &gt;&gt;sys.stderr, (structCentralDir, stringCentralDir, create_version, zinfo.create_system, extract_version, zinfo.reserved, zinfo.flag_bits, zinfo.compress_type, dostime, dosdate, zinfo.CRC, compress_size, file_size, len(zinfo.filename), len(extra_data), len(zinfo.comment), 0, zinfo.internal_attr, zinfo.external_attr, header_offset) raise self.fp.write(centdir) self.fp.write(filename) self.fp.write(extra_data) self.fp.write(zinfo.comment) pos2 = self.fp.tell() # Write end-of-zip-archive record centDirCount = len(self.filelist) centDirSize = pos2 - pos1 centDirOffset = pos1 requires_zip64 = None if centDirCount &gt; ZIP_FILECOUNT_LIMIT: requires_zip64 = &quot;Files count&quot; elif centDirOffset &gt; ZIP64_LIMIT: requires_zip64 = &quot;Central directory offset&quot; elif centDirSize &gt; ZIP64_LIMIT: requires_zip64 = &quot;Central directory size&quot; if requires_zip64: # Need to write the ZIP64 end-of-archive records if not self._allowZip64: raise LargeZipFile(requires_zip64 + &quot; would require ZIP64 extensions&quot;) zip64endrec = struct.pack( structEndArchive64, stringEndArchive64, 44, 45, 45, 0, 0, centDirCount, centDirCount, centDirSize, centDirOffset) self.fp.write(zip64endrec) zip64locrec = struct.pack( structEndArchive64Locator, stringEndArchive64Locator, 0, pos2, 1) self.fp.write(zip64locrec) centDirCount = min(centDirCount, 0xFFFF) centDirSize = min(centDirSize, 0xFFFFFFFF) centDirOffset = min(centDirOffset, 0xFFFFFFFF) endrec = struct.pack(structEndArchive, stringEndArchive, 0, 0, centDirCount, centDirCount, centDirSize, centDirOffset, len(self._comment)) self.fp.write(endrec) self.fp.write(self._comment) self.fp.flush() finally: fp = self.fp self.fp = None if not self._filePassed: fp.close() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985TarFileclass TarFile(object): &quot;&quot;&quot;The TarFile Class provides an interface to tar archives. &quot;&quot;&quot; debug = 0 # May be set from 0 (no msgs) to 3 (all msgs) dereference = False # If true, add content of linked file to the # tar file, else the link. ignore_zeros = False # If true, skips empty or invalid blocks and # continues processing. errorlevel = 1 # If 0, fatal errors only appear in debug # messages (if debug &gt;= 0). If &gt; 0, errors # are passed to the caller as exceptions. format = DEFAULT_FORMAT # The format to use when creating an archive. encoding = ENCODING # Encoding for 8-bit character strings. errors = None # Error handler for unicode conversion. tarinfo = TarInfo # The default TarInfo class to use. fileobject = ExFileObject # The default ExFileObject class to use. def __init__(self, name=None, mode=&quot;r&quot;, fileobj=None, format=None, tarinfo=None, dereference=None, ignore_zeros=None, encoding=None, errors=None, pax_headers=None, debug=None, errorlevel=None): &quot;&quot;&quot;Open an (uncompressed) tar archive `name&apos;. `mode&apos; is either &apos;r&apos; to read from an existing archive, &apos;a&apos; to append data to an existing file or &apos;w&apos; to create a new file overwriting an existing one. `mode&apos; defaults to &apos;r&apos;. If `fileobj&apos; is given, it is used for reading or writing data. If it can be determined, `mode&apos; is overridden by `fileobj&apos;s mode. `fileobj&apos; is not closed, when TarFile is closed. &quot;&quot;&quot; modes = &#123;&quot;r&quot;: &quot;rb&quot;, &quot;a&quot;: &quot;r+b&quot;, &quot;w&quot;: &quot;wb&quot;&#125; if mode not in modes: raise ValueError(&quot;mode must be &apos;r&apos;, &apos;a&apos; or &apos;w&apos;&quot;) self.mode = mode self._mode = modes[mode] if not fileobj: if self.mode == &quot;a&quot; and not os.path.exists(name): # Create nonexistent files in append mode. self.mode = &quot;w&quot; self._mode = &quot;wb&quot; fileobj = bltn_open(name, self._mode) self._extfileobj = False else: if name is None and hasattr(fileobj, &quot;name&quot;): name = fileobj.name if hasattr(fileobj, &quot;mode&quot;): self._mode = fileobj.mode self._extfileobj = True self.name = os.path.abspath(name) if name else None self.fileobj = fileobj # Init attributes. if format is not None: self.format = format if tarinfo is not None: self.tarinfo = tarinfo if dereference is not None: self.dereference = dereference if ignore_zeros is not None: self.ignore_zeros = ignore_zeros if encoding is not None: self.encoding = encoding if errors is not None: self.errors = errors elif mode == &quot;r&quot;: self.errors = &quot;utf-8&quot; else: self.errors = &quot;strict&quot; if pax_headers is not None and self.format == PAX_FORMAT: self.pax_headers = pax_headers else: self.pax_headers = &#123;&#125; if debug is not None: self.debug = debug if errorlevel is not None: self.errorlevel = errorlevel # Init datastructures. self.closed = False self.members = [] # list of members as TarInfo objects self._loaded = False # flag if all members have been read self.offset = self.fileobj.tell() # current position in the archive file self.inodes = &#123;&#125; # dictionary caching the inodes of # archive members already added try: if self.mode == &quot;r&quot;: self.firstmember = None self.firstmember = self.next() if self.mode == &quot;a&quot;: # Move to the end of the archive, # before the first empty block. while True: self.fileobj.seek(self.offset) try: tarinfo = self.tarinfo.fromtarfile(self) self.members.append(tarinfo) except EOFHeaderError: self.fileobj.seek(self.offset) break except HeaderError, e: raise ReadError(str(e)) if self.mode in &quot;aw&quot;: self._loaded = True if self.pax_headers: buf = self.tarinfo.create_pax_global_header(self.pax_headers.copy()) self.fileobj.write(buf) self.offset += len(buf) except: if not self._extfileobj: self.fileobj.close() self.closed = True raise def _getposix(self): return self.format == USTAR_FORMAT def _setposix(self, value): import warnings warnings.warn(&quot;use the format attribute instead&quot;, DeprecationWarning, 2) if value: self.format = USTAR_FORMAT else: self.format = GNU_FORMAT posix = property(_getposix, _setposix) #-------------------------------------------------------------------------- # Below are the classmethods which act as alternate constructors to the # TarFile class. The open() method is the only one that is needed for # public use; it is the &quot;super&quot;-constructor and is able to select an # adequate &quot;sub&quot;-constructor for a particular compression using the mapping # from OPEN_METH. # # This concept allows one to subclass TarFile without losing the comfort of # the super-constructor. A sub-constructor is registered and made available # by adding it to the mapping in OPEN_METH. @classmethod def open(cls, name=None, mode=&quot;r&quot;, fileobj=None, bufsize=RECORDSIZE, **kwargs): &quot;&quot;&quot;Open a tar archive for reading, writing or appending. Return an appropriate TarFile class. mode: &apos;r&apos; or &apos;r:*&apos; open for reading with transparent compression &apos;r:&apos; open for reading exclusively uncompressed &apos;r:gz&apos; open for reading with gzip compression &apos;r:bz2&apos; open for reading with bzip2 compression &apos;a&apos; or &apos;a:&apos; open for appending, creating the file if necessary &apos;w&apos; or &apos;w:&apos; open for writing without compression &apos;w:gz&apos; open for writing with gzip compression &apos;w:bz2&apos; open for writing with bzip2 compression &apos;r|*&apos; open a stream of tar blocks with transparent compression &apos;r|&apos; open an uncompressed stream of tar blocks for reading &apos;r|gz&apos; open a gzip compressed stream of tar blocks &apos;r|bz2&apos; open a bzip2 compressed stream of tar blocks &apos;w|&apos; open an uncompressed stream for writing &apos;w|gz&apos; open a gzip compressed stream for writing &apos;w|bz2&apos; open a bzip2 compressed stream for writing &quot;&quot;&quot; if not name and not fileobj: raise ValueError(&quot;nothing to open&quot;) if mode in (&quot;r&quot;, &quot;r:*&quot;): # Find out which *open() is appropriate for opening the file. for comptype in cls.OPEN_METH: func = getattr(cls, cls.OPEN_METH[comptype]) if fileobj is not None: saved_pos = fileobj.tell() try: return func(name, &quot;r&quot;, fileobj, **kwargs) except (ReadError, CompressionError), e: if fileobj is not None: fileobj.seek(saved_pos) continue raise ReadError(&quot;file could not be opened successfully&quot;) elif &quot;:&quot; in mode: filemode, comptype = mode.split(&quot;:&quot;, 1) filemode = filemode or &quot;r&quot; comptype = comptype or &quot;tar&quot; # Select the *open() function according to # given compression. if comptype in cls.OPEN_METH: func = getattr(cls, cls.OPEN_METH[comptype]) else: raise CompressionError(&quot;unknown compression type %r&quot; % comptype) return func(name, filemode, fileobj, **kwargs) elif &quot;|&quot; in mode: filemode, comptype = mode.split(&quot;|&quot;, 1) filemode = filemode or &quot;r&quot; comptype = comptype or &quot;tar&quot; if filemode not in (&quot;r&quot;, &quot;w&quot;): raise ValueError(&quot;mode must be &apos;r&apos; or &apos;w&apos;&quot;) stream = _Stream(name, filemode, comptype, fileobj, bufsize) try: t = cls(name, filemode, stream, **kwargs) except: stream.close() raise t._extfileobj = False return t elif mode in (&quot;a&quot;, &quot;w&quot;): return cls.taropen(name, mode, fileobj, **kwargs) raise ValueError(&quot;undiscernible mode&quot;) @classmethod def taropen(cls, name, mode=&quot;r&quot;, fileobj=None, **kwargs): &quot;&quot;&quot;Open uncompressed tar archive name for reading or writing. &quot;&quot;&quot; if mode not in (&quot;r&quot;, &quot;a&quot;, &quot;w&quot;): raise ValueError(&quot;mode must be &apos;r&apos;, &apos;a&apos; or &apos;w&apos;&quot;) return cls(name, mode, fileobj, **kwargs) @classmethod def gzopen(cls, name, mode=&quot;r&quot;, fileobj=None, compresslevel=9, **kwargs): &quot;&quot;&quot;Open gzip compressed tar archive name for reading or writing. Appending is not allowed. &quot;&quot;&quot; if mode not in (&quot;r&quot;, &quot;w&quot;): raise ValueError(&quot;mode must be &apos;r&apos; or &apos;w&apos;&quot;) try: import gzip gzip.GzipFile except (ImportError, AttributeError): raise CompressionError(&quot;gzip module is not available&quot;) try: fileobj = gzip.GzipFile(name, mode, compresslevel, fileobj) except OSError: if fileobj is not None and mode == &apos;r&apos;: raise ReadError(&quot;not a gzip file&quot;) raise try: t = cls.taropen(name, mode, fileobj, **kwargs) except IOError: fileobj.close() if mode == &apos;r&apos;: raise ReadError(&quot;not a gzip file&quot;) raise except: fileobj.close() raise t._extfileobj = False return t @classmethod def bz2open(cls, name, mode=&quot;r&quot;, fileobj=None, compresslevel=9, **kwargs): &quot;&quot;&quot;Open bzip2 compressed tar archive name for reading or writing. Appending is not allowed. &quot;&quot;&quot; if mode not in (&quot;r&quot;, &quot;w&quot;): raise ValueError(&quot;mode must be &apos;r&apos; or &apos;w&apos;.&quot;) try: import bz2 except ImportError: raise CompressionError(&quot;bz2 module is not available&quot;) if fileobj is not None: fileobj = _BZ2Proxy(fileobj, mode) else: fileobj = bz2.BZ2File(name, mode, compresslevel=compresslevel) try: t = cls.taropen(name, mode, fileobj, **kwargs) except (IOError, EOFError): fileobj.close() if mode == &apos;r&apos;: raise ReadError(&quot;not a bzip2 file&quot;) raise except: fileobj.close() raise t._extfileobj = False return t # All *open() methods are registered here. OPEN_METH = &#123; &quot;tar&quot;: &quot;taropen&quot;, # uncompressed tar &quot;gz&quot;: &quot;gzopen&quot;, # gzip compressed tar &quot;bz2&quot;: &quot;bz2open&quot; # bzip2 compressed tar &#125; #-------------------------------------------------------------------------- # The public methods which TarFile provides: def close(self): &quot;&quot;&quot;Close the TarFile. In write-mode, two finishing zero blocks are appended to the archive. &quot;&quot;&quot; if self.closed: return if self.mode in &quot;aw&quot;: self.fileobj.write(NUL * (BLOCKSIZE * 2)) self.offset += (BLOCKSIZE * 2) # fill up the end with zero-blocks # (like option -b20 for tar does) blocks, remainder = divmod(self.offset, RECORDSIZE) if remainder &gt; 0: self.fileobj.write(NUL * (RECORDSIZE - remainder)) if not self._extfileobj: self.fileobj.close() self.closed = True def getmember(self, name): &quot;&quot;&quot;Return a TarInfo object for member `name&apos;. If `name&apos; can not be found in the archive, KeyError is raised. If a member occurs more than once in the archive, its last occurrence is assumed to be the most up-to-date version. &quot;&quot;&quot; tarinfo = self._getmember(name) if tarinfo is None: raise KeyError(&quot;filename %r not found&quot; % name) return tarinfo def getmembers(self): &quot;&quot;&quot;Return the members of the archive as a list of TarInfo objects. The list has the same order as the members in the archive. &quot;&quot;&quot; self._check() if not self._loaded: # if we want to obtain a list of self._load() # all members, we first have to # scan the whole archive. return self.members def getnames(self): &quot;&quot;&quot;Return the members of the archive as a list of their names. It has the same order as the list returned by getmembers(). &quot;&quot;&quot; return [tarinfo.name for tarinfo in self.getmembers()] def gettarinfo(self, name=None, arcname=None, fileobj=None): &quot;&quot;&quot;Create a TarInfo object for either the file `name&apos; or the file object `fileobj&apos; (using os.fstat on its file descriptor). You can modify some of the TarInfo&apos;s attributes before you add it using addfile(). If given, `arcname&apos; specifies an alternative name for the file in the archive. &quot;&quot;&quot; self._check(&quot;aw&quot;) # When fileobj is given, replace name by # fileobj&apos;s real name. if fileobj is not None: name = fileobj.name # Building the name of the member in the archive. # Backward slashes are converted to forward slashes, # Absolute paths are turned to relative paths. if arcname is None: arcname = name drv, arcname = os.path.splitdrive(arcname) arcname = arcname.replace(os.sep, &quot;/&quot;) arcname = arcname.lstrip(&quot;/&quot;) # Now, fill the TarInfo object with # information specific for the file. tarinfo = self.tarinfo() tarinfo.tarfile = self # Use os.stat or os.lstat, depending on platform # and if symlinks shall be resolved. if fileobj is None: if hasattr(os, &quot;lstat&quot;) and not self.dereference: statres = os.lstat(name) else: statres = os.stat(name) else: statres = os.fstat(fileobj.fileno()) linkname = &quot;&quot; stmd = statres.st_mode if stat.S_ISREG(stmd): inode = (statres.st_ino, statres.st_dev) if not self.dereference and statres.st_nlink &gt; 1 and \ inode in self.inodes and arcname != self.inodes[inode]: # Is it a hardlink to an already # archived file? type = LNKTYPE linkname = self.inodes[inode] else: # The inode is added only if its valid. # For win32 it is always 0. type = REGTYPE if inode[0]: self.inodes[inode] = arcname elif stat.S_ISDIR(stmd): type = DIRTYPE elif stat.S_ISFIFO(stmd): type = FIFOTYPE elif stat.S_ISLNK(stmd): type = SYMTYPE linkname = os.readlink(name) elif stat.S_ISCHR(stmd): type = CHRTYPE elif stat.S_ISBLK(stmd): type = BLKTYPE else: return None # Fill the TarInfo object with all # information we can get. tarinfo.name = arcname tarinfo.mode = stmd tarinfo.uid = statres.st_uid tarinfo.gid = statres.st_gid if type == REGTYPE: tarinfo.size = statres.st_size else: tarinfo.size = 0L tarinfo.mtime = statres.st_mtime tarinfo.type = type tarinfo.linkname = linkname if pwd: try: tarinfo.uname = pwd.getpwuid(tarinfo.uid)[0] except KeyError: pass if grp: try: tarinfo.gname = grp.getgrgid(tarinfo.gid)[0] except KeyError: pass if type in (CHRTYPE, BLKTYPE): if hasattr(os, &quot;major&quot;) and hasattr(os, &quot;minor&quot;): tarinfo.devmajor = os.major(statres.st_rdev) tarinfo.devminor = os.minor(statres.st_rdev) return tarinfo def list(self, verbose=True): &quot;&quot;&quot;Print a table of contents to sys.stdout. If `verbose&apos; is False, only the names of the members are printed. If it is True, an `ls -l&apos;-like output is produced. &quot;&quot;&quot; self._check() for tarinfo in self: if verbose: print filemode(tarinfo.mode), print &quot;%s/%s&quot; % (tarinfo.uname or tarinfo.uid, tarinfo.gname or tarinfo.gid), if tarinfo.ischr() or tarinfo.isblk(): print &quot;%10s&quot; % (&quot;%d,%d&quot; \ % (tarinfo.devmajor, tarinfo.devminor)), else: print &quot;%10d&quot; % tarinfo.size, print &quot;%d-%02d-%02d %02d:%02d:%02d&quot; \ % time.localtime(tarinfo.mtime)[:6], print tarinfo.name + (&quot;/&quot; if tarinfo.isdir() else &quot;&quot;), if verbose: if tarinfo.issym(): print &quot;-&gt;&quot;, tarinfo.linkname, if tarinfo.islnk(): print &quot;link to&quot;, tarinfo.linkname, print def add(self, name, arcname=None, recursive=True, exclude=None, filter=None): &quot;&quot;&quot;Add the file `name&apos; to the archive. `name&apos; may be any type of file (directory, fifo, symbolic link, etc.). If given, `arcname&apos; specifies an alternative name for the file in the archive. Directories are added recursively by default. This can be avoided by setting `recursive&apos; to False. `exclude&apos; is a function that should return True for each filename to be excluded. `filter&apos; is a function that expects a TarInfo object argument and returns the changed TarInfo object, if it returns None the TarInfo object will be excluded from the archive. &quot;&quot;&quot; self._check(&quot;aw&quot;) if arcname is None: arcname = name # Exclude pathnames. if exclude is not None: import warnings warnings.warn(&quot;use the filter argument instead&quot;, DeprecationWarning, 2) if exclude(name): self._dbg(2, &quot;tarfile: Excluded %r&quot; % name) return # Skip if somebody tries to archive the archive... if self.name is not None and os.path.abspath(name) == self.name: self._dbg(2, &quot;tarfile: Skipped %r&quot; % name) return self._dbg(1, name) # Create a TarInfo object from the file. tarinfo = self.gettarinfo(name, arcname) if tarinfo is None: self._dbg(1, &quot;tarfile: Unsupported type %r&quot; % name) return # Change or exclude the TarInfo object. if filter is not None: tarinfo = filter(tarinfo) if tarinfo is None: self._dbg(2, &quot;tarfile: Excluded %r&quot; % name) return # Append the tar header and data to the archive. if tarinfo.isreg(): with bltn_open(name, &quot;rb&quot;) as f: self.addfile(tarinfo, f) elif tarinfo.isdir(): self.addfile(tarinfo) if recursive: for f in os.listdir(name): self.add(os.path.join(name, f), os.path.join(arcname, f), recursive, exclude, filter) else: self.addfile(tarinfo) def addfile(self, tarinfo, fileobj=None): &quot;&quot;&quot;Add the TarInfo object `tarinfo&apos; to the archive. If `fileobj&apos; is given, tarinfo.size bytes are read from it and added to the archive. You can create TarInfo objects using gettarinfo(). On Windows platforms, `fileobj&apos; should always be opened with mode &apos;rb&apos; to avoid irritation about the file size. &quot;&quot;&quot; self._check(&quot;aw&quot;) tarinfo = copy.copy(tarinfo) buf = tarinfo.tobuf(self.format, self.encoding, self.errors) self.fileobj.write(buf) self.offset += len(buf) # If there&apos;s data to follow, append it. if fileobj is not None: copyfileobj(fileobj, self.fileobj, tarinfo.size) blocks, remainder = divmod(tarinfo.size, BLOCKSIZE) if remainder &gt; 0: self.fileobj.write(NUL * (BLOCKSIZE - remainder)) blocks += 1 self.offset += blocks * BLOCKSIZE self.members.append(tarinfo) def extractall(self, path=&quot;.&quot;, members=None): &quot;&quot;&quot;Extract all members from the archive to the current working directory and set owner, modification time and permissions on directories afterwards. `path&apos; specifies a different directory to extract to. `members&apos; is optional and must be a subset of the list returned by getmembers(). &quot;&quot;&quot; directories = [] if members is None: members = self for tarinfo in members: if tarinfo.isdir(): # Extract directories with a safe mode. directories.append(tarinfo) tarinfo = copy.copy(tarinfo) tarinfo.mode = 0700 self.extract(tarinfo, path) # Reverse sort directories. directories.sort(key=operator.attrgetter(&apos;name&apos;)) directories.reverse() # Set correct owner, mtime and filemode on directories. for tarinfo in directories: dirpath = os.path.join(path, tarinfo.name) try: self.chown(tarinfo, dirpath) self.utime(tarinfo, dirpath) self.chmod(tarinfo, dirpath) except ExtractError, e: if self.errorlevel &gt; 1: raise else: self._dbg(1, &quot;tarfile: %s&quot; % e) def extract(self, member, path=&quot;&quot;): &quot;&quot;&quot;Extract a member from the archive to the current working directory, using its full name. Its file information is extracted as accurately as possible. `member&apos; may be a filename or a TarInfo object. You can specify a different directory using `path&apos;. &quot;&quot;&quot; self._check(&quot;r&quot;) if isinstance(member, basestring): tarinfo = self.getmember(member) else: tarinfo = member # Prepare the link target for makelink(). if tarinfo.islnk(): tarinfo._link_target = os.path.join(path, tarinfo.linkname) try: self._extract_member(tarinfo, os.path.join(path, tarinfo.name)) except EnvironmentError, e: if self.errorlevel &gt; 0: raise else: if e.filename is None: self._dbg(1, &quot;tarfile: %s&quot; % e.strerror) else: self._dbg(1, &quot;tarfile: %s %r&quot; % (e.strerror, e.filename)) except ExtractError, e: if self.errorlevel &gt; 1: raise else: self._dbg(1, &quot;tarfile: %s&quot; % e) def extractfile(self, member): &quot;&quot;&quot;Extract a member from the archive as a file object. `member&apos; may be a filename or a TarInfo object. If `member&apos; is a regular file, a file-like object is returned. If `member&apos; is a link, a file-like object is constructed from the link&apos;s target. If `member&apos; is none of the above, None is returned. The file-like object is read-only and provides the following methods: read(), readline(), readlines(), seek() and tell() &quot;&quot;&quot; self._check(&quot;r&quot;) if isinstance(member, basestring): tarinfo = self.getmember(member) else: tarinfo = member if tarinfo.isreg(): return self.fileobject(self, tarinfo) elif tarinfo.type not in SUPPORTED_TYPES: # If a member&apos;s type is unknown, it is treated as a # regular file. return self.fileobject(self, tarinfo) elif tarinfo.islnk() or tarinfo.issym(): if isinstance(self.fileobj, _Stream): # A small but ugly workaround for the case that someone tries # to extract a (sym)link as a file-object from a non-seekable # stream of tar blocks. raise StreamError(&quot;cannot extract (sym)link as file object&quot;) else: # A (sym)link&apos;s file object is its target&apos;s file object. return self.extractfile(self._find_link_target(tarinfo)) else: # If there&apos;s no data associated with the member (directory, chrdev, # blkdev, etc.), return None instead of a file object. return None def _extract_member(self, tarinfo, targetpath): &quot;&quot;&quot;Extract the TarInfo object tarinfo to a physical file called targetpath. &quot;&quot;&quot; # Fetch the TarInfo object for the given name # and build the destination pathname, replacing # forward slashes to platform specific separators. targetpath = targetpath.rstrip(&quot;/&quot;) targetpath = targetpath.replace(&quot;/&quot;, os.sep) # Create all upper directories. upperdirs = os.path.dirname(targetpath) if upperdirs and not os.path.exists(upperdirs): # Create directories that are not part of the archive with # default permissions. os.makedirs(upperdirs) if tarinfo.islnk() or tarinfo.issym(): self._dbg(1, &quot;%s -&gt; %s&quot; % (tarinfo.name, tarinfo.linkname)) else: self._dbg(1, tarinfo.name) if tarinfo.isreg(): self.makefile(tarinfo, targetpath) elif tarinfo.isdir(): self.makedir(tarinfo, targetpath) elif tarinfo.isfifo(): self.makefifo(tarinfo, targetpath) elif tarinfo.ischr() or tarinfo.isblk(): self.makedev(tarinfo, targetpath) elif tarinfo.islnk() or tarinfo.issym(): self.makelink(tarinfo, targetpath) elif tarinfo.type not in SUPPORTED_TYPES: self.makeunknown(tarinfo, targetpath) else: self.makefile(tarinfo, targetpath) self.chown(tarinfo, targetpath) if not tarinfo.issym(): self.chmod(tarinfo, targetpath) self.utime(tarinfo, targetpath) #-------------------------------------------------------------------------- # Below are the different file methods. They are called via # _extract_member() when extract() is called. They can be replaced in a # subclass to implement other functionality. def makedir(self, tarinfo, targetpath): &quot;&quot;&quot;Make a directory called targetpath. &quot;&quot;&quot; try: # Use a safe mode for the directory, the real mode is set # later in _extract_member(). os.mkdir(targetpath, 0700) except EnvironmentError, e: if e.errno != errno.EEXIST: raise def makefile(self, tarinfo, targetpath): &quot;&quot;&quot;Make a file called targetpath. &quot;&quot;&quot; source = self.extractfile(tarinfo) try: with bltn_open(targetpath, &quot;wb&quot;) as target: copyfileobj(source, target) finally: source.close() def makeunknown(self, tarinfo, targetpath): &quot;&quot;&quot;Make a file from a TarInfo object with an unknown type at targetpath. &quot;&quot;&quot; self.makefile(tarinfo, targetpath) self._dbg(1, &quot;tarfile: Unknown file type %r, &quot; \ &quot;extracted as regular file.&quot; % tarinfo.type) def makefifo(self, tarinfo, targetpath): &quot;&quot;&quot;Make a fifo called targetpath. &quot;&quot;&quot; if hasattr(os, &quot;mkfifo&quot;): os.mkfifo(targetpath) else: raise ExtractError(&quot;fifo not supported by system&quot;) def makedev(self, tarinfo, targetpath): &quot;&quot;&quot;Make a character or block device called targetpath. &quot;&quot;&quot; if not hasattr(os, &quot;mknod&quot;) or not hasattr(os, &quot;makedev&quot;): raise ExtractError(&quot;special devices not supported by system&quot;) mode = tarinfo.mode if tarinfo.isblk(): mode |= stat.S_IFBLK else: mode |= stat.S_IFCHR os.mknod(targetpath, mode, os.makedev(tarinfo.devmajor, tarinfo.devminor)) def makelink(self, tarinfo, targetpath): &quot;&quot;&quot;Make a (symbolic) link called targetpath. If it cannot be created (platform limitation), we try to make a copy of the referenced file instead of a link. &quot;&quot;&quot; if hasattr(os, &quot;symlink&quot;) and hasattr(os, &quot;link&quot;): # For systems that support symbolic and hard links. if tarinfo.issym(): if os.path.lexists(targetpath): os.unlink(targetpath) os.symlink(tarinfo.linkname, targetpath) else: # See extract(). if os.path.exists(tarinfo._link_target): if os.path.lexists(targetpath): os.unlink(targetpath) os.link(tarinfo._link_target, targetpath) else: self._extract_member(self._find_link_target(tarinfo), targetpath) else: try: self._extract_member(self._find_link_target(tarinfo), targetpath) except KeyError: raise ExtractError(&quot;unable to resolve link inside archive&quot;) def chown(self, tarinfo, targetpath): &quot;&quot;&quot;Set owner of targetpath according to tarinfo. &quot;&quot;&quot; if pwd and hasattr(os, &quot;geteuid&quot;) and os.geteuid() == 0: # We have to be root to do so. try: g = grp.getgrnam(tarinfo.gname)[2] except KeyError: g = tarinfo.gid try: u = pwd.getpwnam(tarinfo.uname)[2] except KeyError: u = tarinfo.uid try: if tarinfo.issym() and hasattr(os, &quot;lchown&quot;): os.lchown(targetpath, u, g) else: if sys.platform != &quot;os2emx&quot;: os.chown(targetpath, u, g) except EnvironmentError, e: raise ExtractError(&quot;could not change owner&quot;) def chmod(self, tarinfo, targetpath): &quot;&quot;&quot;Set file permissions of targetpath according to tarinfo. &quot;&quot;&quot; if hasattr(os, &apos;chmod&apos;): try: os.chmod(targetpath, tarinfo.mode) except EnvironmentError, e: raise ExtractError(&quot;could not change mode&quot;) def utime(self, tarinfo, targetpath): &quot;&quot;&quot;Set modification time of targetpath according to tarinfo. &quot;&quot;&quot; if not hasattr(os, &apos;utime&apos;): return try: os.utime(targetpath, (tarinfo.mtime, tarinfo.mtime)) except EnvironmentError, e: raise ExtractError(&quot;could not change modification time&quot;) #-------------------------------------------------------------------------- def next(self): &quot;&quot;&quot;Return the next member of the archive as a TarInfo object, when TarFile is opened for reading. Return None if there is no more available. &quot;&quot;&quot; self._check(&quot;ra&quot;) if self.firstmember is not None: m = self.firstmember self.firstmember = None return m # Read the next block. self.fileobj.seek(self.offset) tarinfo = None while True: try: tarinfo = self.tarinfo.fromtarfile(self) except EOFHeaderError, e: if self.ignore_zeros: self._dbg(2, &quot;0x%X: %s&quot; % (self.offset, e)) self.offset += BLOCKSIZE continue except InvalidHeaderError, e: if self.ignore_zeros: self._dbg(2, &quot;0x%X: %s&quot; % (self.offset, e)) self.offset += BLOCKSIZE continue elif self.offset == 0: raise ReadError(str(e)) except EmptyHeaderError: if self.offset == 0: raise ReadError(&quot;empty file&quot;) except TruncatedHeaderError, e: if self.offset == 0: raise ReadError(str(e)) except SubsequentHeaderError, e: raise ReadError(str(e)) break if tarinfo is not None: self.members.append(tarinfo) else: self._loaded = True return tarinfo #-------------------------------------------------------------------------- # Little helper methods: def _getmember(self, name, tarinfo=None, normalize=False): &quot;&quot;&quot;Find an archive member by name from bottom to top. If tarinfo is given, it is used as the starting point. &quot;&quot;&quot; # Ensure that all members have been loaded. members = self.getmembers() # Limit the member search list up to tarinfo. if tarinfo is not None: members = members[:members.index(tarinfo)] if normalize: name = os.path.normpath(name) for member in reversed(members): if normalize: member_name = os.path.normpath(member.name) else: member_name = member.name if name == member_name: return member def _load(self): &quot;&quot;&quot;Read through the entire archive file and look for readable members. &quot;&quot;&quot; while True: tarinfo = self.next() if tarinfo is None: break self._loaded = True def _check(self, mode=None): &quot;&quot;&quot;Check if TarFile is still open, and if the operation&apos;s mode corresponds to TarFile&apos;s mode. &quot;&quot;&quot; if self.closed: raise IOError(&quot;%s is closed&quot; % self.__class__.__name__) if mode is not None and self.mode not in mode: raise IOError(&quot;bad operation for mode %r&quot; % self.mode) def _find_link_target(self, tarinfo): &quot;&quot;&quot;Find the target member of a symlink or hardlink member in the archive. &quot;&quot;&quot; if tarinfo.issym(): # Always search the entire archive. linkname = &quot;/&quot;.join(filter(None, (os.path.dirname(tarinfo.name), tarinfo.linkname))) limit = None else: # Search the archive before the link, because a hard link is # just a reference to an already archived file. linkname = tarinfo.linkname limit = tarinfo member = self._getmember(linkname, tarinfo=limit, normalize=True) if member is None: raise KeyError(&quot;linkname %r not found&quot; % linkname) return member def __iter__(self): &quot;&quot;&quot;Provide an iterator object. &quot;&quot;&quot; if self._loaded: return iter(self.members) else: return TarIter(self) def _dbg(self, level, msg): &quot;&quot;&quot;Write debugging output to sys.stderr. &quot;&quot;&quot; if level &lt;= self.debug: print &gt;&gt; sys.stderr, msg def __enter__(self): self._check() return self def __exit__(self, type, value, traceback): if type is None: self.close() else: # An exception occurred. We must not call close() because # it would try to write end-of-archive blocks and padding. if not self._extfileobj: self.fileobj.close() self.closed = True# class TarFile 七、ConfigParser用于对特定的配置进行操作，当前模块的名称在 python 3.x 版本中变更为 configparser。 123456789# 注释1; 注释2 [section1]k1 = v1k2:v2 [section2]k1 = v1 12345678910111213141516171819202122232425262728293031import ConfigParser config = ConfigParser.ConfigParser()config.read(&apos;i.cfg&apos;) # ########## 读 ###########secs = config.sections()#print secs#options = config.options(&apos;group2&apos;)#print options #item_list = config.items(&apos;group2&apos;)#print item_list #val = config.get(&apos;group1&apos;,&apos;key&apos;)#val = config.getint(&apos;group1&apos;,&apos;key&apos;) # ########## 改写 ###########sec = config.remove_section(&apos;group1&apos;)#config.write(open(&apos;i.cfg&apos;, &quot;w&quot;)) #sec = config.has_section(&apos;wupeiqi&apos;)#sec = config.add_section(&apos;wupeiqi&apos;)#config.write(open(&apos;i.cfg&apos;, &quot;w&quot;)) #config.set(&apos;group2&apos;,&apos;k1&apos;,11111)#config.write(open(&apos;i.cfg&apos;, &quot;w&quot;)) #config.remove_option(&apos;group2&apos;,&apos;age&apos;)#config.write(open(&apos;i.cfg&apos;, &quot;w&quot;)) 八、logging用于便捷记录日志且线程安全的模块 1234567891011121314import logging logging.basicConfig(filename=&apos;log.log&apos;, format=&apos;%(asctime)s - %(name)s - %(levelname)s -%(module)s: %(message)s&apos;, datefmt=&apos;%Y-%m-%d %H:%M:%S %p&apos;, level=10) logging.debug(&apos;debug&apos;)logging.info(&apos;info&apos;)logging.warning(&apos;warning&apos;)logging.error(&apos;error&apos;)logging.critical(&apos;critical&apos;)logging.log(10,&apos;log&apos;) 对于等级： 12345678CRITICAL = 50FATAL = CRITICALERROR = 40WARNING = 30WARN = WARNINGINFO = 20DEBUG = 10NOTSET = 0 只有大于当前日志等级的操作才会被记录。 对于格式，有如下属性可是配置： 九、time时间相关的操作，时间有三种表示方式： 时间戳 1970年1月1日之后的秒，即：time.time() 格式化的字符串 2014-11-11 11:11， 即：time.strftime(‘%Y-%m-%d’) 结构化时间 元组包含了：年、日、星期等… time.struct_time 即：time.localtime() 12345678910111213141516171819202122232425print time.time()print time.mktime(time.localtime()) print time.gmtime() #可加时间戳参数print time.localtime() #可加时间戳参数print time.strptime(&apos;2014-11-11&apos;, &apos;%Y-%m-%d&apos;) print time.strftime(&apos;%Y-%m-%d&apos;) #默认当前时间print time.strftime(&apos;%Y-%m-%d&apos;,time.localtime()) #默认当前时间print time.asctime()print time.asctime(time.localtime())print time.ctime(time.time()) import datetime&apos;&apos;&apos;datetime.date：表示日期的类。常用的属性有year, month, daydatetime.time：表示时间的类。常用的属性有hour, minute, second, microseconddatetime.datetime：表示日期时间datetime.timedelta：表示时间间隔，即两个时间点之间的长度timedelta([days[, seconds[, microseconds[, milliseconds[, minutes[, hours[, weeks]]]]]]])strftime(&quot;%Y-%m-%d&quot;)&apos;&apos;&apos;import datetimeprint datetime.datetime.now()print datetime.datetime.now() - datetime.timedelta(days=5) 十、rere模块用于对python的正则表达式的操作。 1234567891011121314151617181920212223字符： . 匹配除换行符以外的任意字符 \w 匹配字母或数字或下划线或汉字 \s 匹配任意的空白符 \d 匹配数字 \b 匹配单词的开始或结束 ^ 匹配字符串的开始 $ 匹配字符串的结束次数： * 重复零次或更多次 + 重复一次或更多次 ? 重复零次或一次 &#123;n&#125; 重复n次 &#123;n,&#125; 重复n次或更多次 &#123;n,m&#125; 重复n到m次IP：^(25[0-5]|2[0-4]\d|[0-1]?\d?\d)(\.(25[0-5]|2[0-4]\d|[0-1]?\d?\d))&#123;3&#125;$手机号：^1[3|4|5|8][0-9]\d&#123;8&#125;$ 1、match(pattern, string, flags=0) 从起始位置开始根据模型去字符串中匹配指定内容，匹配单个 正则表达式 要匹配的字符串 标志位，用于控制正则表达式的匹配方式 12345import reobj = re.match(&apos;\d+&apos;, &apos;123uuasf&apos;)if obj: print obj.group() 1234567# flagsI = IGNORECASE = sre_compile.SRE_FLAG_IGNORECASE # ignore caseL = LOCALE = sre_compile.SRE_FLAG_LOCALE # assume current 8-bit localeU = UNICODE = sre_compile.SRE_FLAG_UNICODE # assume unicode localeM = MULTILINE = sre_compile.SRE_FLAG_MULTILINE # make anchors look for newlineS = DOTALL = sre_compile.SRE_FLAG_DOTALL # make dot match newlineX = VERBOSE = sre_compile.SRE_FLAG_VERBOSE # ignore whitespace and comments 2、search(pattern, string, flags=0) 根据模型去字符串中匹配指定内容，匹配单个 12345import reobj = re.search(&apos;\d+&apos;, &apos;u123uu888asf&apos;)if obj: print obj.group() 3、group和groups 12345678a = &quot;123abc456&quot;print re.search(&quot;([0-9]*)([a-z]*)([0-9]*)&quot;, a).group()print re.search(&quot;([0-9]*)([a-z]*)([0-9]*)&quot;, a).group(0)print re.search(&quot;([0-9]*)([a-z]*)([0-9]*)&quot;, a).group(1)print re.search(&quot;([0-9]*)([a-z]*)([0-9]*)&quot;, a).group(2)print re.search(&quot;([0-9]*)([a-z]*)([0-9]*)&quot;, a).groups() 4、findall(pattern, string, flags=0) 上述两中方式均用于匹配单值，即：只能匹配字符串中的一个，如果想要匹配到字符串中所有符合条件的元素，则需要使用 findall。 123import reobj = re.findall(&apos;\d+&apos;, &apos;fa123uu888asf&apos;) 5、sub(pattern, repl, string, count=0, flags=0) 用于替换匹配的字符串 12345content = &quot;123abc456&quot;new_content = re.sub(&apos;\d+&apos;, &apos;sb&apos;, content)# new_content = re.sub(&apos;\d+&apos;, &apos;sb&apos;, content, 1)print new_contentprint obj 相比于str.replace功能更加强大 6、split(pattern, string, maxsplit=0, flags=0) 根据指定匹配进行分组 1234content = &quot;&apos;1 - 2 * ((60-30+1*(9-2*5/3+7/3*99/4*2998+10*568/14))-(-4*3)/(16-3*2) )&apos;&quot;new_content = re.split(&apos;\*&apos;, content)# new_content = re.split(&apos;\*&apos;, content, 1)print new_content 1234content = &quot;&apos;1 - 2 * ((60-30+1*(9-2*5/3+7/3*99/4*2998+10*568/14))-(-4*3)/(16-3*2) )&apos;&quot;new_content = re.split(&apos;[\+\-\*\/]+&apos;, content)# new_content = re.split(&apos;\*&apos;, content, 1)print new_content 1234inpp = &apos;1-2*((60-30 +(-40-5)*(9-2*5/3 + 7 /3*99/4*2998 +10 * 568/14 )) - (-4*3)/ (16-3*2))&apos;inpp = re.sub(&apos;\s*&apos;,&apos;&apos;,inpp)new_content = re.split(&apos;\(([\+\-\*\/]?\d+[\+\-\*\/]?\d+)&#123;1&#125;\)&apos;, inpp, 1)print new_content 相比于str.split更加强大 实例：计算器源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179#!/usr/bin/env python# -*- coding:utf-8 -*-&quot;&quot;&quot;该计算器思路： 1、递归寻找表达式中只含有 数字和运算符的表达式，并计算结果 2、由于整数计算会忽略小数，所有的数字都认为是浮点型操作，以此来保留小数使用技术： 1、正则表达式 2、递归 执行流程如下：******************** 请计算表达式： 1 - 2 * ( (60-30 +(-40.0/5) * (9-2*5/3 + 7 /3*99/4*2998 +10 * 568/14 )) - (-4*3)/ (16-3*2) ) ********************before： [&apos;1-2*((60-30+(-40.0/5)*(9-2*5/3+7/3*99/4*2998+10*568/14))-(-4*3)/(16-3*2))&apos;]-40.0/5=-8.0after： [&apos;1-2*((60-30+-8.0*(9-2*5/3+7/3*99/4*2998+10*568/14))-(-4*3)/(16-3*2))&apos;]========== 上一次计算结束 ==========before： [&apos;1-2*((60-30+-8.0*(9-2*5/3+7/3*99/4*2998+10*568/14))-(-4*3)/(16-3*2))&apos;]9-2*5/3+7/3*99/4*2998+10*568/14=173545.880953after： [&apos;1-2*((60-30+-8.0*173545.880953)-(-4*3)/(16-3*2))&apos;]========== 上一次计算结束 ==========before： [&apos;1-2*((60-30+-8.0*173545.880953)-(-4*3)/(16-3*2))&apos;]60-30+-8.0*173545.880953=-1388337.04762after： [&apos;1-2*(-1388337.04762-(-4*3)/(16-3*2))&apos;]========== 上一次计算结束 ==========before： [&apos;1-2*(-1388337.04762-(-4*3)/(16-3*2))&apos;]-4*3=-12.0after： [&apos;1-2*(-1388337.04762--12.0/(16-3*2))&apos;]========== 上一次计算结束 ==========before： [&apos;1-2*(-1388337.04762--12.0/(16-3*2))&apos;]16-3*2=10.0after： [&apos;1-2*(-1388337.04762--12.0/10.0)&apos;]========== 上一次计算结束 ==========before： [&apos;1-2*(-1388337.04762--12.0/10.0)&apos;]-1388337.04762--12.0/10.0=-1388335.84762after： [&apos;1-2*-1388335.84762&apos;]========== 上一次计算结束 ==========我的计算结果： 2776672.69524&quot;&quot;&quot; import re def compute_mul_div(arg): &quot;&quot;&quot; 操作乘除 :param expression:表达式 :return:计算结果 &quot;&quot;&quot; val = arg[0] mch = re.search(&apos;\d+\.*\d*[\*\/]+[\+\-]?\d+\.*\d*&apos;, val) if not mch: return content = re.search(&apos;\d+\.*\d*[\*\/]+[\+\-]?\d+\.*\d*&apos;, val).group() if len(content.split(&apos;*&apos;))&gt;1: n1, n2 = content.split(&apos;*&apos;) value = float(n1) * float(n2) else: n1, n2 = content.split(&apos;/&apos;) value = float(n1) / float(n2) before, after = re.split(&apos;\d+\.*\d*[\*\/]+[\+\-]?\d+\.*\d*&apos;, val, 1) new_str = &quot;%s%s%s&quot; % (before,value,after) arg[0] = new_str compute_mul_div(arg) def compute_add_sub(arg): &quot;&quot;&quot; 操作加减 :param expression:表达式 :return:计算结果 &quot;&quot;&quot; while True: if arg[0].__contains__(&apos;+-&apos;) or arg[0].__contains__(&quot;++&quot;) or arg[0].__contains__(&apos;-+&apos;) or arg[0].__contains__(&quot;--&quot;): arg[0] = arg[0].replace(&apos;+-&apos;,&apos;-&apos;) arg[0] = arg[0].replace(&apos;++&apos;,&apos;+&apos;) arg[0] = arg[0].replace(&apos;-+&apos;,&apos;-&apos;) arg[0] = arg[0].replace(&apos;--&apos;,&apos;+&apos;) else: break if arg[0].startswith(&apos;-&apos;): arg[1] += 1 arg[0] = arg[0].replace(&apos;-&apos;,&apos;&amp;&apos;) arg[0] = arg[0].replace(&apos;+&apos;,&apos;-&apos;) arg[0] = arg[0].replace(&apos;&amp;&apos;,&apos;+&apos;) arg[0] = arg[0][1:] val = arg[0] mch = re.search(&apos;\d+\.*\d*[\+\-]&#123;1&#125;\d+\.*\d*&apos;, val) if not mch: return content = re.search(&apos;\d+\.*\d*[\+\-]&#123;1&#125;\d+\.*\d*&apos;, val).group() if len(content.split(&apos;+&apos;))&gt;1: n1, n2 = content.split(&apos;+&apos;) value = float(n1) + float(n2) else: n1, n2 = content.split(&apos;-&apos;) value = float(n1) - float(n2) before, after = re.split(&apos;\d+\.*\d*[\+\-]&#123;1&#125;\d+\.*\d*&apos;, val, 1) new_str = &quot;%s%s%s&quot; % (before,value,after) arg[0] = new_str compute_add_sub(arg) def compute(expression): &quot;&quot;&quot; 操作加减乘除 :param expression:表达式 :return:计算结果 &quot;&quot;&quot; inp = [expression,0] # 处理表达式中的乘除 compute_mul_div(inp) # 处理 compute_add_sub(inp) if divmod(inp[1],2)[1] == 1: result = float(inp[0]) result = result * -1 else: result = float(inp[0]) return result def exec_bracket(expression): &quot;&quot;&quot; 递归处理括号，并计算 :param expression: 表达式 :return:最终计算结果 &quot;&quot;&quot; # 如果表达式中已经没有括号，则直接调用负责计算的函数，将表达式结果返回，如：2*1-82+444 if not re.search(&apos;\(([\+\-\*\/]*\d+\.*\d*)&#123;2,&#125;\)&apos;, expression): final = compute(expression) return final # 获取 第一个 只含有 数字/小数 和 操作符 的括号 # 如： # [&apos;1-2*((60-30+(-40.0/5)*(9-2*5/3+7/3*99/4*2998+10*568/14))-(-4*3)/(16-3*2))&apos;] # 找出：(-40.0/5) content = re.search(&apos;\(([\+\-\*\/]*\d+\.*\d*)&#123;2,&#125;\)&apos;, expression).group() # 分割表达式，即： # 将[&apos;1-2*((60-30+(-40.0/5)*(9-2*5/3+7/3*99/4*2998+10*568/14))-(-4*3)/(16-3*2))&apos;] # 分割更三部分：[&apos;1-2*((60-30+( (-40.0/5) *(9-2*5/3+7/3*99/4*2998+10*568/14))-(-4*3)/(16-3*2))&apos;] before, nothing, after = re.split(&apos;\(([\+\-\*\/]*\d+\.*\d*)&#123;2,&#125;\)&apos;, expression, 1) print &apos;before：&apos;,expression content = content[1:len(content)-1] # 计算，提取的表示 (-40.0/5)，并活的结果，即：-40.0/5=-8.0 ret = compute(content) print &apos;%s=%s&apos; %( content, ret) # 将执行结果拼接，[&apos;1-2*((60-30+( -8.0 *(9-2*5/3+7/3*99/4*2998+10*568/14))-(-4*3)/(16-3*2))&apos;] expression = &quot;%s%s%s&quot; %(before, ret, after) print &apos;after：&apos;,expression print &quot;=&quot;*10,&apos;上一次计算结束&apos;,&quot;=&quot;*10 # 循环继续下次括号处理操作，本次携带者的是已被处理后的表达式，即： # [&apos;1-2*((60-30+ -8.0 *(9-2*5/3+7/3*99/4*2998+10*568/14))-(-4*3)/(16-3*2))&apos;] # 如此周而复始的操作，直到表达式中不再含有括号 return exec_bracket(expression) # 使用 __name__ 的目的：# 只有执行 python index.py 时，以下代码才执行# 如果其他人导入该模块，以下代码不执行if __name__ == &quot;__main__&quot;: #print &apos;*&apos;*20,&quot;请计算表达式：&quot;, &quot;1 - 2 * ( (60-30 +(-40.0/5) * (9-2*5/3 + 7 /3*99/4*2998 +10 * 568/14 )) - (-4*3)/ (16-3*2) )&quot; ,&apos;*&apos;*20 #inpp = &apos;1 - 2 * ( (60-30 +(-40.0/5) * (9-2*5/3 + 7 /3*99/4*2998 +10 * 568/14 )) - (-4*3)/ (16-3*2) ) &apos; inpp = &quot;1-2*-30/-12*(-20+200*-3/-200*-300-100)&quot; #inpp = &quot;1-5*980.0&quot; inpp = re.sub(&apos;\s*&apos;,&apos;&apos;,inpp) # 表达式保存在列表中 result = exec_bracket(inpp) print result 十一、random随机数 1234import randomprint random.random()print random.randint(1,2)print random.randrange(1,10) 验证验证码实例: 12345678910import randomcheckcode = &apos;&apos;for i in range(4): current = random.randrange(0,4) if current != i: temp = chr(random.randint(65,90)) else: temp = random.randint(0,9) checkcode += str(temp)print checkcode]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python基础四]]></title>
    <url>%2F2019%2F08%2F17%2Fpython%E5%9F%BA%E7%A1%80%E5%9B%9B%2F</url>
    <content type="text"><![CDATA[python基础本节大纲 迭代器&amp;生成器 装饰器 基本装饰器 多参数装饰器 递归 算法基础：二分查找、二维数组转换 正则表达式 常用模块学习 作业：计算器开发 实现加减乘除及拓号优先级解析 用户输入 1 - 2 * ( (60-30 +(-40/5) * (9-25/3 + 7 /399/42998 +10 * 568/14 )) - (-43)/ (16-32) )等类似公式后，必须自己解析里面的(),+,-,,/符号和公式，运算后得出结果，结果必须与真实的计算器所得出的结果一致 迭代器&amp;生成器迭代器迭代器是访问集合元素的一种方式。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退，不过这也没什么，因为人们很少在迭代途中往后退。另外，迭代器的一大优点是不要求事先准备好整个迭代过程中所有的元素。迭代器仅仅在迭代到某个元素时才计算该元素，而在这之前或之后 ，元素可以不存在或者销毁。这个特点使得它特别适合用于遍历一些巨大的或是无限的集合，比如几个G的文件。 特点： 1.访问者不需要关心迭代器内部的结构，仅需通过next()方法不断去取下一个内容 2.不能随机访问集合的某个值，只能从头到尾依次访问 3.访问到一半时不能往回退 4.便于循环比较大的数据集合，节省内存 生成一个迭代器： 1234567891011121314151617&gt;&gt;&gt; a = iter([1,2,3,4,5])&gt;&gt;&gt; a&lt;list_iterator object at 0x101402630&gt;&gt;&gt;&gt; a.__next__()1&gt;&gt;&gt; a.__next__()2&gt;&gt;&gt; a.__next__()3&gt;&gt;&gt; a.__next__()4&gt;&gt;&gt; a.__next__()5&gt;&gt;&gt; a.__next__()Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration Repeated calls to the iterator’s next() method (or passing it to the built-in function next()) return successive items in the stream. When no more data are available a StopIteration exception is raised instead. At this point, the iterator object is exhausted and any further calls to its next() method just raise StopIteration again. 重复调用迭代器的next __（）方法（或将其传递给内置函数next（））返回流中的连续项。当没有更多数据可用时，会引发StopIteration异常。此时，迭代器对象已耗尽，并且对next __（）方法的任何进一步调用只会再次引发StopIteration。 生成器generator定义：一个函数调用时返回一个迭代器，那这个函数就叫做生成器（generator），如果函数中包含yield语法，那这个函数就会变成生成器 代码： 123456789101112131415161718def cash_out(amount): while amount &gt;0: amount -= 1 yield 1&lt;br&gt; print(&quot;擦，又来取钱了。。。败家子！&quot;) ATM = cash_out(5) print(&quot;取到钱 %s 万&quot; % ATM.__next__())print(&quot;花掉花掉!&quot;)print(&quot;取到钱 %s 万&quot; % ATM.__next__())print(&quot;取到钱 %s 万&quot; % ATM.__next__())print(&quot;花掉花掉!&quot;)print(&quot;取到钱 %s 万&quot; % ATM.__next__())print(&quot;取到钱 %s 万&quot; % ATM.__next__())print(&quot;取到钱 %s 万&quot; % ATM.__next__()) #到这时钱就取没了,再取就报错了print(&quot;取到钱 %s 万&quot; % ATM.__next__()) 作用： 这个yield的主要效果呢，就是可以使函数中断，并保存中断状态，中断后，代码可以继续往下执行，过一段时间还可以再重新调用这个函数，从上次yield的下一句开始执行。 另外，还可通过yield实现在单线程的情况下实现并发运算的效果 123456789101112131415161718192021import timedef consumer(name): print(&quot;%s 准备吃包子啦!&quot; %name) while True: baozi = yield print(&quot;包子[%s]来了,被[%s]吃了!&quot; %(baozi,name)) def producer(name): c = consumer(&apos;A&apos;) c2 = consumer(&apos;B&apos;) c.__next__() c2.__next__() print(&quot;老子开始准备做包子啦!&quot;) for i in range(10): time.sleep(1) print(&quot;做了2个包子!&quot;) c.send(i) c2.send(i) producer(&quot;alex&quot;) 装饰器直接 看银角大王写的文档 http://www.cnblogs.com/wupeiqi/articles/4980620.html 递归特点递归算法是一种直接或者间接地调用自身算法的过程。在计算机编写程序中，递归算法对解决一大类问题是十分有效的，它往往使算法的描述简洁而且易于理解。 递归算法解决问题的特点： 1.递归就是在过程或函数时调用自身。 2.在使用递归策略时，必须有一个明确的递归结束条件，称为递归出口。 3.递归算法解题通常显得很简洁，但递归算法解题的运行效率较低。所以一般不提倡用递归算法设计程序。 4.在递归调用的过程当中系统为每一层的返回点、局部量开辟了栈来存储。递归次数过多容易造成栈溢出等。所以一般不提倡用递归算法设计程序。 要求递归算法所体现的”重复”一般有三个要求: 一是每次调用在规模上都有所缩小(通常是减半); 二是相邻两次重复之间有紧密的联系，前一次要为后一次做准备(通常前一次的输出就作为后一次的输入); 三是在问题的规模极小时必须用直接给出解答而不再进行递归调用，因而每次递归调用都是有条件的(以规模未达到直接解答的大小为条件),无条件递归调用将会成为死循环而不能正常结束。 实现： 1234567891011121314151617181920212223242526272829303132333435361. 通过递归实现2分查找 现有列表 primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]， 要求尔等 用最快的方式 找出23 。 请Low B, Low 2B ,Low 3B 三个同学来回答这个问题。 Low B: 这个很简单，直接用 if 41 in primes:print(&quot;found it!&quot;) , 话音未落就被老师打了，让你自己实现，不是让你用现成提供的功能， Low B于是说，那只能从头开始一个个数了，然后Low B被 开除了。。。 Low 2B: 因为这个列表是有序的， 我可以把列表从中截取一半，大概如下： p1 = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37,41] p2 = [ 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97] 然后看p1[-1]也就是41是否比23大， 如果比23大就代表23肯定在p1里面，否则那就肯定在p2里面。现在我们知道23比41小，所以23肯定在p1里，但p1里依然有很多元素， 怎么找到23呢？很简单，依然按上一次的方法，把p1分成2部分，如下： p1_a = [2, 3, 5, 7, 11, 13,17] p1_b = [19, 23, 29, 31, 37,41] 然后我们发现，23 比p1_a最后一个值 17 大，那代表23肯定在p1_b中， p1_b中依然有很多元素，那就再按之前的方法继续分半，最终用不了几次，肯定就把23找出来了！ 说完，Low 2B满有成就感的甩了下头上的头皮屑。 老师：很好，确实较Low B的方案强很多。 然后转头问Low 3B ，你有更好的想法 么？ Low 3B: 啊。。。噢 ，我。。。我跟Low 2B的想法一样，结果被他说了。 老师：噢，那你帮我把代码写出来吧。 Low 3B此时冷汗直冒，因为他根本没思路，但还是硬着头皮去写了。。。。虽然自己没思路，但是会谷歌呀，三个小时过去了，终于憋出了以下代码：def binary_search(data_list,find_num): mid_pos = int(len(data_list) /2 ) #find the middle position of the list mid_val = data_list[mid_pos] # get the value by it&apos;s position print(data_list) if len(data_list) &gt;1: if mid_val &gt; find_num: # means the find_num is in left hand of mid_val print(&quot;[%s] should be in left of [%s]&quot; %(find_num,mid_val)) binary_search(data_list[:mid_pos],find_num) elif mid_val &lt; find_num: # means the find_num is in the right hand of mid_val print(&quot;[%s] should be in right of [%s]&quot; %(find_num,mid_val)) binary_search(data_list[mid_pos:],find_num) else: # means the mid_val == find_num print(&quot;Find &quot;, find_num) else: print(&quot;cannot find [%s] in data_list&quot; %find_num) if __name__ == &apos;__main__&apos;: primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97] binary_search(primes,67)以上就是典型的递归用法，在程序里自己调用自己 算法基础要求：生成一个4*4的2维数组并将其顺时针旋转90度 1234567891011121314151617181920#!_*_coding:utf-8_*_ array=[[col for col in range(5)] for row in range(5)] #初始化一个4*4数组#array=[[col for col in &apos;abcde&apos;] for row in range(5)] for row in array: #旋转前先看看数组长啥样 print(row) print(&apos;-------------&apos;)for i,row in enumerate(array): for index in range(i,len(row)): tmp = array[index][i] #get each rows&apos; data by column&apos;s index array[index][i] = array[i][index] # print tmp,array[i][index] #= tmp array[i][index] = tmp for r in array:print r print(&apos;--one big loop --&apos;) 冒泡排序将一个不规则的数组按从小到大的顺序进行排序 123456789101112131415data = [10,4,33,21,54,3,8,11,5,22,2,1,17,13,6] print(&quot;before sort:&quot;,data) previous = data[0]for j in range(len(data)): tmp = 0 for i in range(len(data)-1): if data[i] &gt; data[i+1]: tmp=data[i] data[i] = data[i+1] data[i+1] = tmp print(data) print(&quot;after sort:&quot;,data) 时间复杂度 （1）时间频度 一个算法执行所耗费的时间，从理论上是不能算出来的，必须上机运行测试才能知道。但我们不可能也没有必要对每个算法都上机测试，只需知道哪个算法花费的时间多，哪个算法花费的时间少就可以了。并且一个算法花费的时间与算法中语句的执行次数成正比例，哪个算法中语句执行次数多，它花费时间就多。一个算法中的语句执行次数称为语句频度或时间频度。记为T(n)。 （2）时间复杂度 在刚才提到的时间频度中，n称为问题的规模，当n不断变化时，时间频度T(n)也会不断变化。但有时我们想知道它变化时呈现什么规律。为此，我们引入时间复杂度概念。 一般情况下，算法中基本操作重复执行的次数是问题规模n的某个函数，用T(n)表示，若有某个辅助函数f(n),使得当n趋近于无穷大时，T(n)/f(n)的极限值为不等于零的常数，则称f(n)是T(n)的同数量级函数。记作T(n)=Ｏ(f(n)),称Ｏ(f(n)) 为算法的渐进时间复杂度，简称时间复杂度。 指数时间 指的是一个问题求解所需要的计算时间m(n)，依输入数据的大小n而呈指数成长（即输入数据的数量依线性成长，所花的时间将会以指数成长） 12345for (i=1; i&lt;=n; i++) x++;for (i=1; i&lt;=n; i++) for (j=1; j&lt;=n; j++) x++; 第一个for循环的时间复杂度为Ο(n)，第二个for循环的时间复杂度为Ο(n2)，则整个算法的时间复杂度为Ο(n+n2)=Ο(n2)。 常数时间 若对于一个算法，T(n)的上界与输入大小无关，则称其具有常数时间，记作O(1)时间。一个例子是访问数组中的单个元素，因为访问它只需要一条指令。但是，找到无序数组中的最小元素则不是，因为这需要遍历所有元素来找出最小值。这是一项线性时间的操作，或称O(n)时间。但如果预先知道元素的数量并假设数量保持不变，则该操作也可被称为具有常数时间。 对数时间 若算法的T(n) = O(log n)，则称其具有对数时间 常见的具有对数时间的算法有二叉树的相关操作和二分搜索。 对数时间的算法是非常有效的，因为每增加一个输入，其所需要的额外计算时间会变小。 递归地将字符串砍半并且输出是这个类别函数的一个简单例子。它需要O（log n）的时间因为每次输出之前我们都将字符串砍半。 这意味着，如果我们想增加输出的次数，我们需要将字符串长度加倍。 线性时间 如果一个算法的时间复杂度为O(n)，则称这个算法具有线性时间，或O(n)时间。非正式地说，这意味着对于足够大的输入，运行时间增加的大小与输入成线性关系。例如，一个计算列表所有元素的和的程序，需要的时间与列表的长度成正比。 正则表达式语法: 123456import re #导入模块名 p = re.compile(&quot;^[0-9]&quot;) #生成要匹配的正则对象 ， ^代表从开头匹配，[0-9]代表匹配0至9的任意一个数字， 所以这里的意思是对传进来的字符串进行匹配，如果这个字符串的开头第一个字符是数字，就代表匹配上了 m = p.match(&apos;14534Abc&apos;) #按上面生成的正则对象 去匹配 字符串， 如果能匹配成功，这个m就会有值， 否则m为None&lt;br&gt;&lt;br&gt;if m: #不为空代表匹配上了 print(m.group()) #m.group()返回匹配上的结果，此处为1，因为匹配上的是1这个字符&lt;br&gt;else:&lt;br&gt; print(&quot;doesn&apos;t match.&quot;)&lt;br&gt; 上面的第2 和第3行也可以合并成一行来写： 1m = p.match(&quot;^[0-9]&quot;,&apos;14534Abc&apos;) 效果是一样的，区别在于，第一种方式是提前对要匹配的格式进行了编译（对匹配公式进行解析），这样再去匹配的时候就不用在编译匹配的格式，第2种简写是每次匹配的时候 都 要进行一次匹配公式的编译，所以，如果你需要一个5w行的文件中匹配出所有以数字开头的行，建议先把正则公式进行编译再匹配，这样速度会快点。 匹配格式 1234567891011121314151617181920212223242526272829303132333435363738模式 描述^ 匹配字符串的开头$ 匹配字符串的末尾。. 匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。[...] 用来表示一组字符,单独列出：[amk] 匹配 &apos;a&apos;，&apos;m&apos;或&apos;k&apos;[^...] 不在[]中的字符：[^abc] 匹配除了a,b,c之外的字符。re* 匹配0个或多个的表达式。re+ 匹配1个或多个的表达式。re? 匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式re&#123; n&#125; re&#123; n,&#125; 精确匹配n个前面表达式。re&#123; n, m&#125; 匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式a| b 匹配a或b(re) G匹配括号内的表达式，也表示一个组(?imx) 正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域。(?-imx) 正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域。(?: re) 类似 (...), 但是不表示一个组(?imx: re) 在括号中使用i, m, 或 x 可选标志(?-imx: re) 在括号中不使用i, m, 或 x 可选标志(?#...) 注释.(?= re) 前向肯定界定符。如果所含正则表达式，以 ... 表示，在当前位置成功匹配时成功，否则失败。但一旦所含表达式已经尝试，匹配引擎根本没有提高；模式的剩余部分还要尝试界定符的右边。(?! re) 前向否定界定符。与肯定界定符相反；当所含表达式不能在字符串当前位置匹配时成功(?&gt; re) 匹配的独立模式，省去回溯。\w 匹配字母数字\W 匹配非字母数字\s 匹配任意空白字符，等价于 [\t\n\r\f].\S 匹配任意非空字符\d 匹配任意数字，等价于 [0-9].\D 匹配任意非数字\A 匹配字符串开始\Z 匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串。c\z 匹配字符串结束\G 匹配最后匹配完成的位置。\b 匹配一个单词边界，也就是指单词和空格间的位置。例如， &apos;er\b&apos; 可以匹配&quot;never&quot; 中的 &apos;er&apos;，但不能匹配 &quot;verb&quot; 中的 &apos;er&apos;。\B 匹配非单词边界。&apos;er\B&apos; 能匹配 &quot;verb&quot; 中的 &apos;er&apos;，但不能匹配 &quot;never&quot; 中的 &apos;er&apos;。\n, \t, 等. 匹配一个换行符。匹配一个制表符。等\1...\9 匹配第n个分组的子表达式。\10 匹配第n个分组的子表达式，如果它经匹配。否则指的是八进制字符码的表达式。 正则表达式常用5种操作 123456789101112131415161718192021222324252627re.match(pattern, string) # 从头匹配re.search(pattern, string) # 匹配整个字符串，直到找到一个匹配re.split() # 将匹配到的格式当做分割点对字符串分割成列表12&gt;&gt;&gt;m = re.split(&quot;[0-9]&quot;, &quot;alex1rain2jack3helen rachel8&quot;)&gt;&gt;&gt;print(m)输出： [&apos;alex&apos;, &apos;rain&apos;, &apos;jack&apos;, &apos;helen rachel&apos;, &apos;&apos;]re.findall() # 找到所有要匹配的字符并返回列表格式12&gt;&gt;&gt;m = re.findall(&quot;[0-9]&quot;, &quot;alex1rain2jack3helen rachel8&quot;)&gt;&gt;&gt;print(m)&lt;br&gt;输出：[&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;8&apos;]re.sub(pattern, repl, string, count,flag) # 替换匹配到的字符12m=re.sub(&quot;[0-9]&quot;,&quot;|&quot;, &quot;alex1rain2jack3helen rachel8&quot;,count=2 )print(m)输出：alex|rain|jack3helen rachel8 1234正则表达式实例字符匹配实例 描述python 匹配 &quot;python&quot;. 1234567891011字符类实例 描述[Pp]ython 匹配 &quot;Python&quot; 或 &quot;python&quot;rub[ye] 匹配 &quot;ruby&quot; 或 &quot;rube&quot;[aeiou] 匹配中括号内的任意一个字母[0-9] 匹配任何数字。类似于 [0123456789][a-z] 匹配任何小写字母[A-Z] 匹配任何大写字母[a-zA-Z0-9] 匹配任何字母及数字[^aeiou] 除了aeiou字母以外的所有字符[^0-9] 匹配除了数字外的字符 123456789特殊字符类实例 描述. 匹配除 &quot;\n&quot; 之外的任何单个字符。要匹配包括 &apos;\n&apos; 在内的任何字符，请使用象 &apos;[.\n]&apos; 的模式。\d 匹配一个数字字符。等价于 [0-9]。\D 匹配一个非数字字符。等价于 [^0-9]。\s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。\S 匹配任何非空白字符。等价于 [^ \f\n\r\t\v]。\w 匹配包括下划线的任何单词字符。等价于&apos;[A-Za-z0-9_]&apos;。\W 匹配任何非单词字符。等价于 &apos;[^A-Za-z0-9_]&apos;。 ####re.match与re.search的区别re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。 12345678910Regular Expression Modifiers: Option FlagsRegular expression literals may include an optional modifier to control various aspects of matching. The modifiers are specified as an optional flag. You can provide multiple modifiers using exclusive OR (|), as shown previously and may be represented by one of these −Modifier Descriptionre.I Performs case-insensitive matching.re.L Interprets words according to the current locale. This interpretation affects the alphabetic group (\w and \W), as well as word boundary behavior (\b and \B).re.M Makes $ match the end of a line (not just the end of the string) and makes ^ match the start of any line (not just the start of the string).re.S Makes a period (dot) match any character, including a newline.re.U Interprets letters according to the Unicode character set. This flag affects the behavior of \w, \W, \b, \B.re.X Permits &quot;cuter&quot; regular expression syntax. It ignores whitespace (except inside a set [] or when escaped by a backslash) and treats unescaped # as a comment marker. 几个常见正则例子： 匹配手机号 123456phone_str = &quot;hey my name is alex, and my phone number is 13651054607, please call me if you are pretty!&quot;phone_str2 = &quot;hey my name is alex, and my phone number is 18651054604, please call me if you are pretty!&quot; m = re.search(&quot;(1)([358]\d&#123;9&#125;)&quot;,phone_str2)if m: print(m.group()) 匹配IP V4 12345ip_addr = &quot;inet 192.168.60.223 netmask 0xffffff00 broadcast 192.168.60.255&quot; m = re.search(&quot;\d&#123;1,3&#125;\.\d&#123;1,3&#125;\.\d&#123;1,3&#125;\.\d&#123;1,3&#125;&quot;, ip_addr) print(m.group()) 分组匹配地址 12345678910111213141516171819contactInfo = &apos;Oldboy School, Beijing Changping Shahe: 010-8343245&apos;match = re.search(r&apos;(\w+), (\w+): (\S+)&apos;, contactInfo) #分组&quot;&quot;&quot;&gt;&gt;&gt; match.group(1) &apos;Doe&apos; &gt;&gt;&gt; match.group(2) &apos;John&apos; &gt;&gt;&gt; match.group(3) &apos;555-1212&apos;&quot;&quot;&quot;match = re.search(r&apos;(?P&lt;last&gt;\w+), (?P&lt;first&gt;\w+): (?P&lt;phone&gt;\S+)&apos;, contactInfo)&quot;&quot;&quot; &gt;&gt;&gt; match.group(&apos;last&apos;) &apos;Doe&apos; &gt;&gt;&gt; match.group(&apos;first&apos;) &apos;John&apos; &gt;&gt;&gt; match.group(&apos;phone&apos;) &apos;555-1212&apos;&quot;&quot;&quot; 匹配email 1234email = &quot;alex.li@126.com http://www.oldboyedu.com&quot; m = re.search(r&quot;[0-9.a-z]&#123;0,26&#125;@[0-9.a-z]&#123;0,20&#125;.[0-9a-z]&#123;0,8&#125;&quot;, email)print(m.group()) json 和 pickle 用于序列化的两个模块 json，用于字符串 和 python数据类型间进行转换 pickle，用于python特有的类型 和 python的数据类型间进行转换 Json模块提供了四个功能：dumps、dump、loads、load pickle模块提供了四个功能：dumps、dump、loads、load 其它常用模块学习 http://www.cnblogs.com/wupeiqi/articles/4963027.html]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python字符格式化]]></title>
    <url>%2F2019%2F08%2F17%2Fpython%E5%AD%97%E7%AC%A6%E6%A0%BC%E5%BC%8F%E5%8C%96%2F</url>
    <content type="text"><![CDATA[python字符串格式化python的字符串格式化有两种方式:百分号方式、format方式 百分的方式相对来说比较老，而format方式则是比较先进的方式，企图替换古老的方式，目前两者并存。 1、百分号方式 12345678910111213141516171819202122232425%[(name)][flags][width].[precision]typecode(name) 可选，用于选择指定的keyflags 可选，可供选择的值有:+ 右对齐；正数前加正好，负数前加负号；- 左对齐；正数前无符号，负数前加负号；空格 右对齐；正数前加空格，负数前加负号；0 右对齐；正数前无符号，负数前加负号；用0填充空白处width 可选，占有宽度.precision 可选，小数点后保留的位数typecode 必选s，获取传入对象的__str__方法的返回值，并将其格式化到指定位置r，获取传入对象的__repr__方法的返回值，并将其格式化到指定位置c，整数：将数字转换成其unicode对应的值，10进制范围为 0 &lt;= i &lt;= 1114111（py27则只支持0-255）；字符：将字符添加到指定位置o，将整数转换成 八 进制表示，并将其格式化到指定位置x，将整数转换成十六进制表示，并将其格式化到指定位置d，将整数、浮点数转换成 十 进制表示，并将其格式化到指定位置e，将整数、浮点数转换成科学计数法，并将其格式化到指定位置（小写e）E，将整数、浮点数转换成科学计数法，并将其格式化到指定位置（大写E）f， 将整数、浮点数转换成浮点数表示，并将其格式化到指定位置（默认保留小数点后6位）F，同上g，自动调整将整数、浮点数转换成 浮点型或科学计数法表示（超过6位数用科学计数法），并将其格式化到指定位置（如果是科学计数则是e；）G，自动调整将整数、浮点数转换成 浮点型或科学计数法表示（超过6位数用科学计数法），并将其格式化到指定位置（如果是科学计数则是E；）%，当字符串中存在格式化标志时，需要用 %%表示一个百分号注：Python中百分号格式化是不存在自动将整数转换成二进制表示的方式 常用格式化： 1234567891011tpl = &quot;i am %s&quot; % &quot;alex&quot; tpl = &quot;i am %s age %d&quot; % (&quot;alex&quot;, 18) tpl = &quot;i am %(name)s age %(age)d&quot; % &#123;&quot;name&quot;: &quot;alex&quot;, &quot;age&quot;: 18&#125; tpl = &quot;percent %.2f&quot; % 99.97623 tpl = &quot;i am %(pp).2f&quot; % &#123;&quot;pp&quot;: 123.425556, &#125; tpl = &quot;i am %.2f %%&quot; % &#123;&quot;pp&quot;: 123.425556, &#125; 2、Format方式 1234567891011121314151617181920212223242526272829303132333435[[fill]align][sign][#][0][width][,][.precision][type]fill 【可选】空白处填充的字符align 【可选】对齐方式（需配合width使用）&lt;，内容左对齐&gt;，内容右对齐(默认)＝，内容右对齐，将符号放置在填充字符的左侧，且只对数字类型有效。 即使：符号+填充物+数字^，内容居中sign 【可选】有无符号数字+，正号加正，负号加负； -，正号不变，负号加负；空格 ，正号空格，负号加负；# 【可选】对于二进制、八进制、十六进制，如果加上#，会显示 0b/0o/0x，否则不显示， 【可选】为数字添加分隔符，如：1,000,000width 【可选】格式化位所占宽度.precision 【可选】小数位保留精度type 【可选】格式化类型传入” 字符串类型 “的参数s，格式化字符串类型数据空白，未指定类型，则默认是None，同s传入“ 整数类型 ”的参数b，将10进制整数自动转换成2进制表示然后格式化c，将10进制整数自动转换为其对应的unicode字符d，十进制整数o，将10进制整数自动转换成8进制表示然后格式化；x，将10进制整数自动转换成16进制表示然后格式化（小写x）X，将10进制整数自动转换成16进制表示然后格式化（大写X）传入“ 浮点型或小数类型 ”的参数e， 转换为科学计数法（小写e）表示，然后格式化；E， 转换为科学计数法（大写E）表示，然后格式化;f ， 转换为浮点型（默认小数点后保留6位）表示，然后格式化；F， 转换为浮点型（默认小数点后保留6位）表示，然后格式化；g， 自动在e和f中切换G， 自动在E和F中切换%，显示百分比（默认显示小数点后6位） 常用格式化： 1234567891011121314151617181920212223242526272829 tpl = &quot;i am &#123;&#125;, age &#123;&#125;, &#123;&#125;&quot;.format(&quot;seven&quot;, 18, &apos;alex&apos;) tpl = &quot;i am &#123;&#125;, age &#123;&#125;, &#123;&#125;&quot;.format(*[&quot;seven&quot;, 18, &apos;alex&apos;]) tpl = &quot;i am &#123;0&#125;, age &#123;1&#125;, really &#123;0&#125;&quot;.format(&quot;seven&quot;, 18) tpl = &quot;i am &#123;0&#125;, age &#123;1&#125;, really &#123;0&#125;&quot;.format(*[&quot;seven&quot;, 18]) tpl = &quot;i am &#123;name&#125;, age &#123;age&#125;, really &#123;name&#125;&quot;.format(name=&quot;seven&quot;, age=18) tpl = &quot;i am &#123;name&#125;, age &#123;age&#125;, really &#123;name&#125;&quot;.format(**&#123;&quot;name&quot;: &quot;seven&quot;, &quot;age&quot;: 18&#125;) tpl = &quot;i am &#123;0[0]&#125;, age &#123;0[1]&#125;, really &#123;0[2]&#125;&quot;.format([1, 2, 3], [11, 22, 33]) tpl = &quot;i am &#123;:s&#125;, age &#123;:d&#125;, money &#123;:f&#125;&quot;.format(&quot;seven&quot;, 18, 88888.1) tpl = &quot;i am &#123;:s&#125;, age &#123;:d&#125;&quot;.format(*[&quot;seven&quot;, 18]) tpl = &quot;i am &#123;name:s&#125;, age &#123;age:d&#125;&quot;.format(name=&quot;seven&quot;, age=18) tpl = &quot;i am &#123;name:s&#125;, age &#123;age:d&#125;&quot;.format(**&#123;&quot;name&quot;: &quot;seven&quot;, &quot;age&quot;: 18&#125;) tpl = &quot;numbers: &#123;:b&#125;,&#123;:o&#125;,&#123;:d&#125;,&#123;:x&#125;,&#123;:X&#125;, &#123;:%&#125;&quot;.format(15, 15, 15, 15, 15, 15.87623, 2) tpl = &quot;numbers: &#123;:b&#125;,&#123;:o&#125;,&#123;:d&#125;,&#123;:x&#125;,&#123;:X&#125;, &#123;:%&#125;&quot;.format(15, 15, 15, 15, 15, 15.87623, 2) tpl = &quot;numbers: &#123;0:b&#125;,&#123;0:o&#125;,&#123;0:d&#125;,&#123;0:x&#125;,&#123;0:X&#125;, &#123;0:%&#125;&quot;.format(15) tpl = &quot;numbers: &#123;num:b&#125;,&#123;num:o&#125;,&#123;num:d&#125;,&#123;num:x&#125;,&#123;num:X&#125;, &#123;num:%&#125;&quot;.format(num=15) 更多格式化操作：https://docs.python.org/3/library/string.html]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python装饰器]]></title>
    <url>%2F2019%2F08%2F16%2Fpython%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[python装饰器1、必备1234567891011121314#### 第一波 ####def foo(): print &apos;foo&apos; foo #表示是函数foo() #表示执行foo函数 #### 第二波 ####def foo(): print &apos;foo&apos; foo = lambda x: x + 1 foo() # 执行下面的lambda表达式，而不再是原来的foo函数，因为函数 foo 被重新定义了 2、需求来了初创公司有N个业务部门，1个基础平台部门，基础平台负责提供底层的功能，如：数据库操作、redis调用、监控API等功能。业务部门使用基础功能时，只需调用基础平台提供的功能即可。如下： 123456789101112131415161718192021222324252627############### 基础平台提供的功能如下 ############### def f1(): print &apos;f1&apos; def f2(): print &apos;f2&apos; def f3(): print &apos;f3&apos; def f4(): print &apos;f4&apos; ############### 业务部门A 调用基础平台提供的功能 ############### f1()f2()f3()f4() ############### 业务部门B 调用基础平台提供的功能 ############### f1()f2()f3()f4() 目前公司有条不紊的进行着，但是，以前基础平台的开发人员在写代码时候没有关注验证相关的问题，即：基础平台的提供的功能可以被任何人使用。现在需要对基础平台的所有功能进行重构，为平台提供的所有功能添加验证机制，即：执行功能前，先进行验证。 老大把工作交给 Low B，他是这么做的： 跟每个业务部门交涉，每个业务部门自己写代码，调用基础平台的功能之前先验证。诶，这样一来基础平台就不需要做任何修改了。 当天Low B 被开除了… 老大把工作交给 Low BB，他是这么做的： 只对基础平台的代码进行重构，让N业务部门无需做任何修改 12345678910111213141516171819202122232425262728293031323334353637383940############### 基础平台提供的功能如下 ############### def f1(): # 验证1 # 验证2 # 验证3 print &apos;f1&apos;def f2(): # 验证1 # 验证2 # 验证3 print &apos;f2&apos;def f3(): # 验证1 # 验证2 # 验证3 print &apos;f3&apos;def f4(): # 验证1 # 验证2 # 验证3 print &apos;f4&apos;############### 业务部门不变 ############### ### 业务部门A 调用基础平台提供的功能### f1()f2()f3()f4()### 业务部门B 调用基础平台提供的功能 ### f1()f2()f3()f4() 过了一周 Low BB 被开除了… 老大把工作交给 Low BBB，他是这么做的： 只对基础平台的代码进行重构，其他业务部门无需做任何修改 1234567891011121314151617181920212223242526272829303132############### 基础平台提供的功能如下 ############### def check_login(): # 验证1 # 验证2 # 验证3 passdef f1(): check_login() print &apos;f1&apos;def f2(): check_login() print &apos;f2&apos;def f3(): check_login() print &apos;f3&apos;def f4(): check_login() print &apos;f4&apos; 老大看了下Low BBB 的实现，嘴角漏出了一丝的欣慰的笑，语重心长的跟Low BBB聊了个天： 老大说： 写代码要遵循开发封闭原则，虽然在这个原则是用的面向对象开发，但是也适用于函数式编程，简单来说，它规定已经实现的功能代码不允许被修改，但可以被扩展，即： 封闭：已实现的功能代码块 开放：对扩展开发 如果将开放封闭原则应用在上述需求中，那么就不允许在函数 f1 、f2、f3、f4的内部进行修改代码，老板就给了Low BBB一个实现方案： 1234567891011121314151617181920def w1(func): def inner(): # 验证1 # 验证2 # 验证3 return func() return inner @w1def f1(): print &apos;f1&apos;@w1def f2(): print &apos;f2&apos;@w1def f3(): print &apos;f3&apos;@w1def f4(): print &apos;f4&apos; 对于上述代码，也是仅仅对基础平台的代码进行修改，就可以实现在其他人调用函数 f1 f2 f3 f4 之前都进行【验证】操作，并且其他业务部门无需做任何操作。 Low BBB心惊胆战的问了下，这段代码的内部执行原理是什么呢？ 老大正要生气，突然Low BBB的手机掉到地上，恰恰屏保就是Low BBB的女友照片，老大一看一紧一抖，喜笑颜开，交定了Low BBB这个朋友。详细的开始讲解了： 单独以f1为例： 1234567891011def w1(func): def inner(): # 验证1 # 验证2 # 验证3 return func() return inner @w1def f1(): print &apos;f1&apos; 当写完这段代码后（函数未被执行、未被执行、未被执行），python解释器就会从上到下解释代码，步骤如下： 1.def w1(func): ==&gt;将w1函数加载到内存 2.@w1 没错，从表面上看解释器仅仅会解释这两句代码，因为函数在没有被调用之前其内部代码不会被执行。 从表面上看解释器着实会执行这两句，但是 @w1 这一句代码里却有大文章，@函数名 是python的一种语法糖。 如上例@w1内部会执行一下操作： 执行w1函数，并将 @w1 下面的 函数 作为w1函数的参数，即：@w1 等价于 w1(f1) 12345678910w1函数的返回值是： def inner: #验证 return 原来f1() # 此处的 f1 表示原来的f1函数然后，将此返回值再重新赋值给 f1，即：新f1 = def inner: #验证 return 原来f1() 所以，以后业务部门想要执行 f1 函数时，就会执行 新f1 函数，在 新f1 函数内部先执行验证，再执行原来的f1函数，然后将 原来f1 函数的返回值 返回给了业务调用者。如此一来， 即执行了验证的功能，又执行了原来f1函数的内容，并将原f1函数返回值 返回给业务调用着 Low BBB 你明白了吗？要是没明白的话，我晚上去你家帮你解决吧！！！]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python迭代器与生成器]]></title>
    <url>%2F2019%2F08%2F15%2Fpython%E8%BF%AD%E4%BB%A3%E5%99%A8%E4%B8%8E%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[迭代如果给定一个list或tuple，set，我们可以通过for循环来遍历这个list或tuple，set，这种遍历我们称之为迭代(iteration) 在python中，迭代是通过for…in..来完成的，而很多语言比如c语言，迭代list是通过下标完成的，比如java代码: 123for (i=0; i&lt;list.length; i++) &#123; n = list[i];&#125; 可以看出，Python的for循环抽象程度要高于C的for循环，因为Python的for循环不仅可以用在list或tuple上，还可以作用在其他可迭代对象上。 list这种数据类型虽然有下标，但很多其他数据类型是没有下标的，但是，只要是可迭代对象，无论有无下标，都可以迭代，比如dict就可以迭代： 1234567&gt;&gt;&gt; d = &#123;&apos;a&apos;: 1, &apos;b&apos;: 2, &apos;c&apos;: 3&#125;&gt;&gt;&gt; for key in d:... print(key)...acb 因为dict的存储不是按照list的方式顺序排列，所以，迭代出的结果顺序很可能不一样。 默认情况下，dict迭代的是key。如果要迭代value，可以用for value in d.values()，如果要同时迭代key和value，可以用for k, v in d.items()。 由于字符串也是可迭代对象，因此，也可以作用于for循环： 123456&gt;&gt;&gt; for ch in &apos;ABC&apos;:... print(ch)...ABC 所以，当我们使用for循环时，只要作用于一个可迭代对象，for循环就可以正常运行，而我们不太关心该对象究竟是list还是其他数据类型。 那么，如何判断一个对象是可迭代对象呢？方法是通过collections模块的Iterable类型判断： 1234567&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterable) # str是否可迭代True&gt;&gt;&gt; isinstance([1,2,3], Iterable) # list是否可迭代True&gt;&gt;&gt; isinstance(123, Iterable) # 整数是否可迭代False 最后一个小问题，如果要对list实现类似Java那样的下标循环怎么办？Python内置的enumerate函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身： 123456&gt;&gt;&gt; for i, value in enumerate([&apos;A&apos;, &apos;B&apos;, &apos;C&apos;]):... print(i, value)...0 A1 B2 C 上面的for循环里，同时引用了两个变量，在Python里是很常见的，比如下面的代码： 123456&gt;&gt;&gt; for x, y in [(1, 1), (2, 4), (3, 9)]:... print(x, y)...1 12 43 9 列表生成式列表生成式即list comperehensions,是python内置的非常简单却强大的可以用来创建list的生成式。 举个例子，要生成list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]可以用list(range(1, 11))： 12list(range(1, 11))[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 但如果要生成[1x1, 2x2, 3x3, …, 10x10]怎么做？方法一是循环： 123456&gt;&gt;&gt; L = []&gt;&gt;&gt; for x in range(1, 11):... L.append(x * x)...&gt;&gt;&gt; L[1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 但是循环太繁琐，而列表生成式则可以用一行语句代替循环生成上面的list： 12&gt;&gt;&gt; [x * x for x in range(1, 11)][1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 写列表生成式时，把要生成的元素x * x放到前面，后面跟for循环，就可以把list创建出来，十分有用，多写几次，很快就可以熟悉这种语法。 for循环后面还可以加上if判断，这样我们就可以筛选出仅偶数的平方： 12&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0][4, 16, 36, 64, 100] 还可以使用两层循环，可以生成全排列： 12&gt;&gt;&gt; [m + n for m in &apos;ABC&apos; for n in &apos;XYZ&apos;][&apos;AX&apos;, &apos;AY&apos;, &apos;AZ&apos;, &apos;BX&apos;, &apos;BY&apos;, &apos;BZ&apos;, &apos;CX&apos;, &apos;CY&apos;, &apos;CZ&apos;] 三层和三层以上的循环就很少用到了。 运用列表生成式，可以写出非常简洁的代码。例如，列出当前目录下的所有文件和目录名，可以通过一行代码实现： 123&gt;&gt;&gt; import os # 导入os模块，模块的概念后面讲到&gt;&gt;&gt; [d for d in os.listdir(&apos;.&apos;)] # os.listdir可以列出文件和目录[&apos;.emacs.d&apos;, &apos;.ssh&apos;, &apos;.Trash&apos;, &apos;Adlm&apos;, &apos;Applications&apos;, &apos;Desktop&apos;, &apos;Documents&apos;, &apos;Downloads&apos;, &apos;Library&apos;, &apos;Movies&apos;, &apos;Music&apos;, &apos;Pictures&apos;, &apos;Public&apos;, &apos;VirtualBox VMs&apos;, &apos;Workspace&apos;, &apos;XCode&apos;] for循环其实可以同时使用两个甚至多个变量，比如dict的items()可以同时迭代key和value： 1234567&gt;&gt;&gt; d = &#123;&apos;x&apos;: &apos;A&apos;, &apos;y&apos;: &apos;B&apos;, &apos;z&apos;: &apos;C&apos; &#125;&gt;&gt;&gt; for k, v in d.items():... print(k, &apos;=&apos;, v)...y = Bx = Az = C 因此，列表生成式也可以使用两个变量来生成list： 123&gt;&gt;&gt; d = &#123;&apos;x&apos;: &apos;A&apos;, &apos;y&apos;: &apos;B&apos;, &apos;z&apos;: &apos;C&apos; &#125;&gt;&gt;&gt; [k + &apos;=&apos; + v for k, v in d.items()][&apos;y=B&apos;, &apos;x=A&apos;, &apos;z=C&apos;] 最后把一个list中所有的字符串变成小写： 123&gt;&gt;&gt; L = [&apos;Hello&apos;, &apos;World&apos;, &apos;IBM&apos;, &apos;Apple&apos;]&gt;&gt;&gt; [s.lower() for s in L][&apos;hello&apos;, &apos;world&apos;, &apos;ibm&apos;, &apos;apple&apos;] 练习 如果list中既包含字符串，又包含整数，由于非字符串类型没有lower()方法，所以列表生成式会报错： 123456&gt;&gt;&gt; L = [&apos;Hello&apos;, &apos;World&apos;, 18, &apos;Apple&apos;, None]&gt;&gt;&gt; [s.lower() for s in L]Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;listcomp&gt;AttributeError: &apos;int&apos; object has no attribute &apos;lower&apos; 使用内建的isinstance函数可以判断一个变量是不是字符串： 123456&gt;&gt;&gt; x = &apos;abc&apos;&gt;&gt;&gt; y = 123&gt;&gt;&gt; isinstance(x, str)True&gt;&gt;&gt; isinstance(y, str)False 生成器通过列表生成式，我们可以直接创建一个列表。但是，受到内存限制，列表容量肯定是有限的。而且，创建一个包含100万元素的列表，不仅占用很大的存储空间，如果我们仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。 所以，如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。在python中，这种一边循环一边计算的机制，称为生成器:generator。 要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建一个generator： 123456&gt;&gt;&gt; L = [x * x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt; 创建L和g的区别仅在于最外层的[]和()，L是一个list，而g是一个generator。 我们可以直接打印出list的每一个元素，但我们怎么打印出generator的每一个元素呢？ 如果要一个一个打印出来，可以通过next()函数获得generator的下一个返回值： 123456789101112131415161718192021222324&gt;&gt;&gt; next(g)0&gt;&gt;&gt; next(g)1&gt;&gt;&gt; next(g)4&gt;&gt;&gt; next(g)9&gt;&gt;&gt; next(g)16&gt;&gt;&gt; next(g)25&gt;&gt;&gt; next(g)36&gt;&gt;&gt; next(g)49&gt;&gt;&gt; next(g)64&gt;&gt;&gt; next(g)81&gt;&gt;&gt; next(g)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration 我们讲过，generator保存的是算法，每次调用next(g)，就计算出g的下一个元素的值，直到计算到最后一个元素，没有更多的元素时，抛出StopIteration的错误。 当然，上面这种不断调用next(g)实在是太变态了，正确的方法是使用for循环，因为generator也是可迭代对象： 1234567891011121314&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; for n in g:... print(n)... 0149162536496481 所以，我们创建了一个generator后，基本上永远不会调用next()，而是通过for循环来迭代它，并且不需要关心StopIteration的错误。 generator非常强大。如果推算的算法比较复杂，用类似列表生成式的for循环无法实现的时候，还可以用函数来实现。 比如，著名的斐波拉契数列（Fibonacci），除第一个和第二个数外，任意一个数都可由前两个数相加得到： 1, 1, 2, 3, 5, 8, 13, 21, 34, … 斐波拉契数列用列表生成式写不出来，但是，用函数把它打印出来却很容易： 123456789101112131415def fib(max): n, a, b = 0, 0, 1 while n &lt; max: print(b) a, b = b, a + b n = n + 1 return &apos;done&apos;注意，赋值语句：a, b = b, a + b相当于：t = (b, a + b) # t是一个tuplea = t[0]b = t[1] 但不必显式写出临时变量t就可以赋值。 上面的函数可以输出斐波那契数列的前N个数： 12345678&gt;&gt;&gt; fib(6)112358&apos;done&apos; 仔细观察，可以看出，fib函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似generator。 也就是说，上面的函数和generator仅一步之遥。要把fib函数变成generator，只需要把print(b)改为yield b就可以了： 1234567def fib(max): n, a, b = 0, 0, 1 while n &lt; max: yield b a, b = b, a + b n = n + 1 return &apos;done&apos; 这就是定义generator的另一种方法。如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator： 123&gt;&gt;&gt; f = fib(6)&gt;&gt;&gt; f&lt;generator object fib at 0x104feaaa0&gt; 这里，最难理解的就是generator和函数的执行流程不一样。函数是顺序执行，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。 举个简单的例子，定义一个generator，依次返回数字1，3，5： 1234567def odd(): print(&apos;step 1&apos;) yield 1 print(&apos;step 2&apos;) yield(3) print(&apos;step 3&apos;) yield(5) 调用该generator时，首先要生成一个generator对象，然后用next()函数不断获得下一个返回值： 1234567891011121314&gt;&gt;&gt; o = odd()&gt;&gt;&gt; next(o)step 11&gt;&gt;&gt; next(o)step 23&gt;&gt;&gt; next(o)step 35&gt;&gt;&gt; next(o)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration 可以看到，odd不是普通函数，而是generator，在执行过程中，遇到yield就中断，下次又继续执行。执行3次yield后，已经没有yield可以执行了，所以，第4次调用next(o)就报错。 回到fib的例子，我们在循环过程中不断调用yield，就会不断中断。当然要给循环设置一个条件来退出循环，不然就会产生一个无限数列出来。 同样的，把函数改成generator后，我们基本上从来不会用next()来获取下一个返回值，而是直接使用for循环来迭代： 123456789&gt;&gt;&gt; for n in fib(6):... print(n)...112358 但是用for循环调用generator时，发现拿不到generator的return语句的返回值。如果想要拿到返回值，必须捕获StopIteration错误，返回值包含在StopIteration的value中： 小结generator是非常强大的工具，在Python中，可以简单地把列表生成式改成generator，也可以通过函数实现复杂逻辑的generator。 要理解generator的工作原理，它是在for循环的过程中不断计算出下一个元素，并在适当的条件结束for循环。对于函数改成的generator来说，遇到return语句或者执行到函数体最后一行语句，就是结束generator的指令，for循环随之结束。 请注意区分普通函数和generator函数，普通函数调用直接返回结果： 123&gt;&gt;&gt; r = abs(6)&gt;&gt;&gt; r6 generator函数的“调用”实际返回一个generator对象： 123&gt;&gt;&gt; g = fib(6)&gt;&gt;&gt; g&lt;generator object fib at 0x1022ef948&gt; 迭代器我们已经知道，可以直接作用于for循环的数据类型有以下几种: 一类是集合数据类型，如list、tuple、dict、set、str等; 一类是generator，包括生成器和带yield的generator function。 这些可以直接用于for循环的对象统称为可迭代对象:Iterable。 可以使用isinstance()判断一个对象是否是Iterable对象: 1234567891011&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance([], Iterable)True&gt;&gt;&gt; isinstance(&#123;&#125;, Iterable)True&gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterable)True&gt;&gt;&gt; isinstance((x for x in range(10)), Iterable)True&gt;&gt;&gt; isinstance(100, Iterable)False 而生成器不但可以作用于for循环，还可以被next()函数不断调用并返回下一个值，直到最后抛出StopIteration错误表示无法继续返回下一个值了。 可以被next()函数调用并不断返回一个值的对象称为迭代器：Interator。 可以使用isinstance()判断一个对象是否是Iterable对象： 123456789&gt;&gt;&gt; from collections import Iterator&gt;&gt;&gt; isinstance((x for x in range(10)), Iterator)True&gt;&gt;&gt; isinstance([], Iterator)False&gt;&gt;&gt; isinstance(&#123;&#125;, Iterator)False&gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterator)False 生成器都是Iterator对象，但list、dict、str虽然是Iterable(可迭代对象)，却不是Iterator(迭代器对象) 把list、dict、str等Iterable变成Iterator可以使用iter()函数： 1234&gt;&gt;&gt; isinstance(iter([]), Iterator)True&gt;&gt;&gt; isinstance(iter(&apos;abc&apos;), Iterator)True 你可能会问，为什么list、dict、str等数据类型不是Iterator？ 这是因为python的Iterator对象表示的是一个数据流，Iterator对象可以被next()函数调用并不断返回下一个数据，直到没有数据时抛出StopIneration错误。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据，所以Iterator的计算是惰性的，只有在需要返回一个数据时它才会计算。 Iterator甚至可以表示一个无限大的数据流，例如全体自然数，而使用list是永远不可能存储全体自然数的。 小结凡是可作用于for循环的对象都是Iterable类型； 凡是可作用于next()函数的对象都是Iterator类型，它们表示一个惰性计算的序列； 集合数据类型如list、dict、str等是Iterable但不是Iterator，不过可以通过iter()函数获得一个Iterator对象。 Python的for循环本质上就是通过不断调用next()函数实现的，例如： 1234567891011121314for x in [1, 2, 3, 4, 5]: pass实际上完全等价于：# 首先获得Iterator对象:it = iter([1, 2, 3, 4, 5])# 循环:while True: try: # 获得下一个值: x = next(it) except StopIteration: # 遇到StopIteration就退出循环 break]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python基础三]]></title>
    <url>%2F2019%2F08%2F15%2Fpython%E5%9F%BA%E7%A1%80%E4%B8%89%2F</url>
    <content type="text"><![CDATA[python基础三内容: 函数基本语法及特性 参数与局部变量 返回值 嵌套函数 4.递归 5.匿名函数 6.函数式编程介绍 7.高阶函数 8.内置函数 温故而知新1.集合主要作用： 1.去重 2.关系测试， 交集＼差集＼并集＼反向(对称)差集 1234567891011121314151617181920&gt;&gt;&gt; a = &#123;1,2,3,4&#125;&gt;&gt;&gt; b =&#123;3,4,5,6&#125;&gt;&gt;&gt; a&#123;1, 2, 3, 4&#125;&gt;&gt;&gt; type(a)&lt;class &apos;set&apos;&gt;&gt;&gt;&gt; a.symmetric_difference(b)&#123;1, 2, 5, 6&#125;&gt;&gt;&gt; b.symmetric_difference(a)&#123;1, 2, 5, 6&#125;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; a.difference(b)&#123;1, 2&#125;&gt;&gt;&gt; a.union(b)&#123;1, 2, 3, 4, 5, 6&#125;&gt;&gt;&gt; a.issua.issubset( a.issuperset(&gt;&gt;&gt; a.issubset(b)False 2. 元组 只读列表，只有count, index 2 个方法 作用：如果一些数据不想被人修改， 可以存成元组，比如身份证列表 3. 字典key-value对 特性： 无顺序 去重 查询速度快，比列表快多了 比list占用内存多 为什么会查询速度会快呢？因为他是hash类型的，那什么是hash呢？ 哈希算法将任意长度的二进制值映射为较短的固定长度的二进制值，这个小的二进制值称为哈希值。哈希值是一段数据唯一且极其紧凑的数值表示形式。如果散列一段明文而且哪怕只更改该段落的一个字母，随后的哈希都将产生不同的值。要找到散列为同一个值的两个不同的输入，在计算上是不可能的，所以数据的哈希值可以检验数据的完整性。一般用于快速查找和加密算法 dict会把所有的key变成hash 表，然后将这个表进行排序，这样，你通过data[key]去查data字典中一个key的时候，python会先把这个key hash成一个数字，然后拿这个数字到hash表中看没有这个数字， 如果有，拿到这个key在hash表中的索引，拿到这个索引去与此key对应的value的内存地址那取值就可以了。 4. 字符编码先说python2 py2里默认编码是ascii 文件开头那个编码声明是告诉解释这个代码的程序 以什么编码格式把这段代码读入到内存，因为到了内存里，这段代码其实是以bytes二进制格式存的，不过即使是2进制流，也可以按不同的编码格式转成2进制流，你懂么？ 如果在文件头声明了#*coding:utf-8*_，就可以写中文了， 不声明的话，python在处理这段代码时按ascii，显然会出错， 加了这个声明后，里面的代码就全是utf-8格式了 在有#*coding:utf-8*_的情况下，你在声明变量如果写成name=u”大保健”，那这个字符就是unicode格式，不加这个u,那你声明的字符串就是utf-8格式 utf-8 to gbk怎么转，utf8先decode成unicode,再encode成gbk 再说python3 py3里默认文件编码就是utf-8,所以可以直接写中文，也不需要文件头声明编码了，干的漂亮 你声明的变量默认是unicode编码，不是utf-8, 因为默认即是unicode了（不像在py2里，你想直接声明成unicode还得在变量前加个u）, 此时你想转成gbk的话，直接your_str.encode(“gbk”)即可以 但py3里，你在your_str.encode(“gbk”)时，感觉好像还加了一个动作，就是就是encode的数据变成了bytes里，我擦，这是怎么个情况，因为在py3里，str and bytes做了明确的区分，你可以理解为bytes就是2进制流，你会说，我看到的不是010101这样的2进制呀， 那是因为python为了让你能对数据进行操作而在内存级别又帮你做了一层封装，否则让你直接看到一堆2进制，你能看出哪个字符对应哪段2进制么？什么？自己换算，得了吧，你连超过2位数的数字加减运算都费劲，还还是省省心吧。 那你说，在py2里好像也有bytes呀，是的，不过py2里的bytes只是对str做了个别名(python2里的str就是bytes, py3里的str是unicode)，没有像py3一样给你显示的多出来一层封装，但其实其内部还是封装了的。 这么讲吧， 无论是2还是三， 从硬盘到内存，数据格式都是 010101二进制到–&gt;b’\xe4\xbd\xa0\xe5\xa5\xbd’ bytes类型－－&gt;按照指定编码转成你能看懂的文字 编码应用比较多的场景应该是爬虫了，互联网上很多网站用的编码格式很杂，虽然整体趋向都变成utf-8，但现在还是很杂，所以爬网页时就需要你进行各种编码的转换，不过生活正在变美好，期待一个不需要转码的世界。 最后，编码is a piece of fucking shit, noboby likes it. 1.函数基本语法及特性背景提要 现在老板让你写一个监控程序，监控服务器的系统状况，当cpu＼memory＼disk等指标的使用量超过阀值时即发邮件报警，你掏空了所有的知识量，写出了以下代码 123456789101112131415161718while True： if cpu利用率 &gt; 90%: #发送邮件提醒 连接邮箱服务器 发送邮件 关闭连接 if 硬盘使用空间 &gt; 90%: #发送邮件提醒 连接邮箱服务器 发送邮件 关闭连接 if 内存占用 &gt; 80%: #发送邮件提醒 连接邮箱服务器 发送邮件 关闭连接 上面的代码实现了功能，但即使是邻居老王也看出了端倪，老王亲切的摸了下你家儿子的脸蛋，说，你这个重复代码太多了，每次报警都要重写一段发邮件的代码，太low了，这样干存在2个问题： 1.代码重复过多，一个劲的copy and paste不符合高端程序员的气质 2.如果日后需要修改发邮件的这段代码，比如加入群发功能，那你就需要在所有用到这段代码的地方都修改一遍 你觉得老王说的对，你也不想写重复代码，但又不知道怎么搞，老王好像看出了你的心思，此时他抱起你儿子，笑着说，其实很简单，只需要把重复的代码提取出来，放在一个公共的地方，起个名字，以后谁想用这段代码，就通过这个名字调用就行了，如下 12345678910111213141516def 发送邮件(内容) #发送邮件提醒 连接邮箱服务器 发送邮件 关闭连接 while True： if cpu利用率 &gt; 90%: 发送邮件(&apos;CPU报警&apos;) if 硬盘使用空间 &gt; 90%: 发送邮件(&apos;硬盘报警&apos;) if 内存占用 &gt; 80%: 发送邮件(&apos;内存报警&apos;) 你看着老王写的代码，气势恢宏、磅礴大气，代码里透露着一股内敛的傲气，心想，老王这个人真是不一般，突然对他的背景更感兴趣了，问老王，这些花式玩法你都是怎么知道的？ 老王亲了一口你儿子，捋了捋不存在的胡子，淡淡的讲，“老夫，年少时，师从京西沙河淫魔银角大王 ”， 你一听“银角大王”这几个字，不由的娇躯一震，心想，真nb,怪不得代码写的这么6, 这“银角大王”当年在江湖上可是数得着的响当当的名字，只可惜后期纵欲过度，卒于公元2016年， 真是可惜了，只留下其哥哥孤守当年兄弟俩一起打下来的江山。 此时你看着的老王离开的身影，感觉你儿子跟他越来越像了。。。 函数是什么?函数一词来源于数学，但编程中的「函数」概念，与数学中的函数是有很大不同的，具体区别，我们后面会讲，编程中的函数在英文中也有很多不同的叫法。在BASIC中叫做subroutine(子过程或子程序)，在Pascal中叫做procedure(过程)和function，在C中只有function，在Java里面叫做method。 定义: 函数是指将一组语句的集合通过一个名字(函数名)封装起来，要想执行这个函数，只需调用其函数名即可 特性: 减少重复代码 使程序变的可扩展 使程序变得易维护 语法定义1234def sayhi():#函数名 print(&quot;Hello, I&apos;m nobody!&quot;) sayhi() #调用函数 可以带参数 12345678910111213#下面这段代码a,b = 5,8c = a**bprint(c) #改成用函数写def calc(x,y): res = x**y return res #返回函数执行结果 c = calc(a,b) ＃结果赋值给c变量print(c) 2.函数参数与局部变量形参变量只有在被调用时才分配内存单元，在调用结束时，即刻释放所分配的内存单元。因此，形参只在函数内部有效。函数调用结束返回主调用函数后则不能再使用该形参变量 实参可以是常量、变量、表达式、函数等，无论实参是何种类型的量，在进行函数调用时，它们都必须有确定的值，以便把这些值传送给形参。因此应预先用赋值，输入等办法使参数获得确定值 默认参数看下面代码 12345678910def stu_register(name,age,country,course): print(&quot;----注册学生信息------&quot;) print(&quot;姓名:&quot;,name) print(&quot;age:&quot;,age) print(&quot;国籍:&quot;,country) print(&quot;课程:&quot;,course) stu_register(&quot;王山炮&quot;,22,&quot;CN&quot;,&quot;python_devops&quot;)stu_register(&quot;张叫春&quot;,21,&quot;CN&quot;,&quot;linux&quot;)stu_register(&quot;刘老根&quot;,25,&quot;CN&quot;,&quot;linux&quot;) 发现 country 这个参数 基本都 是”CN”, 就像我们在网站上注册用户，像国籍这种信息，你不填写，默认就会是 中国， 这就是通过默认参数实现的，把country变成默认参数非常简单 1def stu_register(name,age,course,country=&quot;CN&quot;): 这样，这个参数在调用时不指定，那默认就是CN，指定了的话，就用你指定的值。 另外，你可能注意到了，在把country变成默认参数后，我同时把它的位置移到了最后面，为什么呢？ 关键参数正常情况下，给函数传参数要按顺序，不想按顺序就可以用关键参数，只需指定参数名即可，但记住一个要求就是，关键参数必须放在位置参数之后。 1stu_register(age=22,name=&apos;alex&apos;,course=&quot;python&quot;,) ##### 非固定参数若你的函数在定义时不确定用户想传入多少个参数，就可以使用非固定参数 12345678910def stu_register(name,age,*args): # *args 会把多传入的参数变成一个元组形式 print(name,age,args) stu_register(&quot;Alex&quot;,22)#输出#Alex 22 () #后面这个()就是args,只是因为没传值,所以为空 stu_register(&quot;Jack&quot;,32,&quot;CN&quot;,&quot;Python&quot;)#输出# Jack 32 (&apos;CN&apos;, &apos;Python&apos;) 还可以有一个**kwargs 12345678910def stu_register(name,age,*args,**kwargs): # *kwargs 会把多传入的参数变成一个dict形式 print(name,age,args,kwargs) stu_register(&quot;Alex&quot;,22)#输出#Alex 22 () &#123;&#125;#后面这个&#123;&#125;就是kwargs,只是因为没传值,所以为空 stu_register(&quot;Jack&quot;,32,&quot;CN&quot;,&quot;Python&quot;,sex=&quot;Male&quot;,province=&quot;ShanDong&quot;)#输出# Jack 32 (&apos;CN&apos;, &apos;Python&apos;) &#123;&apos;province&apos;: &apos;ShanDong&apos;, &apos;sex&apos;: &apos;Male&apos;&#125; 局部变量1234567891011name = &quot;Alex Li&quot; def change_name(name): print(&quot;before change:&quot;,name) name = &quot;金角大王,一个有Tesla的男人&quot; print(&quot;after change&quot;, name) change_name(name) print(&quot;在外面看看name改了么?&quot;,name) 输出 123before change: Alex Liafter change 金角大王,一个有Tesla的男人在外面看看name改了么? Alex Li 全局与局部变量在子程序中定义的变量称为局部变量，在程序的一开始定义的变量称为全局变量。 全局变量作用域是整个程序，局部变量作用域是定义该变量的子程序。 当全局变量与局部变量同名时： 在定义局部变量的子程序内，局部变量起作用；在其它地方全局变量起作用。 3.返回值 要想获取函数的执行结果，就可以用return语句把结果返回 注意: 1.函数在执行过程中只要遇到return语句，就会停止执行并返回结果，so 也可以理解为 return 语句代表着函数的结束 2.如果未在函数中指定return,那这个函数的返回值为None 嵌套函数 123456789101112131415161718192021name = &quot;Alex&quot; def change_name(): name = &quot;Alex2&quot; def change_name2(): name = &quot;Alex3&quot; print(&quot;第3层打印&quot;,name) change_name2() #调用内层函数 print(&quot;第2层打印&quot;,name) change_name()print(&quot;最外层打印&quot;,name)``` 此时，在最外层调用change_name2()会出现什么效果？没错， 出错了， 为什么呢？##### 递归在函数内部，可以调用其他函数。如果一个函数在内部调用自身本身，这个函数就是递归函数。 def calc(n): print(n) if int(n/2) ==0: return n return calc(int(n/2)) calc(10) 输出：10521 1234567891011递归特性:1. 必须有一个明确的结束条件2. 每次进入更深一层递归时，问题规模相比上次递归都应有所减少3.递归效率不高，递归层次过多会导致栈溢出（在计算机中，函数调用是通过栈（stack）这种数据结构实现的，每当进入一个函数调用，栈就会加一层栈帧，每当函数返回，栈就会减一层栈帧。由于栈的大小不是无限的，所以，递归调用的次数过多，会导致栈溢出）堆栈扫盲http://www.cnblogs.com/lln7777/archive/2012/03/14/2396164.html 递归函数实际应用案例，二分查找 data = [1, 3, 6, 7, 9, 12, 14, 16, 17, 18, 20, 21, 22, 23, 30, 32, 33, 35] def binary_search(dataset,find_num): print(dataset) if len(dataset) &gt;1: mid = int(len(dataset)/2) if dataset[mid] == find_num: #find it print(&quot;找到数字&quot;,dataset[mid]) elif dataset[mid] &gt; find_num :# 找的数在mid左面 print(&quot;\033[31;1m找的数在mid[%s]左面\033[0m&quot; % dataset[mid]) return binary_search(dataset[0:mid], find_num) else:# 找的数在mid右面 print(&quot;\033[32;1m找的数在mid[%s]右面\033[0m&quot; % dataset[mid]) return binary_search(dataset[mid+1:],find_num) else: if dataset[0] == find_num: #find it print(&quot;找到数字啦&quot;,dataset[0]) else: print(&quot;没的分了,要找的数字[%s]不在列表里&quot; % find_num)binary_search(data,66) 12345678##### 5. 匿名函数 匿名函数就是不需要显式的指定函数关键字lambda表示匿名函数，冒号前面的x表示函数参数。匿名函数有个限制，就是只能有一个表达式，不用写return，返回值就是该表达式的结果。用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函 #这段代码def calc(n): return n**nprint(calc(10)) #换成匿名函数calc = lambda n:n**nprint(calc(10)) 1你也许会说，用上这个东西没感觉有毛方便呀， 。。。。呵呵，如果是这么用，确实没毛线改进，不过匿名函数主要是和其它函数搭配使用的呢，如下 res = map(lambda x:x**2,[1,5,7,4,8])for i in res: print(i)输出 125491664 123456789101112131415##### 6.函数式编程介绍 函数是Python内建支持的一种封装，我们通过把大段代码拆成函数，通过一层一层的函数调用，就可以把复杂任务分解成简单的任务，这种分解可以称之为面向过程的程序设计。函数就是面向过程的程序设计的基本单元。函数式编程中的函数这个术语不是指计算机中的函数（实际上是Subroutine），而是指数学中的函数，即自变量的映射。也就是说一个函数的值仅决定于函数参数的值，不依赖其他状态。比如sqrt(x)函数计算x的平方根，只要x不变，不论什么时候调用，调用几次，值都是不变的。 Python对函数式编程提供部分支持。由于Python允许使用变量，因此，Python不是纯函数式编程语言。一、定义简单说，&quot;函数式编程&quot;是一种&quot;编程范式&quot;（programming paradigm），也就是如何编写程序的方法论。主要思想是把运算过程尽量写成一系列嵌套的函数调用。举例来说，现在有这样一个数学表达式： (1 + 2) * 3 - 4 1传统的过程式编程，可能这样写： var a = 1 + 2; var b = a * 3; var c = b - 4; 1函数式编程要求使用函数，我们可以把运算过程定义为不同的函数，然后写成下面这样： var result = subtract(multiply(add(1,2), 3), 4); 1这段代码再演进以下，可以变成这样 add(1,2).multiply(3).subtract(4) 12345因此，函数式编程的代码更容易理解。要想学好函数式编程，不要玩py,玩Erlang,Haskell#### 7.高阶函数变量可以指向函数，函数的参数能接收变量，那么一个函数就可以接收另一个函数作为参数，这种函数就称之为高阶函数。 def add(x,y,f): return f(x) + f(y) res = add(3,-6,abs)print(res) 12#### 8.内置函数内置参数详解 https://docs.python.org/3/library/functions.html?highlight=built#ascii #compilef = open(“函数递归.py”)data =compile(f.read(),’’,’exec’)exec(data) #printmsg = “又回到最初的起点”f = open(“tofile”,”w”)print(msg,”记忆中你青涩的脸”,sep=”|”,end=””,file=f) #slicea = range(20)pattern = slice(3,8,2)for i in a[pattern]: #等于a[3:8:2]print(i)## #memoryview #usage: #&gt;&gt;&gt; memoryview(b’abcd’) #&lt;memory at 0x104069648&gt; #在进行切片并赋值数据时，不需要重新copy原列表数据，可以直接映射原数据内存，import timefor n in (100000, 200000, 300000, 400000): data = b’x’*n start = time.time() b = data while b: b = b[1:] print(‘bytes’, n, time.time()-start) for n in (100000, 200000, 300000, 400000): data = b’x’*n start = time.time() b = memoryview(data) while b: b = b[1:] print(‘memoryview’, n, time.time()-start) 1234567891011121314```作业1,Alex Li,22,13651054608,IT,2013-04-01现需要对这个员工信息文件，实现增删改查操作可进行模糊查询，语法至少支持下面3种: select name,age from staff_table where age &gt; 22 select * from staff_table where dept = &quot;IT&quot; select * from staff_table where enroll_date like &quot;2013&quot;查到的信息，打印后，最后面还要显示查到的条数 可创建新员工纪录，以phone做唯一键，staff_id需自增可删除指定员工信息纪录，输入员工id，即可删除可修改员工信息，语法如下: UPDATE staff_table SET dept=&quot;Market&quot; WHERE where dept = &quot;IT&quot;]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python封装]]></title>
    <url>%2F2019%2F08%2F14%2Fpython%E5%B0%81%E8%A3%85%2F</url>
    <content type="text"><![CDATA[python基础之封装一、什么是封装在程序设计中，封装是对具体对象的一种抽象，即将某些部分隐藏起来，在程序外部看不到，其含义是其他程序无法调用。 要了解封装，离不开“私有化”，就是将类或者是函数中的某些属性限制在某个区域之内，外部无法调用。 二、为什么要封装封装数据的主要原因是：保护隐私（把不想别人知道的东西封装起来） 封装方法的主要原因是：隔离复杂度（比如：电视机，我们看见的就是一个黑匣子，其实里面有很多电器元件，对于用户来说，我们不需要清楚里面都有些元件，电视机把那些电器元件封装在黑匣子里，提供给用户的只是几个按钮接口，通过按钮就对实现对电视机的操作。） 提示：在编程语言里，对外提供的接口（接口可理解为一个入口），就是函数，称为接口函数，这与接口的概念还一样，接口代表一组接口函数的集合体。 三、封装分为两个层面封装其实分为两个层面，但无论哪种层面的封装，都要对外界提供好访问你内部隐藏内容的接口（接口可以理解为入口，有了这个入口，使用者无需且不能够直接访问到内部隐藏的细节，只能走接口，并且我们可以在接口的实现上附加更多的处理逻辑，从而严格控制使用者的访问） 第一个层面的封装（什么都不用做）：创建类和对象会分别创建二者的名称空间，我们只能用类名.或者obj.的方式去访问里面的名字，这本身就是一种封装。 12345print(m1.brand) #实例化对象（m1.）print(motor_vehicle.tag) #类名（motor_vehicle.）-------------输出结果--------------春风fuel oil 注意:对于这一层面的封装(隐藏),类名.和实例名.就是访问隐藏属性的接口 第二个层面的封装：类中把某些属性和方法隐藏起来(或者说定义成私有的)，只在类的内部使用、外部无法访问，或者留下少量接口（函数）供外部访问。 Python中私有化的方法也比较简单，即在准备私有化的属性（包括方法、数据）名字前面加两个下划线即可。 类中所有双下划线开头的名称如x都会自动变形成：_类名x的形式： 12345678class A: __N=0 #类的数据属性就应该是共享的,但是语法上是可以把类的数据属性设置成私有的如__N,会变形为_A__N def __init__(self): self.__X=10 #变形为self._A__X def __foo(self): #变形为_A__foo print(&apos;from A&apos;) def bar(self): self.__foo() #只有在类内部才可以通过__foo的形式访问到. 这种自动变形的特点： 1、类中定义的x只能在内部使用，如self.x，引用的就是变形的结果。 2、这种变形其实正是针对外部的变形，在外部是无法通过__x这个名字访问到的。 3、在子类定义的x不会覆盖在父类定义的x，因为子类中变形成了：_子类名x,而父类中变形成了：_父类名x，即双下滑线开头的属性在继承给子类时，子类是无法覆盖的。 注意：对于这一层面的封装（隐藏），我们需要在类中定义一个函数（接口函数）在它内部访问被隐藏的属性，然后外部就可以使用了 这种变形需要注意的问题是： 1、这种机制也并没有真正意义上限制我们从外部直接访问属性，知道了类名和属性名就可以拼出名字：_类名属性，然后就可以访问了，如a._AN 12345678a = A()print(a._A__N)print(a._A__X)print(A._A__N)--------输出结果--------0100 2、变形的过程只在类的定义是发生一次,在定义后的赋值操作，不会变形 1234567a = A() #实例化对象aprint(a.__dict__) #打印变形的内容a.__Y = 20 #新增Y的值，此时加__不会变形print(a.__dict__) #打印变形的内容---------输出结果----------&#123;&apos;_A__X&apos;: 10&#125;&#123;&apos;_A__X&apos;: 10, &apos;__Y&apos;: 20&#125; #发现后面的Y并没有变形 3、在继承中，父类如果不想让子类覆盖自己的方法，可以将方法定义为私有的 1234567891011121314class A: #这是正常情况 def fa(self): print(&quot;from A&quot;) def test(self): self.fa() class B(A): def fa(self): print(&quot;from B&quot;) b = B()b.test()--------输出结果----------from B 看一下把fa被定义成私有的情况： 1234567891011121314class A: #把fa定义成私有的，即__fa def __fa(self): #在定义时就变形为_A__fa print(&quot;from A&quot;) def test(self): self.__fa() #只会与自己所在的类为准,即调用_A__fa class B(A): def __fa(self): #b调用的是test，跟这个没关系 print(&quot;from B&quot;) b = B()b.test()-------输出结果---------from A 特性(property)1、什么是特性property property是一种特殊的属性，访问它时会执行一段功能（函数）然后返回值（就是一个装饰器） 注意：被property装饰的属性会优先于对象的属性被使用，而被propery装饰的属性,分成三种：property、被装饰的函数名.setter、被装饰的函数名.deleter（都是以装饰器的形式）。 1234567891011121314151617181920212223class room: #定义一个房间的类 def __init__(self,length,width,high): self.length = length #房间的长 self.width = width #房间的宽 self.high = high #房间的高 @property def area(self): #求房间的平方的功能 return self.length * self.width #房间的面积就是：长x宽 @property def perimeter(self): #求房间的周长的功能 return 2 * (self.length + self.width) #公式为：（长 + 宽）x 2 @property def volume(self): #求房间的体积的功能 return self.length * self.width * self.high #公式为：长 x 宽 x 高 r1 = room(2,3,4) #实例化一个对象r1print(&quot;r1.area：&quot;,r1.area) #可以像访问数据属性一样去访问area,会触发一个函数的执行,动态计算出一个值print(&quot;r1.perimeter：&quot;,r1.perimeter) #同上，就不用像调用绑定方法一样，还得加括号，才能运行print(&quot;r1.volume：&quot;,r1.volume) #同上，就像是把运算过程封装到一个函数内部，我们不管过程，只要有结果就行------------输出结果---------------r1.area： 6r1.perimeter： 10r1.volume： 24 注意：此时的特性arear、perimeter和volume不能被赋值。 1234567r1.area = 8 #为特性area赋值r1.perimeter = 14 #为特性perimeter赋值r1.volume = 24 #为特性volume赋值&apos;&apos;&apos;抛出异常： r1.area = 8 #第一个就抛异常了，后面的也一样AttributeError: can&apos;t set attribute 2、为什么要用property 将一个类的函数定义成特性以后，对象再去使用的时候obj.name,根本无法察觉自己的name是执行了一个函数然后计算出来的，这种特性的使用方式遵循了统一访问的原则。 1234567891011121314151617181920212223242526272829303132class people: #定义一个人的类 def __init__(self,name,sex): self.name = name self.sex = sex #p1.sex = &quot;male&quot;,遇到property，优先用property @property #查看sex的值 def sex(self): return self.__sex #返回正真存值的地方 @sex.setter #修改sex的值 def sex(self,value): if not isinstance(value,str): #在设定值之前进行类型检查 raise TypeError(&quot;性别必须是字符串类型&quot;) #不是str类型时，主动抛出异常 self.__sex = value #类型正确的时候，直接修改__sex的值，这是值正真存放的地方 #这里sex前加&quot;__&quot;,对sex变形，隐藏。 @sex.deleter #删除sex def sex(self): del self.__sex p1 = people(&quot;egon&quot;,&quot;male&quot;) #实例化对象p1print(p1.sex) #查看p1的sex，此时要注意self.sex的优先级p1.sex = &quot;female&quot; #修改sex的值print(p1.sex) #查看修改后p1的sexprint(p1.__dict__) #查看p1的名称空间，此时里面有sexdel p1.sex #删除p1的sexprint(p1.__dict__) #查看p1的名称空间，此时发现里面已经没有sex了-------------------输出结果--------------------malefemale&#123;&apos;name&apos;: &apos;egon&apos;, &apos;_people__sex&apos;: &apos;female&apos;&#125;&#123;&apos;name&apos;: &apos;egon&apos;&#125; python并没有在语法上把它们三个内建到自己的class机制中，在C++里一般会将所有的所有的数据都设置为私有的，然后提供set和get方法（接口）去设置和获取，在python中通过property方法可以实现。 五、封装与扩展性 封装在于明确区分内外，使得类实现者可以修改封装内的东西而不影响外部调用者的代码；而外部使用用者只知道一个接口(函数)，只要接口（函数）名、参数不变，使用者的代码永远无需改变。这就提供一个良好的合作基础——或者说，只要接口这个基础约定不变，则代码改变不足为虑。 123456789101112#类的设计者class room: #定义一个房间的类 def __init__(self,name,owner,length,width,high): self.name = name self.owner = owner self.__length = length #房间的长 self.__width = width #房间的宽 self.__high = high #房间的高 @property def area(self): #求房间的平方的功能 return self.__length * self.__width #对外提供的接口，隐藏了内部的实现细节，\ # 此时我们想求的是房间的面积就是：长x宽 实例化对象通过接口，调用相关属性得到想要的值： 12345#类的使用者r1 = room(&quot;客厅&quot;,&quot;michael&quot;,20,30,9) #实例化一个对象r1print(r1.area) #通过接口使用（area），使用者得到了客厅的面积-------------输出结果--------------600 #得到了客厅的面积 扩展原有的代码，使功能增加： 1234567891011121314#类的设计者，轻松的扩展了功能，而类的使用者完全不需要改变自己的代码class room: #定义一个房间的类 def __init__(self,name,owner,length,width,high): self.name = name #房间名 self.owner = owner #房子的主人 self.__length = length #房间的长 self.__width = width #房间的宽 self.__high = high #房间的高 @property def area(self): #对外提供的接口，隐藏内部实现 return self.__length * self.__width,\ self.__length * self.__width * self.__high #此时我们增加了求体积, # 内部逻辑变了,只需增加这行代码就能简单实现,而且外部调用感知不到,仍然使 # 用该方法，但是功能已经增加了 对于类的使用者，仍然在调用area接口的人来说，根本无需改动自己的代码，就可以用上新功能： 12345#类的使用者r1 = room(&quot;客厅&quot;,&quot;michael&quot;,20,30,9) #实例化一个对象r1print(r1.area) #通过接口使用（area），使用者得到了客厅的面积--------------输出结果---------------(600, 5400) #得到了新增的功能的值]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python基础二]]></title>
    <url>%2F2019%2F08%2F14%2Fpython%E5%9F%BA%E7%A1%80%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[python基础二内容： 1.列表、元组操作 2.字符串操作 3.字典操作 4.集合操作 5.文件操作 6.字符编码与转码 1.列表、元组操作列表是我们最常用的数据类型之一，通过列表可以对数据实现最方便的有存储、修改等操作。 定义列表 1names = [&apos;Alex&apos;,&quot;Tenglan&quot;,&apos;Eric&apos;] 通过下标访问列表中的元素，下标从0开始计数 12345678&gt;&gt;&gt; names[0]&apos;Alex&apos;&gt;&gt;&gt; names[2]&apos;Eric&apos;&gt;&gt;&gt; names[-1]&apos;Eric&apos;&gt;&gt;&gt; names[-2] #还可以倒着取&apos;Tenglan&apos; 切片:取多个元素 1234567891011121314151617&gt;&gt;&gt; names = [&quot;Alex&quot;,&quot;Tenglan&quot;,&quot;Eric&quot;,&quot;Rain&quot;,&quot;Tom&quot;,&quot;Amy&quot;]&gt;&gt;&gt; names[1:4] #取下标1至下标4之间的数字，包括1，不包括4[&apos;Tenglan&apos;, &apos;Eric&apos;, &apos;Rain&apos;]&gt;&gt;&gt; names[1:-1] #取下标1至-1的值，不包括-1[&apos;Tenglan&apos;, &apos;Eric&apos;, &apos;Rain&apos;, &apos;Tom&apos;]&gt;&gt;&gt; names[0:3] [&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Eric&apos;]&gt;&gt;&gt; names[:3] #如果是从头开始取，0可以忽略，跟上句效果一样[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Eric&apos;]&gt;&gt;&gt; names[3:] #如果想取最后一个，必须不能写-1，只能这么写[&apos;Rain&apos;, &apos;Tom&apos;, &apos;Amy&apos;] &gt;&gt;&gt; names[3:-1] #这样-1就不会被包含了[&apos;Rain&apos;, &apos;Tom&apos;]&gt;&gt;&gt; names[0::2] #后面的2是代表，每隔一个元素，就取一个[&apos;Alex&apos;, &apos;Eric&apos;, &apos;Tom&apos;] &gt;&gt;&gt; names[::2] #和上句效果一样[&apos;Alex&apos;, &apos;Eric&apos;, &apos;Tom&apos;] 追加1234567&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Eric&apos;, &apos;Rain&apos;, &apos;Tom&apos;, &apos;Amy&apos;]&gt;&gt;&gt; names.append(&quot;我是新来的&quot;)&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Eric&apos;, &apos;Rain&apos;, &apos;Tom&apos;, &apos;Amy&apos;, &apos;我是新来的&apos;]append从未尾插入 插入1234567891011&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Eric&apos;, &apos;Rain&apos;, &apos;Tom&apos;, &apos;Amy&apos;, &apos;我是新来的&apos;]&gt;&gt;&gt; names.insert(2,&quot;强行从Eric前面插入&quot;)&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;强行从Eric前面插入&apos;, &apos;Eric&apos;, &apos;Rain&apos;, &apos;Tom&apos;, &apos;Amy&apos;, &apos;我是新来的&apos;]&gt;&gt;&gt; names.insert(5,&quot;从eric后面插入试试新姿势&quot;)&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;强行从Eric前面插入&apos;, &apos;Eric&apos;, &apos;Rain&apos;, &apos;从eric后面插入试试新姿势&apos;, &apos;Tom&apos;, &apos;Amy&apos;, &apos;我是新来的&apos;]insert从指定的数字后面插入 修改12345&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;强行从Eric前面插入&apos;, &apos;Eric&apos;, &apos;Rain&apos;, &apos;从eric后面插入试试新姿势&apos;, &apos;Tom&apos;, &apos;Amy&apos;, &apos;我是新来的&apos;]&gt;&gt;&gt; names[2] = &quot;该换人了&quot;&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;该换人了&apos;, &apos;Eric&apos;, &apos;Rain&apos;, &apos;从eric后面插入试试新姿势&apos;, &apos;Tom&apos;, &apos;Amy&apos;, &apos;我是新来的&apos;] 删除1234567891011121314&gt;&gt;&gt; del names[2] &gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Eric&apos;, &apos;Rain&apos;, &apos;从eric后面插入试试新姿势&apos;, &apos;Tom&apos;, &apos;Amy&apos;, &apos;我是新来的&apos;]&gt;&gt;&gt; del names[4]&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Eric&apos;, &apos;Rain&apos;, &apos;Tom&apos;, &apos;Amy&apos;, &apos;我是新来的&apos;]&gt;&gt;&gt; &gt;&gt;&gt; names.remove(&quot;Eric&quot;) #删除指定元素&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Rain&apos;, &apos;Tom&apos;, &apos;Amy&apos;, &apos;我是新来的&apos;]&gt;&gt;&gt; names.pop() #删除列表最后一个值 &apos;我是新来的&apos;&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Rain&apos;, &apos;Tom&apos;, &apos;Amy&apos;] 扩展12345678&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Rain&apos;, &apos;Tom&apos;, &apos;Amy&apos;]&gt;&gt;&gt; b = [1,2,3]&gt;&gt;&gt; names.extend(b)&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Rain&apos;, &apos;Tom&apos;, &apos;Amy&apos;, 1, 2, 3]extend将两个列表合并 拷贝123456&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Rain&apos;, &apos;Tom&apos;, &apos;Amy&apos;, 1, 2, 3]&gt;&gt;&gt; name_copy = names.copy()&gt;&gt;&gt; name_copy[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Rain&apos;, &apos;Tom&apos;, &apos;Amy&apos;, 1, 2, 3] 统计12345&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Amy&apos;, &apos;Tom&apos;, &apos;Amy&apos;, 1, 2, 3]&gt;&gt;&gt; names.count(&quot;Amy&quot;)2count统计重复出现的元素 排序&amp;翻转123456789101112131415161718&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Tenglan&apos;, &apos;Amy&apos;, &apos;Tom&apos;, &apos;Amy&apos;, 1, 2, 3]&gt;&gt;&gt; names.sort() #排序Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: unorderable types: int() &lt; str() #3.0里不同数据类型不能放在一起排序了，擦&gt;&gt;&gt; names[-3] = &apos;1&apos;&gt;&gt;&gt; names[-2] = &apos;2&apos;&gt;&gt;&gt; names[-1] = &apos;3&apos;&gt;&gt;&gt; names[&apos;Alex&apos;, &apos;Amy&apos;, &apos;Amy&apos;, &apos;Tenglan&apos;, &apos;Tom&apos;, &apos;1&apos;, &apos;2&apos;, &apos;3&apos;]&gt;&gt;&gt; names.sort()&gt;&gt;&gt; names[&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;Alex&apos;, &apos;Amy&apos;, &apos;Amy&apos;, &apos;Tenglan&apos;, &apos;Tom&apos;]&gt;&gt;&gt; names.reverse() #反转&gt;&gt;&gt; names[&apos;Tom&apos;, &apos;Tenglan&apos;, &apos;Amy&apos;, &apos;Amy&apos;, &apos;Alex&apos;, &apos;3&apos;, &apos;2&apos;, &apos;1&apos;] 获取下标1234&gt;&gt;&gt; names[&apos;Tom&apos;, &apos;Tenglan&apos;, &apos;Amy&apos;, &apos;Amy&apos;, &apos;Alex&apos;, &apos;3&apos;, &apos;2&apos;, &apos;1&apos;]&gt;&gt;&gt; names.index(&quot;Amy&quot;)2 #只返回找到的第一个下标 元组元组其实跟列表差不多，也是存一组数，只不是它一旦创建，便不能再修改，所以又叫只读列表 语法1names = (&quot;alex&quot;,&quot;jack&quot;,&quot;eric&quot;) 它只有2个方法，一个是count,一个是index，完毕 2.字符串操作特性:不可修改 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768name.capitalize() 首字母大写name.casefold() 大写全部变小写name.center(50,&quot;-&quot;) 输出 &apos;---------------------Alex Li----------------------&apos;name.count(&apos;lex&apos;) 统计 lex出现次数name.encode() 将字符串编码成bytes格式name.endswith(&quot;Li&quot;) 判断字符串是否以 Li结尾 &quot;Alex\tLi&quot;.expandtabs(10) 输出&apos;Alex Li&apos;， 将\t转换成多长的空格 name.find(&apos;A&apos;) 查找A,找到返回其索引， 找不到返回-1 format : &gt;&gt;&gt; msg = &quot;my name is &#123;&#125;, and age is &#123;&#125;&quot; &gt;&gt;&gt; msg.format(&quot;alex&quot;,22) &apos;my name is alex, and age is 22&apos; &gt;&gt;&gt; msg = &quot;my name is &#123;1&#125;, and age is &#123;0&#125;&quot; &gt;&gt;&gt; msg.format(&quot;alex&quot;,22) &apos;my name is 22, and age is alex&apos; &gt;&gt;&gt; msg = &quot;my name is &#123;name&#125;, and age is &#123;age&#125;&quot; &gt;&gt;&gt; msg.format(age=22,name=&quot;ale&quot;) &apos;my name is ale, and age is 22&apos;format_map &gt;&gt;&gt; msg.format_map(&#123;&apos;name&apos;:&apos;alex&apos;,&apos;age&apos;:22&#125;) &apos;my name is alex, and age is 22&apos;msg.index(&apos;a&apos;) 返回a所在字符串的索引&apos;9aA&apos;.isalnum() True 方法检测字符串是否由字母和数字组成。&apos;9&apos;.isdigit() 是否整数name.isnumeric 方法检测字符串是否只由数字组成。这种方法是只针对unicode对象name.isprintable 检查文本中的所有字符是否都可打印：name.isspace 方法检测字符串是否只由空格组成name.istitle 方法检测字符串中所有的单词拼写首字母是否为大写，且其他字母为小写。name.isupper 方法检测字符串中所有的字母是否都为大写 &quot;|&quot;.join([&apos;alex&apos;,&apos;jack&apos;,&apos;rain&apos;])&apos;alex|jack|rain&apos;maketrans &gt;&gt;&gt; intab = &quot;aeiou&quot; #This is the string having actual characters. &gt;&gt;&gt; outtab = &quot;12345&quot; #This is the string having corresponding mapping character &gt;&gt;&gt; trantab = str.maketrans(intab, outtab) &gt;&gt;&gt; &gt;&gt;&gt; str = &quot;this is string example....wow!!!&quot; &gt;&gt;&gt; str.translate(trantab) &apos;th3s 3s str3ng 2x1mpl2....w4w!!!&apos; msg.partition(&apos;is&apos;) 输出 (&apos;my name &apos;, &apos;is&apos;, &apos; &#123;name&#125;, and age is &#123;age&#125;&apos;) &gt;&gt;&gt; &quot;alex li, chinese name is lijie&quot;.replace(&quot;li&quot;,&quot;LI&quot;,1) &apos;alex LI, chinese name is lijie&apos; msg.swapcase 大小写互换 &gt;&gt;&gt; msg.zfill(40)&apos;00000my name is &#123;name&#125;, and age is &#123;age&#125;&apos;&gt;&gt;&gt; n4.ljust(40,&quot;-&quot;)&apos;Hello 2orld-----------------------------&apos;&gt;&gt;&gt; n4.rjust(40,&quot;-&quot;)&apos;-----------------------------Hello 2orld&apos;&gt;&gt;&gt; b=&quot;ddefdsdff_哈哈&quot; &gt;&gt;&gt; b.isidentifier() #检测一段字符串可否被当作标志符，即是否符合变量命名规则True 3. 字典操作字典一种key - value 的数据类型，使用就像我们上学用的字典，通过笔划、字母来查对应页的详细内容。 语法: 12345info = &#123; &apos;stu1101&apos;: &quot;TengLan Wu&quot;, &apos;stu1102&apos;: &quot;LongZe Luola&quot;, &apos;stu1103&apos;: &quot;XiaoZe Maliya&quot;,&#125; 字典的特性： dict是无序的 key必须是唯一的,so 天生去重 增加123&gt;&gt;&gt; info[&quot;stu1104&quot;] = &quot;苍井空&quot;&gt;&gt;&gt; info&#123;&apos;stu1102&apos;: &apos;LongZe Luola&apos;, &apos;stu1104&apos;: &apos;苍井空&apos;, &apos;stu1103&apos;: &apos;XiaoZe Maliya&apos;, &apos;stu1101&apos;: &apos;TengLan Wu&apos;&#125; 修改123&gt;&gt;&gt; info[&apos;stu1101&apos;] = &quot;武藤兰&quot;&gt;&gt;&gt; info&#123;&apos;stu1102&apos;: &apos;LongZe Luola&apos;, &apos;stu1103&apos;: &apos;XiaoZe Maliya&apos;, &apos;stu1101&apos;: &apos;武藤兰&apos;&#125; 删除12345678910111213141516171819&gt;&gt;&gt; info&#123;&apos;stu1102&apos;: &apos;LongZe Luola&apos;, &apos;stu1103&apos;: &apos;XiaoZe Maliya&apos;, &apos;stu1101&apos;: &apos;武藤兰&apos;&#125;&gt;&gt;&gt; info.pop(&quot;stu1101&quot;) #标准删除姿势&apos;武藤兰&apos;&gt;&gt;&gt; info&#123;&apos;stu1102&apos;: &apos;LongZe Luola&apos;, &apos;stu1103&apos;: &apos;XiaoZe Maliya&apos;&#125;&gt;&gt;&gt; del info[&apos;stu1103&apos;] #换个姿势删除&gt;&gt;&gt; info&#123;&apos;stu1102&apos;: &apos;LongZe Luola&apos;&#125;&gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; info = &#123;&apos;stu1102&apos;: &apos;LongZe Luola&apos;, &apos;stu1103&apos;: &apos;XiaoZe Maliya&apos;&#125;&gt;&gt;&gt; info&#123;&apos;stu1102&apos;: &apos;LongZe Luola&apos;, &apos;stu1103&apos;: &apos;XiaoZe Maliya&apos;&#125; #随机删除&gt;&gt;&gt; info.popitem()(&apos;stu1102&apos;, &apos;LongZe Luola&apos;)&gt;&gt;&gt; info&#123;&apos;stu1103&apos;: &apos;XiaoZe Maliya&apos;&#125; 查找123456789101112&gt;&gt;&gt; info = &#123;&apos;stu1102&apos;: &apos;LongZe Luola&apos;, &apos;stu1103&apos;: &apos;XiaoZe Maliya&apos;&#125;&gt;&gt;&gt; &gt;&gt;&gt; &quot;stu1102&quot; in info #标准用法True&gt;&gt;&gt; info.get(&quot;stu1102&quot;) #获取&apos;LongZe Luola&apos;&gt;&gt;&gt; info[&quot;stu1102&quot;] #同上，但是看下面&apos;LongZe Luola&apos;&gt;&gt;&gt; info[&quot;stu1105&quot;] #如果一个key不存在，就报错，get不会，不存在只返回NoneTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;KeyError: &apos;stu1105&apos; 多级字典嵌套及操作12345678910111213141516171819av_catalog = &#123; &quot;欧美&quot;:&#123; &quot;www.youporn.com&quot;: [&quot;很多免费的,世界最大的&quot;,&quot;质量一般&quot;], &quot;www.pornhub.com&quot;: [&quot;很多免费的,也很大&quot;,&quot;质量比yourporn高点&quot;], &quot;letmedothistoyou.com&quot;: [&quot;多是自拍,高质量图片很多&quot;,&quot;资源不多,更新慢&quot;], &quot;x-art.com&quot;:[&quot;质量很高,真的很高&quot;,&quot;全部收费,屌比请绕过&quot;] &#125;, &quot;日韩&quot;:&#123; &quot;tokyo-hot&quot;:[&quot;质量怎样不清楚,个人已经不喜欢日韩范了&quot;,&quot;听说是收费的&quot;] &#125;, &quot;大陆&quot;:&#123; &quot;1024&quot;:[&quot;全部免费,真好,好人一生平安&quot;,&quot;服务器在国外,慢&quot;] &#125;&#125;av_catalog[&quot;大陆&quot;][&quot;1024&quot;][1] += &quot;,可以用爬虫爬下来&quot;print(av_catalog[&quot;大陆&quot;][&quot;1024&quot;])#ouput [&apos;全部免费,真好,好人一生平安&apos;, &apos;服务器在国外,慢,可以用爬虫爬下来&apos;] 其它姿势123456789101112131415161718192021222324252627282930313233343536#values&gt;&gt;&gt; info.values()dict_values([&apos;LongZe Luola&apos;, &apos;XiaoZe Maliya&apos;])#keys&gt;&gt;&gt; info.keys()dict_keys([&apos;stu1102&apos;, &apos;stu1103&apos;])#setdefault 如果键不存在于字典中，将会添加键并将值设为默认值。&gt;&gt;&gt; info.setdefault(&quot;stu1106&quot;,&quot;Alex&quot;)&apos;Alex&apos;&gt;&gt;&gt; info&#123;&apos;stu1102&apos;: &apos;LongZe Luola&apos;, &apos;stu1103&apos;: &apos;XiaoZe Maliya&apos;, &apos;stu1106&apos;: &apos;Alex&apos;&#125;&gt;&gt;&gt; info.setdefault(&quot;stu1102&quot;,&quot;龙泽萝拉&quot;)&apos;LongZe Luola&apos;&gt;&gt;&gt; info&#123;&apos;stu1102&apos;: &apos;LongZe Luola&apos;, &apos;stu1103&apos;: &apos;XiaoZe Maliya&apos;, &apos;stu1106&apos;: &apos;Alex&apos;&#125;#update 把字典dict2的键/值对更新到dict里。&gt;&gt;&gt; info&#123;&apos;stu1102&apos;: &apos;LongZe Luola&apos;, &apos;stu1103&apos;: &apos;XiaoZe Maliya&apos;, &apos;stu1106&apos;: &apos;Alex&apos;&#125;&gt;&gt;&gt; b = &#123;1:2,3:4, &quot;stu1102&quot;:&quot;龙泽萝拉&quot;&#125;&gt;&gt;&gt; info.update(b)&gt;&gt;&gt; info&#123;&apos;stu1102&apos;: &apos;龙泽萝拉&apos;, 1: 2, 3: 4, &apos;stu1103&apos;: &apos;XiaoZe Maliya&apos;, &apos;stu1106&apos;: &apos;Alex&apos;&#125;#items 以列表返回可遍历的(键, 值) 元组数组info.items()dict_items([(&apos;stu1102&apos;, &apos;龙泽萝拉&apos;), (1, 2), (3, 4), (&apos;stu1103&apos;, &apos;XiaoZe Maliya&apos;), (&apos;stu1106&apos;, &apos;Alex&apos;)])#通过一个列表生成默认dict,有个没办法解释的坑，少用吧这个&gt;&gt;&gt; dict.fromkeys([1,2,3],&apos;testd&apos;)&#123;1: &apos;testd&apos;, 2: &apos;testd&apos;, 3: &apos;testd&apos;&#125; 循环dict1234567#方法1for key in info: print(key,info[key])#方法2for k,v in info.items(): #会先把dict转成list,数据里大时莫用 print(k,v) 程序练习 程序: 三级菜单 要求: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566打印省、市、县三级菜单可返回上一级可随时退出程序menu = &#123; &apos;北京&apos;:&#123; &apos;海淀&apos;:&#123; &apos;五道口&apos;:&#123; &apos;soho&apos;:&#123;&#125;, &apos;网易&apos;:&#123;&#125;, &apos;google&apos;:&#123;&#125; &#125;, &apos;中关村&apos;:&#123; &apos;爱奇艺&apos;:&#123;&#125;, &apos;汽车之家&apos;:&#123;&#125;, &apos;youku&apos;:&#123;&#125;, &#125;, &apos;上地&apos;:&#123; &apos;百度&apos;:&#123;&#125;, &#125;, &#125;, &apos;昌平&apos;:&#123; &apos;沙河&apos;:&#123; &apos;老男孩&apos;:&#123;&#125;, &apos;北航&apos;:&#123;&#125;, &#125;, &apos;天通苑&apos;:&#123;&#125;, &apos;回龙观&apos;:&#123;&#125;, &#125;, &apos;朝阳&apos;:&#123;&#125;, &apos;东城&apos;:&#123;&#125;, &#125;, &apos;上海&apos;:&#123; &apos;闵行&apos;:&#123; &quot;人民广场&quot;:&#123; &apos;炸鸡店&apos;:&#123;&#125; &#125; &#125;, &apos;闸北&apos;:&#123; &apos;火车战&apos;:&#123; &apos;携程&apos;:&#123;&#125; &#125; &#125;, &apos;浦东&apos;:&#123;&#125;, &#125;, &apos;山东&apos;:&#123;&#125;,&#125;exit_flag = Falsecurrent_layer = menulayers = [menu]while not exit_flag: for k in current_layer: print(k) choice = input(&quot;&gt;&gt;:&quot;).strip() if choice == &quot;b&quot;: current_layer = layers[-1] #print(&quot;change to laster&quot;, current_layer) layers.pop() elif choice not in current_layer:continue else: layers.append(current_layer) current_layer = current_layer[choice] #### 4.集合操作集合是一个无序的，不重复的数据组合，它的主要作用如下： 去重，把一个列表变成集合，就自动去重了 关系测试，测试两组数据之前的交集、差集、并集等关系 常用操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263s = set([3,5,9,10]) #创建一个数值集合 t = set(&quot;Hello&quot;) #创建一个唯一字符的集合 a = t | s # t 和 s的并集 把不同的全部打印出来 b = t &amp; s # t 和 s的交集 把相同打印出来 c = t – s # 求差集（项在t中，但不在s中） 只打印t项中在s项中没有的 d = t ^ s # 对称差集（项在t或s中，但不会同时出现在二者中） 把两者间不同的项打印出来，相同不打印 基本操作： t.add(&apos;x&apos;) # 添加一项 s.update([10,37,42]) # 在s中添加多项 使用remove()可以删除一项： t.remove(&apos;H&apos;) len(s) set 的长度 x in s 测试 x 是否是 s 的成员 x not in s 测试 x 是否不是 s 的成员 s.issubset(t) s &lt;= t 测试是否 s 中的每一个元素都在 t 中 s.issuperset(t) s &gt;= t 测试是否 t 中的每一个元素都在 s 中 s.union(t) s | t 返回一个新的 set 包含 s 和 t 中的每一个元素 s.intersection(t) s &amp; t 返回一个新的 set 包含 s 和 t 中的公共元素 s.difference(t) s - t 返回一个新的 set 包含 s 中有但是 t 中没有的元素 s.symmetric_difference(t) s ^ t 返回一个新的 set 包含 s 和 t 中不重复的元素 s.copy() 返回 set “s”的一个浅复制 5. 文件操作对文件操作流程 打开文件，得到文件句柄并赋值给一个变量 通过句柄对文件进行操作 关闭文件 现有文件如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364Somehow, it seems the love I knew was always the most destructive kind不知为何，我经历的爱情总是最具毁灭性的的那种Yesterday when I was young昨日当我年少轻狂The taste of life was sweet生命的滋味是甜的As rain upon my tongue就如舌尖上的雨露I teased at life as if it were a foolish game我戏弄生命 视其为愚蠢的游戏The way the evening breeze就如夜晚的微风May tease the candle flame逗弄蜡烛的火苗The thousand dreams I dreamed我曾千万次梦见The splendid things I planned那些我计划的绚丽蓝图I always built to last on weak and shifting sand但我总是将之建筑在易逝的流沙上I lived by night and shunned the naked light of day我夜夜笙歌 逃避白昼赤裸的阳光And only now I see how the time ran away事到如今我才看清岁月是如何匆匆流逝Yesterday when I was young昨日当我年少轻狂So many lovely songs were waiting to be sung有那么多甜美的曲儿等我歌唱So many wild pleasures lay in store for me有那么多肆意的快乐等我享受And so much pain my eyes refused to see还有那么多痛苦 我的双眼却视而不见I ran so fast that time and youth at last ran out我飞快地奔走 最终时光与青春消逝殆尽I never stopped to think what life was all about我从未停下脚步去思考生命的意义And every conversation that I can now recall如今回想起的所有对话Concerned itself with me and nothing else at all除了和我相关的 什么都记不得了The game of love I played with arrogance and pride我用自负和傲慢玩着爱情的游戏And every flame I lit too quickly, quickly died所有我点燃的火焰都熄灭得太快The friends I made all somehow seemed to slip away所有我交的朋友似乎都不知不觉地离开了And only now I&apos;m left alone to end the play, yeah只剩我一个人在台上来结束这场闹剧Oh, yesterday when I was young噢 昨日当我年少轻狂So many, many songs were waiting to be sung有那么那么多甜美的曲儿等我歌唱So many wild pleasures lay in store for me有那么多肆意的快乐等我享受And so much pain my eyes refused to see还有那么多痛苦 我的双眼却视而不见There are so many songs in me that won&apos;t be sung我有太多歌曲永远不会被唱起I feel the bitter taste of tears upon my tongue我尝到了舌尖泪水的苦涩滋味The time has come for me to pay for yesterday终于到了付出代价的时间 为了昨日When I was young当我年少轻狂 基本操作 12345678f = open(&apos;lyrics&apos;) #打开文件first_line = f.readline()print(&apos;first line:&apos;,first_line) #读一行print(&apos;我是分隔线&apos;.center(50,&apos;-&apos;))data = f.read()# 读取剩下的所有内容,文件大时不要用print(data) #打印文件 f.close() #关闭文件 打开文件的模式有： 1234567891011121314151617r，只读模式（默认）。w，只写模式。【不可读；不存在则创建；存在则删除内容；】a，追加模式。【可读； 不存在则创建；存在则只追加内容；】&quot;+&quot; 表示可以同时读写某个文件r+，可读写文件。【可读；可写；可追加】w+，写读a+，同a&quot;U&quot;表示在读取时，可以将 \r \n \r\n自动转换成 \n （与 r 或 r+ 模式同使用）rUr+U&quot;b&quot;表示处理二进制文件（如：FTP发送上传ISO镜像文件，linux可忽略，windows处理二进制文件时需标注）rbwbab 其它语法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293def close(self): # real signature unknown; restored from __doc__ &quot;&quot;&quot; Close the file. A closed file cannot be used for further I/O operations. close() may be called more than once without error. &quot;&quot;&quot; pass def fileno(self, *args, **kwargs): # real signature unknown &quot;&quot;&quot; Return the underlying file descriptor (an integer). &quot;&quot;&quot; pass def isatty(self, *args, **kwargs): # real signature unknown &quot;&quot;&quot; True if the file is connected to a TTY device. &quot;&quot;&quot; pass def read(self, size=-1): # known case of _io.FileIO.read &quot;&quot;&quot; 注意，不一定能全读回来 Read at most size bytes, returned as bytes. Only makes one system call, so less data may be returned than requested. In non-blocking mode, returns None if no data is available. Return an empty bytes object at EOF. &quot;&quot;&quot; return &quot;&quot; def readable(self, *args, **kwargs): # real signature unknown &quot;&quot;&quot; True if file was opened in a read mode. &quot;&quot;&quot; pass def readall(self, *args, **kwargs): # real signature unknown &quot;&quot;&quot; Read all data from the file, returned as bytes. In non-blocking mode, returns as much as is immediately available, or None if no data is available. Return an empty bytes object at EOF. &quot;&quot;&quot; pass def readinto(self): # real signature unknown; restored from __doc__ &quot;&quot;&quot; Same as RawIOBase.readinto(). &quot;&quot;&quot; pass #不要用,没人知道它是干嘛用的 def seek(self, *args, **kwargs): # real signature unknown &quot;&quot;&quot; Move to new file position and return the file position. Argument offset is a byte count. Optional argument whence defaults to SEEK_SET or 0 (offset from start of file, offset should be &gt;= 0); other values are SEEK_CUR or 1 (move relative to current position, positive or negative), and SEEK_END or 2 (move relative to end of file, usually negative, although many platforms allow seeking beyond the end of a file). Note that not all file objects are seekable. &quot;&quot;&quot; pass def seekable(self, *args, **kwargs): # real signature unknown &quot;&quot;&quot; True if file supports random-access. &quot;&quot;&quot; pass def tell(self, *args, **kwargs): # real signature unknown &quot;&quot;&quot; Current file position. Can raise OSError for non seekable files. &quot;&quot;&quot; pass def truncate(self, *args, **kwargs): # real signature unknown &quot;&quot;&quot; Truncate the file to at most size bytes and return the truncated size. Size defaults to the current file position, as returned by tell(). The current file position is changed to the value of size. &quot;&quot;&quot; pass def writable(self, *args, **kwargs): # real signature unknown &quot;&quot;&quot; True if file was opened in a write mode. &quot;&quot;&quot; pass def write(self, *args, **kwargs): # real signature unknown &quot;&quot;&quot; Write bytes b to file, return number written. Only makes one system call, so not all of the data may be written. The number of bytes actually written is returned. In non-blocking mode, returns None if the write would block. &quot;&quot;&quot; pass with语句 为了避免打开文件后忘记关闭，可以通过管理上下文，即： 123with open(&apos;log&apos;,&apos;r&apos;) as f: ... 如此方式，当with代码块执行完毕时，内部会自动关闭并释放文件资源。 在Python 2.7 后，with又支持同时对多个文件的上下文进行管理，即： 12with open(&apos;log1&apos;) as obj1, open(&apos;log2&apos;) as obj2: pass 程序练习 程序1: 实现简单的shell sed替换功能 程序2:修改haproxy配置文件 需求： 123456789101112131415161718192021222324251、查 输入：www.oldboy.org 获取当前backend下的所有记录2、新建 输入： arg = &#123; &apos;bakend&apos;: &apos;www.oldboy.org&apos;, &apos;record&apos;:&#123; &apos;server&apos;: &apos;100.1.7.9&apos;, &apos;weight&apos;: 20, &apos;maxconn&apos;: 30 &#125; &#125;3、删除 输入： arg = &#123; &apos;bakend&apos;: &apos;www.oldboy.org&apos;, &apos;record&apos;:&#123; &apos;server&apos;: &apos;100.1.7.9&apos;, &apos;weight&apos;: 20, &apos;maxconn&apos;: 30 &#125; &#125; 原配置文件 1234567891011121314151617181920212223242526272829global log 127.0.0.1 local2 daemon maxconn 256 log 127.0.0.1 local2 infodefaults log global mode http timeout connect 5000ms timeout client 50000ms timeout server 50000ms option dontlognulllisten stats :8888 stats enable stats uri /admin stats auth admin:1234frontend oldboy.org bind 0.0.0.0:80 option httplog option httpclose option forwardfor log global acl www hdr_reg(host) -i www.oldboy.org use_backend www.oldboy.org if wwwbackend www.oldboy.org server 100.1.7.9 100.1.7.9 weight 20 maxconn 3000 6. 字符编码与转码详细文章: http://www.cnblogs.com/yuanchenqi/articles/5956943.html http://www.diveintopython3.net/strings.html 需知: 1.在python2默认编码是ASCII, python3里默认是unicode 2.unicode 分为 utf-32(占4个字节),utf-16(占两个字节)，utf-8(占1-4个字节)， so utf-16就是现在最常用的unicode版本， 不过在文件里存的还是utf-8，因为utf8省空间 3.在py3中encode,在转码的同时还会把string 变成bytes类型，decode在解码的同时还会把bytes变回string in python2 1234567891011121314#-*-coding:utf-8-*-__author__ = &apos;Alex Li&apos;import sysprint(sys.getdefaultencoding())msg = &quot;我爱北京天安门&quot;msg_gb2312 = msg.decode(&quot;utf-8&quot;).encode(&quot;gb2312&quot;)gb2312_to_gbk = msg_gb2312.decode(&quot;gbk&quot;).encode(&quot;gbk&quot;)print(msg)print(msg_gb2312)print(gb2312_to_gbk) in python3 1234567891011121314151617#-*-coding:gb2312 -*- #这个也可以去掉__author__ = &apos;Alex Li&apos;import sysprint(sys.getdefaultencoding())msg = &quot;我爱北京天安门&quot;#msg_gb2312 = msg.decode(&quot;utf-8&quot;).encode(&quot;gb2312&quot;)msg_gb2312 = msg.encode(&quot;gb2312&quot;) #默认就是unicode,不用再decode,喜大普奔gb2312_to_unicode = msg_gb2312.decode(&quot;gb2312&quot;)gb2312_to_utf8 = msg_gb2312.decode(&quot;gb2312&quot;).encode(&quot;utf-8&quot;)print(msg)print(msg_gb2312)print(gb2312_to_unicode)print(gb2312_to_utf8) 7.内置函数https://docs.python.org/zh-cn/3/library/functions.html]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的浅拷贝与深拷贝区别]]></title>
    <url>%2F2019%2F08%2F13%2Fpython%E4%B8%AD%E7%9A%84%E6%B5%85%E6%8B%B7%E8%B4%9D%E4%B8%8E%E6%B7%B1%E6%8B%B7%E8%B4%9D%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[python中的深拷贝和浅拷贝python中的深拷贝和浅拷贝和java里面的概念是一样的，所谓浅拷贝就是对引用的拷贝，所谓深拷贝就是对对象的资源的拷贝。 赋值操作: 1.赋值是将一个对象的地址赋值给一个变量，让变量指向该地址(旧瓶装旧酒)。 2.修改不可变对象(str、tuple)需要开辟新的空间 3.修改可变对象(list等)不需要开辟新的空间。 浅拷贝仅仅复制了容器中元素的地址 123456789101112&gt;&gt;&gt; a=[&apos;hello&apos;,[1,2,3]]&gt;&gt;&gt; b=a[:]&gt;&gt;&gt; [id(x) for x in a][55792504, 6444104]&gt;&gt;&gt; [id(x) for x in b][55792504, 6444104]&gt;&gt;&gt; a[0]=&apos;world&apos;&gt;&gt;&gt; a[1].append(4)&gt;&gt;&gt; print(a)[&apos;world&apos;, [1, 2, 3, 4]]&gt;&gt;&gt; print(b)[&apos;hello&apos;, [1, 2, 3, 4]] 这里可以看出，未修改前，a和b中元素的地址都是相同的，不可变的hello和可变的list地址都一样，说明浅拷贝只是将容器内的元素的地址复制了一份。这可以通过修改后，b中字符串没改变，但是list元素随着a相应改变得到验证 浅拷贝是在另一块地址中创建一个新的变量或容器，但是容器内的元素的地址均是源对象的元素的地址的拷贝。也就是说新的容器中指向了旧的元素（ 新瓶装旧酒 ）。 深拷贝，完全拷贝了一个副本，容器内部元素地址都不一样 1234567891011121314&gt;&gt;&gt; from copy import deepcopy&gt;&gt;&gt; a=[&apos;hello&apos;,[1,2,3]]&gt;&gt;&gt; b=deepcopy(a)&gt;&gt;&gt; [id(x) for x in a][55792504, 55645000]&gt;&gt;&gt; [id(x) for x in b][55792504, 58338824]&gt;&gt;&gt; a[0]=&apos;world&apos;&gt;&gt;&gt; a[1].append(4)&gt;&gt;&gt; &gt;&gt;&gt; print(a)[&apos;world&apos;, [1, 2, 3, 4]]&gt;&gt;&gt; print(b)[&apos;hello&apos;, [1, 2, 3]] 这里可以看出，深拷贝后，a和b的地址以及a和b中的元素地址均不同，这是完全拷贝的一个副本，修改a后，发现b没有发生任何改变，因为b是一个完全的副本，元素地址与a均不同，a修改不影响b。 深拷贝是在另一块地址中创建一个新的变量或容器，同时容器内的元素的地址也是新开辟的，仅仅是值相同而已，是完全的副本。也就是说（ 新瓶装新酒 ）。]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python基础一]]></title>
    <url>%2F2019%2F08%2F12%2Fpython%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[一、python介绍python的创始人为吉多.范罗苏姆。1989年的圣诞节期间，吉多.范罗苏姆为了在阿姆期特丹打发时间，决心开发一个新的脚本解释程序，作为ABC语言的一种继承。 最新的TIOBE排行榜，Python赶超PHP占据第五， Python崇尚优美、清晰、简单，是一个优秀并广泛使用的语言。 Python可以应用于众多领域，如：数据分析、组件集成、网络服务、图像处理、数值计算和科学计算等众多领域。目前业内几乎所有大中型互联网企业都在使用Python，如：Youtube、Dropbox、BT、Quora（中国知乎）、豆瓣、知乎、Google、Yahoo!、Facebook、NASA、百度、腾讯、汽车之家、美团等。 目前Python主要应用领域：123456云计算: 云计算最火的语言， 典型应用OpenStackWEB开发: 众多优秀的WEB框架，众多大型网站均为Python开发，Youtube, Dropbox, 豆瓣。。。， 典型WEB框架有Django科学运算、人工智能: 典型库NumPy, SciPy, Matplotlib, Enthought librarys,pandas系统运维: 运维人员必备语言金融：量化交易，金融分析，在金融工程领域，Python不但在用，且用的最多，而且重要性逐年提高。原因：作为动态语言的Python，语言结构清晰简单，库丰富，成熟稳定，科学计算和统计分析都很牛逼，生产效率远远高于c,c++,java,尤其擅长策略回测图形GUI: PyQT, WxPython,TkInter Python在一些公司的应用：123456789101112谷歌：Google App Engine 、code.google.com 、Google earth 、谷歌爬虫、Google广告等项目都在大量使用Python开发CIA: 美国中情局网站就是用Python开发的NASA: 美国航天局(NASA)大量使用Python进行数据分析和运算YouTube:世界上最大的视频网站YouTube就是用Python开发的Dropbox:美国最大的在线云存储网站，全部用Python实现，每天网站处理10亿个文件的上传和下载Instagram:美国最大的图片分享社交网站，每天超过3千万张照片被分享，全部用python开发Facebook:大量的基础库均通过Python实现的Redhat: 世界上最流行的Linux发行版本中的yum包管理工具就是用python开发的豆瓣: 公司几乎所有的业务均是通过Python开发的知乎: 国内最大的问答社区，通过Python开发(国外Quora)春雨医生：国内知名的在线医疗网站是用Python开发的除上面之外，还有搜狐、金山、腾讯、盛大、网易、百度、阿里、淘宝 、土豆、新浪、果壳等公司都在使用Python完成各种各样的任务。 Python 是一门什么样的语言？编程语言主要从以下几个角度为进行分类，编译型和解释型、静态语言和动态语言、强类型定义语言和弱类型定义语言，每个分类代表什么意思呢，我们一起来看一下。 编译和解释的区别是什么？ 编译器是把源程序的每一条语句都编译成机器语言,并保存成二进制文件,这样运行时计算机可以直接以机器语言来运行此程序,速度很快; 而解释器则是只在执行程序时,才一条一条的解释成机器语言给计算机来执行,所以运行速度是不如编译后的程序运行的快的. 这是因为计算机不能直接认识并执行我们写的语句,它只能认识机器语言(是二进制的形式) 如编绎型:c c++ go swift object-C pascal 解释型:javascript python ruby php perl erlan 混合型：java c# 编译型vs解释型编译型 优点：编译器一般会有预编译的过程对代码进行优化。因为编译只做一次，运行时不需要编译，所以编译型语言的程序执行效率高，可以脱离语言环境独立运行。 缺点:编译之后如果需要修改就需要整个模块重新编译。编译的时候根据对就的运行环境生成机器码，不同的操作系统之间移植就会有问题，需要根据运行的操作系统环境编译不同的可执行文件。 解释型优点：有良好的平台兼容性，在任何环境中都可以运行，前提是安装了解释器（虚拟机）。灵活，修改代码的时候直接修改就可以，可以快速部署，不用停机维护。 缺点：每次运行的时候都要解释一遍，性能上不如编译型语言。 1234567891011121314一、低级语言与高级语言最初的计算机程序都是用0和1的序列表示的，程序员直接使用的是机器指令，无需翻译，从纸带打孔输入即可执行得到结果。后来为了方便记忆，就将用0、1序列表示的机器指令都用符号助记，这些与机器指令一一对应的助记符就成了汇编指令，从而诞生了汇编语言。无论是机器指令还是汇编指令都是面向机器的，统称为低级语言。因为是针对特定机器的机器指令的助记符，所以汇编语言是无法独立于机器(特定的CPU体系结构)的。但汇编语言也是要经过翻译成机器指令才能执行的，所以也有将运行在一种机器上的汇编语言翻译成运行在另一种机器上的机器指令的方法，那就是交叉汇编技术。高级语言是从人类的逻辑思维角度出发的计算机语言，抽象程度大大提高，需要经过编译成特定机器上的目标代码才能执行，一条高级语言的语句往往需要若干条机器指令来完成。高级语言独立于机器的特性是靠编译器为不同机器生成不同的目标代码(或机器指令)来实现的。那具体的说，要将高级语言编译到什么程度呢，这又跟编译的技术有关了，既可以编译成直接可执行的目标代码，也可以编译成一种中间表示，然后拿到不同的机器和系统上去执行，这种情况通常又需要支撑环境，比如解释器或虚拟机的支持，Java程序编译成bytecode，再由不同平台上的虚拟机执行就是很好的例子。所以，说高级语言不依赖于机器，是指在不同的机器或平台上高级语言的程序本身不变，而通过编译器编译得到的目标代码去适应不同的机器。从这个意义上来说，通过交叉汇编，一些汇编程序也可以获得不同机器之间的可移植性，但这种途径获得的移植性远远不如高级语言来的方便和实用性大。二、编译与解释编译是将源程序翻译成可执行的目标代码，翻译与执行是分开的；而解释是对源程序的翻译与执行一次性完成，不生成可存储的目标代码。这只是表象，二者背后的最大区别是：对解释执行而言，程序运行时的控制权在解释器而不在用户程序；对编译执行而言，运行时的控制权在用户程序。解释具有良好的动态特性和可移植性，比如在解释执行时可以动态改变变量的类型、对程序进行修改以及在程序中插入良好的调试诊断信息等，而将解释器移植到不同的系统上，则程序不用改动就可以在移植了解释器的系统上运行。同时解释器也有很大的缺点，比如执行效率低，占用空间大，因为不仅要给用户程序分配空间，解释器本身也占用了宝贵的系统资源。编译器是把源程序的每一条语句都编译成机器语言,并保存成二进制文件,这样运行时计算机可以直接以机器语言来运行此程序,速度很快;而解释器则是只在执行程序时,才一条一条的解释成机器语言给计算机来执行,所以运行速度是不如编译后的程序运行的快的. 12345678910111213141516171819202122232425262728编译型和解释型我们先看看编译型，其实它和汇编语言是一样的：也是有一个负责翻译的程序来对我们的源代码进行转换，生成相对应的可执行代码。这个过程说得专业一点，就称为编译（Compile），而负责编译的程序自然就称为编译器（Compiler）。如果我们写的程序代码都包含在一个源文件中，那么通常编译之后就会直接生成一个可执行文件，我们就可以直接运行了。但对于一个比较复杂的项目，为了方便管理，我们通常把代码分散在各个源文件中，作为不同的模块来组织。这时编译各个文件时就会生成目标文件（Object file）而不是前面说的可执行文件。一般一个源文件的编译都会对应一个目标文件。这些目标文件里的内容基本上已经是可执行代码了，但由于只是整个项目的一部分，所以我们还不能直接运行。待所有的源文件的编译都大功告成，我们就可以最后把这些半成品的目标文件“打包”成一个可执行文件了，这个工作由另一个程序负责完成，由于此过程好像是把包含可执行代码的目标文件连接装配起来，所以又称为链接（Link），而负责链接的程序就叫……就叫链接程序（Linker）。链接程序除了链接目标文件外，可能还有各种资源，像图标文件啊、声音文件啊什么的，还要负责去除目标文件之间的冗余重复代码，等等，所以……也是挺累的。链接完成之后，一般就可以得到我们想要的可执行文件了。 上面我们大概地介绍了编译型语言的特点，现在再看看解释型。噢，从字面上看，“编译”和“解释”的确都有“翻译”的意思，它们的区别则在于翻译的时机安排不大一样。打个比方：假如你打算阅读一本外文书，而你不知道这门外语，那么你可以找一名翻译，给他足够的时间让他从头到尾把整本书翻译好，然后把书的母语版交给你阅读；或者，你也立刻让这名翻译辅助你阅读，让他一句一句给你翻译，如果你想往回看某个章节，他也得重新给你翻译。 两种方式，前者就相当于我们刚才所说的编译型：一次把所有的代码转换成机器语言，然后写成可执行文件；而后者就相当于我们要说的解释型：在程序运行的前一刻，还只有源程序而没有可执行程序；而程序每执行到源程序的某一条指令，则会有一个称之为解释程序的外壳程序将源代码转换成二进制代码以供执行，总言之，就是不断地解释、执行、解释、执行……所以，解释型程序是离不开解释程序的。像早期的BASIC就是一门经典的解释型语言，要执行BASIC程序，就得进入BASIC环境，然后才能加载程序源文件、运行。解释型程序中，由于程序总是以源代码的形式出现，因此只要有相应的解释器，移植几乎不成问题。编译型程序虽然源代码也可以移植，但前提是必须针对不同的系统分别进行编译，对于复杂的工程来说，的确是一件不小的时间消耗，况且很可能一些细节的地方还是要修改源代码。而且，解释型程序省却了编译的步骤，修改调试也非常方便，编辑完毕之后即可立即运行，不必像编译型程序一样每次进行小小改动都要耐心等待漫长的Compiling…Linking…这样的编译链接过程。不过凡事有利有弊，由于解释型程序是将编译的过程放到执行过程中，这就决定了解释型程序注定要比编译型慢上一大截，像几百倍的速度差距也是不足为奇的。 编译型与解释型，两者各有利弊。前者由于程序执行速度快，同等条件下对系统要求较低，因此像开发操作系统、大型应用程序、数据库系统等时都采用它，像C/C++、Pascal/Object Pascal（Delphi）、VB等基本都可视为编译语言，而一些网页脚本、服务器脚本及辅助开发接口这样的对速度要求不高、对不同系统平台间的兼容性有一定要求的程序则通常使用解释性语言，如Java、JavaScript、VBScript、Perl、Python等等。 但既然编译型与解释型各有优缺点又相互对立，所以一批新兴的语言都有把两者折衷起来的趋势，例如Java语言虽然比较接近解释型语言的特征，但在执行之前已经预先进行一次预编译，生成的代码是介于机器码和Java源代码之间的中介代码，运行的时候则由JVM（Java的虚拟机平台，可视为解释器）解释执行。它既保留了源代码的高抽象、可移植的特点，又已经完成了对源代码的大部分预编译工作，所以执行起来比“纯解释型”程序要快许多。而像VB6（或者以前版本）、C#这样的语言，虽然表面上看生成的是.exe可执行程序文件，但VB6编译之后实际生成的也是一种中介码，只不过编译器在前面安插了一段自动调用某个外部解释器的代码（该解释程序独立于用户编写的程序，存放于系统的某个DLL文件中，所有以VB6编译生成的可执行程序都要用到它），以解释执行实际的程序体。C#（以及其它.net的语言编译器）则是生成.net目标代码，实际执行时则由.net解释系统（就像JVM一样，也是一个虚拟机平台）进行执行。当然.net目标代码已经相当“低级”，比较接近机器语言了，所以仍将其视为编译语言，而且其可移植程度也没有Java号称的这么强大，Java号称是“一次编译，到处执行”，而.net则是“一次编码，到处编译”。呵呵，当然这些都是题外话了。总之，随着设计技术与硬件的不断发展，编译型与解释型两种方式的界限正在不断变得模糊。动态语言和静态语言通常我们所说的动态语言、静态语言是指动态类型语言和静态类型语言。（1）动态类型语言：动态类型语言是指在运行期间才去做数据类型检查的语言，也就是说，在用动态类型的语言编程时，永远也不用给任何变量指定数据类型，该语言会在你第一次赋值给变量时，在内部将数据类型记录下来。Python和Ruby就是一种典型的动态类型语言，其他的各种脚本语言如VBScript也多少属于动态类型语言。（2）静态类型语言：静态类型语言与动态类型语言刚好相反，它的数据类型是在编译其间检查的，也就是说在写程序时要声明所有变量的数据类型，C/C++是静态类型语言的典型代表，其他的静态类型语言还有C#、JAVA等。 强类型定义语言和弱类型定义语言（1）强类型定义语言：强制数据类型定义的语言。也就是说，一旦一个变量被指定了某个数据类型，如果不经过强制转换，那么它就永远是这个数据类型了。举个例子：如果你定义了一个整型变量a,那么程序根本不可能将a当作字符串类型处理。强类型定义语言是类型安全的语言。（2）弱类型定义语言：数据类型可以被忽略的语言。它与强类型定义语言相反, 一个变量可以赋不同数据类型的值。强类型定义语言在速度上可能略逊色于弱类型定义语言，但是强类型定义语言带来的严谨性能够有效的避免许多错误。另外，“这门语言是不是动态语言”与“这门语言是否类型安全”之间是完全没有联系的！例如：Python是动态语言，是强类型定义语言（类型安全的语言）; VBScript是动态语言，是弱类型定义语言（类型不安全的语言）; JAVA是静态语言，是强类型定义语言（类型安全的语言）。 Python的优缺点12345678910111213141516先看优点Python的定位是“优雅”、“明确”、“简单”，所以Python程序看上去总是简单易懂，初学者学Python，不但入门容易，而且将来深入下去，可以编写那些非常非常复杂的程序。开发效率非常高，Python有非常强大的第三方库，基本上你想通过计算机实现任何功能，Python官方库里都有相应的模块进行支持，直接下载调用后，在基础库的基础上再进行开发，大大降低开发周期，避免重复造轮子。高级语言————当你用Python语言编写程序的时候，你无需考虑诸如如何管理你的程序使用的内存一类的底层细节可移植性————由于它的开源本质，Python已经被移植在许多平台上（经过改动使它能够工 作在不同平台上）。如果你小心地避免使用依赖于系统的特性，那么你的所有Python程序无需修改就几乎可以在市场上所有的系统平台上运行可扩展性————如果你需要你的一段关键代码运行得更快或者希望某些算法不公开，你可以把你的部分程序用C或C++编写，然后在你的Python程序中使用它们。可嵌入性————你可以把Python嵌入你的C/C++程序，从而向你的程序用户提供脚本功能。再看缺点：速度慢，Python 的运行速度相比C语言确实慢很多，跟JAVA相比也要慢一些，因此这也是很多所谓的大牛不屑于使用Python的主要原因，但其实这里所指的运行速度慢在大多数情况下用户是无法直接感知到的，必须借助测试工具才能体现出来，比如你用C运一个程序花了0.01s,用Python是0.1s,这样C语言直接比Python快了10倍,算是非常夸张了，但是你是无法直接通过肉眼感知的，因为一个正常人所能感知的时间最小单位是0.15-0.4s左右，哈哈。其实在大多数情况下Python已经完全可以满足你对程序速度的要求，除非你要写对速度要求极高的搜索引擎等，这种情况下，当然还是建议你用C去实现的。代码不能加密，因为PYTHON是解释性语言，它的源码都是以名文形式存放的，不过我不认为这算是一个缺点，如果你的项目要求源代码必须是加密的，那你一开始就不应该用Python来去实现。线程不能利用多CPU问题，这是Python被人诟病最多的一个缺点，GIL即全局解释器锁（Global Interpreter Lock），是计算机程序设计语言解释器用于同步线程的工具，使得任何时刻仅有一个线程在执行，Python的线程是操作系统的原生线程。在Linux上为pthread，在Windows上为Win thread，完全由操作系统调度线程的执行。一个python解释器进程内有一条主线程，以及多条用户程序的执行线程。即使在多核CPU平台上，由于GIL的存在，所以禁止多线程的并行执行。关于这个问题的折衷解决方法，我们在以后线程和进程章节里再进行详细探讨。 当然，Python还有一些其它的小缺点，在这就不一一列举了，我想说的是，任何一门语言都不是完美的，都有擅长和不擅长做的事情，建议各位不要拿一个语言的劣势去跟另一个语言的优势来去比较，语言只是一个工具，是实现程序设计师思想的工具，就像我们之前中学学几何时，有的时候需要要圆规，有的时候需要用三角尺一样，拿相应的工具去做它最擅长的事才是正确的选择。之前很多人问我Shell和Python到底哪个好？我回答说Shell是个脚本语言，但Python不只是个脚本语言，能做的事情更多，然后又有钻牛角尖的人说完全没必要学Python, Python能做的事情Shell都可以做，只要你足够牛B,然后又举了用Shell可以写俄罗斯方块这样的游戏，对此我能说表达只能是，不要跟SB理论，SB会把你拉到跟他一样的高度，然后用充分的经验把你打倒。 Python解释器当我们编写Python代码时，我们得到的是一个包含Python代码的以.py为扩展名的文本文件。要运行代码，就需要Python解释器去执行.py文件。 由于整个Python语言从规范到解释器都是开源的，所以理论上，只要水平够高，任何人都可以编写Python解释器来执行Python代码（当然难度很大）。事实上，确实存在多种Python解释器。 CPython当我们从Python官方网站下载并安装好Python 2.7后，我们就直接获得了一个官方版本的解释器：CPython。这个解释器是用C语言开发的，所以叫CPython。在命令行下运行python就是启动CPython解释器。 CPython是使用最广的Python解释器。教程的所有代码也都在CPython下执行。 IPythonIPython是基于CPython之上的一个交互式解释器，也就是说，IPython只是在交互方式上有所增强，但是执行Python代码的功能和CPython是完全一样的。好比很多国产浏览器虽然外观不同，但内核其实都是调用了IE。 CPython用&gt;&gt;&gt;作为提示符，而IPython用In [序号]:作为提示符。 PyPyPyPy是另一个Python解释器，它的目标是执行速度。PyPy采用JIT技术，对Python代码进行动态编译（注意不是解释），所以可以显著提高Python代码的执行速度。 绝大部分Python代码都可以在PyPy下运行，但是PyPy和CPython有一些是不同的，这就导致相同的Python代码在两种解释器下执行可能会有不同的结果。如果你的代码要放到PyPy下执行，就需要了解PyPy和CPython的不同点。 JythonJython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码执行。 IronPythonIronPython和Jython类似，只不过IronPython是运行在微软.Net平台上的Python解释器，可以直接把Python代码编译成.Net的字节码。 小结Python的解释器很多，但使用最广泛的还是CPython。如果要和Java或.Net平台交互，最好的办法不是用Jython或IronPython，而是通过网络调用来交互，确保各程序之间的独立性。 二、Python发展史123456789101112131415161989年，为了打发圣诞节假期，Guido开始写Python语言的编译器。Python这个名字，来自Guido所挚爱的电视剧Monty Python’s Flying Circus。他希望这个新的叫做Python的语言，能符合他的理想：创造一种C和shell之间，功能全面，易学易用，可拓展的语言。1991年，第一个Python编译器诞生。它是用C语言实现的，并能够调用C语言的库文件。从一出生，Python已经具有了：类，函数，异常处理，包含表和词典在内的核心数据类型，以及模块为基础的拓展系统。Granddaddy of Python web frameworks, Zope 1 was released in 1999Python 1.0 - January 1994 增加了 lambda, map, filter and reduce.Python 2.0 - October 16, 2000，加入了内存回收机制，构成了现在Python语言框架的基础Python 2.4 - November 30, 2004, 同年目前最流行的WEB框架Django 诞生Python 2.5 - September 19, 2006Python 2.6 - October 1, 2008Python 2.7 - July 3, 2010In November 2014, it was announced that Python 2.7 would be supported until 2020, and reaffirmed that there would be no 2.8 release as users were expected to move to Python 3.4+ as soon as possiblePython 3.0 - December 3, 2008Python 3.1 - June 27, 2009Python 3.2 - February 20, 2011Python 3.3 - September 29, 2012Python 3.4 - March 16, 2014Python 3.5 - September 13, 2015 三、Python 2 or 3?In summary : Python 2.x is legacy, Python 3.x is the present and future of the language Python 3.0 was released in 2008. The final 2.x version 2.7 release came out in mid-2010, with a statement of extended support for this end-of-life release. The 2.x branch will see no new major releases after that. 3.x is under active development and has already seen over five years of stable releases, including version 3.3 in 2012, 3.4 in 2014, and 3.5 in 2015. This means that all recent standard library improvements, for example, are only available by default in Python 3.x. Guido van Rossum (the original creator of the Python language) decided to clean up Python 2.x properly, with less regard for backwards compatibility than is the case for new releases in the 2.x range. The most drastic improvement is the better Unicode support (with all text strings being Unicode by default) as well as saner bytes/Unicode separation. Besides, several aspects of the core language (such as print and exec being statements, integers using floor division) have been adjusted to be easier for newcomers to learn and to be more consistent with the rest of the language, and old cruft has been removed (for example, all classes are now new-style, “range()” returns a memory efficient iterable, not a list as in 2.x). py2与3的详细区别123456789101112131415161718192021222324PRINT IS A FUNCTIONThe statement has been replaced with a print() function, with keyword arguments to replace most of the special syntax of the old statement (PEP 3105). Examples: Old: print &quot;The answer is&quot;, 2*2 New: print(&quot;The answer is&quot;, 2*2)Old: print x, # Trailing comma suppresses newline New: print(x, end=&quot; &quot;) # Appends a space instead of a newlineOld: print # Prints a newlineNew: print() # You must call the function!Old: print &gt;&gt;sys.stderr, &quot;fatal error&quot; New: print(&quot;fatal error&quot;, file=sys.stderr)Old: print (x, y) # prints repr((x, y))New: print((x, y)) # Not the same as print(x, y)!You can also customize the separator between items, e.g.: print(&quot;There are &lt;&quot;, 2**32, &quot;&gt; possibilities!&quot;, sep=&quot;&quot;)ALL IS UNICODE NOW从此不再为讨厌的字符编码而烦恼还可以这样玩： (A,*REST,B)=RANGE(5)&lt;strong&gt;&gt;&gt;&gt; a,*rest,b = range(5)&gt;&gt;&gt; a,rest,b(0, [1, 2, 3], 4)&lt;/strong&gt; 某些库改名了 Old Name New Name _winreg winreg ConfigParser configparser copy_reg copyreg Queue queue SocketServer socketserver markupbase _markupbase repr reprlib test.test_support test.support 还有谁不支持PYTHON3? One popular module that don’t yet support Python 3 is Twisted (for networking and other applications). Most actively maintained libraries have people working on 3.x support. For some libraries, it’s more of a priority than others: Twisted, for example, is mostly focused on production servers, where supporting older versions of Python is important, let alone supporting a new version that includes major changes to the language. (Twisted is a prime example of a major package where porting to 3.x is far from trivial 四、Python安装windows 123456789101112131、下载安装包 https://www.python.org/downloads/2、安装 默认安装路径：C:\python273、配置环境变量 【右键计算机】--》【属性】--》【高级系统设置】--》【高级】--》【环境变量】--》【在第二个内容框中找到 变量名为Path 的一行，双击】 --&gt; 【Python安装目录追加到变值值中，用 ； 分割】 如：原来的值;C:\python27，切记前面有分号linux、Mac无需安装，原装Python环境 ps：如果自带2.6，请更新至2.7 五、Hello World程序在linux 下创建一个文件叫hello.py,并输入 1print(&quot;Hello World!&quot;) 然后执行命令:python hello.py ,输出 123localhost:~ jieli$ vim hello.pylocalhost:~ jieli$ python hello.pyHello World! 指定解释器上一步中执行 python hello.py 时，明确的指出 hello.py 脚本由 python 解释器来执行。 如果想要类似于执行shell脚本一样执行python脚本，例： ./hello.py ，那么就需要在 hello.py 文件的头部指定解释器，如下： 123#!/usr/bin/env python print &quot;hello,world&quot; 如此一来，执行： ./hello.py 即可。 ps：执行前需给予 hello.py 执行权限，chmod 755 hello.py 在交互器中执行 除了把程序写在文件里，还可以直接调用python自带的交互器运行代码， 123456localhost:~ jieli$ pythonPython 2.7.10 (default, Oct 23 2015, 18:05:06)[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; print(&quot;Hello World!&quot;)Hello World! 对比下其它语言的hello world123456789101112131415161718192021222324252627282930313233343536373839404142c++1 #include &lt;iostream&gt;2 int main(void)3 &#123;4 std::cout&lt;&lt;&quot;Hello world&quot;;5 &#125;java1 public class HelloWorld&#123;2 // 程序的入口3 public static void main(String args[])&#123;4 // 向控制台输出信息5 System.out.println(&quot;Hello World!&quot;);6 &#125;7 &#125;c1 #include &lt;stdio.h&gt;2 int main(void)3 &#123;4 printf(&quot;\nhello world!&quot;);5 return 0;6 &#125;php1 &lt;?php 2 echo &quot;hello world!&quot;; 3 ?&gt; ruby1 puts &quot;Hello world.&quot; go1 package main2 3 import &quot;fmt&quot;4 5 func main()&#123;6 7 fmt.Printf(&quot;Hello World!\n God Bless You!&quot;);8 9 &#125; 六、变量\字符编码 Variables are used to store information to be referenced and manipulated in a computer program. They also provide a way of labeling data with a descriptive name, so our programs can be understood more clearly by the reader and ourselves. It is helpful to think of variables as containers that hold information. Their sole purpose is to label and store data in memory. This data can then be used throughout your program. 声明变量123#_*_coding:utf-8_*_ name = &quot;Alex Li&quot; 上述代码声明了一个变量，变量名为： name，变量name的值为：”Alex Li” #### 变量定义的规则： 1234变量名只能是 字母、数字或下划线的任意组合变量名的第一个字符不能是数字以下关键字不能声明为变量名[&apos;and&apos;, &apos;as&apos;, &apos;assert&apos;, &apos;break&apos;, &apos;class&apos;, &apos;continue&apos;, &apos;def&apos;, &apos;del&apos;, &apos;elif&apos;, &apos;else&apos;, &apos;except&apos;, &apos;exec&apos;, &apos;finally&apos;, &apos;for&apos;, &apos;from&apos;, &apos;global&apos;, &apos;if&apos;, &apos;import&apos;, &apos;in&apos;, &apos;is&apos;, &apos;lambda&apos;, &apos;not&apos;, &apos;or&apos;, &apos;pass&apos;, &apos;print&apos;, &apos;raise&apos;, &apos;return&apos;, &apos;try&apos;, &apos;while&apos;, &apos;with&apos;, &apos;yield&apos;] 变量的赋值12345678name = &quot;Alex Li&quot; name2 = nameprint(name,name2) name = &quot;Jack&quot; print(&quot;What is the value of name2 now?&quot;) 七、字符编码python解释器在加载 .py 文件中的代码时，会对内容进行编码（默认ascill） ASCII（American Standard Code for Information Interchange，美国标准信息交换代码）是基于拉丁字母的一套电脑编码系统，主要用于显示现代英语和其他西欧语言，其最多只能用 8 位来表示（一个字节），即：2**8 = 256-1，所以，ASCII码最多只能表示 255 个符号。 关于中文为了处理汉字，程序员设计了用于简体中文的GB2312和用于繁体中文的big5。 GB2312(1980年)一共收录了7445个字符，包括6763个汉字和682个其它符号。汉字区的内码范围高字节从B0-F7，低字节从A1-FE，占用的码位是72*94=6768。其中有5个空位是D7FA-D7FE。 GB2312 支持的汉字太少。1995年的汉字扩展规范GBK1.0收录了21886个符号，它分为汉字区和图形符号区。汉字区包括21003个字符。2000年的 GB18030是取代GBK1.0的正式国家标准。该标准收录了27484个汉字，同时还收录了藏文、蒙文、维吾尔文等主要的少数民族文字。现在的PC平台必须支持GB18030，对嵌入式产品暂不作要求。所以手机、MP3一般只支持GB2312。 从ASCII、GB2312、GBK 到GB18030，这些编码方法是向下兼容的，即同一个字符在这些方案中总是有相同的编码，后面的标准支持更多的字符。在这些编码中，英文和中文可以统一地处理。区分中文编码的方法是高字节的最高位不为0。按照程序员的称呼，GB2312、GBK到GB18030都属于双字节字符集 (DBCS)。 有的中文Windows的缺省内码还是GBK，可以通过GB18030升级包升级到GB18030。不过GB18030相对GBK增加的字符，普通人是很难用到的，通常我们还是用GBK指代中文Windows内码。 显然ASCII码无法将世界上的各种文字和符号全部表示，所以，就需要新出一种可以代表所有字符和符号的编码，即：Unicode Unicode（统一码、万国码、单一码）是一种在计算机上使用的字符编码。Unicode 是为了解决传统的字符编码方案的局限而产生的，它为每种语言中的每个字符设定了统一并且唯一的二进制编码，规定虽有的字符和符号最少由 16 位来表示（2个字节），即：2 **16 = 65536，注：此处说的的是最少2个字节，可能更多 UTF-8，是对Unicode编码的压缩和优化，他不再使用最少使用2个字节，而是将所有的字符和符号进行分类：ascii码中的内容用1个字节保存、欧洲的字符用2个字节保存，东亚的字符用3个字节保存… 所以，python解释器在加载 .py 文件中的代码时，会对内容进行编码（默认ascill），如果是如下代码的话： 报错：ascii码无法表示中文 1234#!/usr/bin/env python print &quot;你好，世界&quot;` 改正：应该显示的告诉python解释器，用什么编码来执行源代码，即： 1234#!/usr/bin/env python# -*- coding: utf-8 -*- print &quot;你好，世界&quot; 注释123 当行注视：# 被注释内容 多行注释：&quot;&quot;&quot; 被注释内容 &quot;&quot;&quot; 八、用户输入 1234567#!/usr/bin/env python#_*_coding:utf-8_*_ #name = raw_input(&quot;What is your name?&quot;) #only on python 2.xname = input(&quot;What is your name?&quot;)print(&quot;Hello &quot; + name ) 输入密码时，如果想要不可见，需要利用getpass 模块中的 getpass方法，即： 12345678910#!/usr/bin/env python# -*- coding: utf-8 -*- import getpass # 将用户输入的内容赋值给 name 变量pwd = getpass.getpass(&quot;请输入密码：&quot;) # 打印输入的内容print(pwd) 九、模块初识 Python的强大之处在于他有非常丰富和强大的标准库和第三方库，几乎你想实现的任何功能都有相应的Python库支持，以后的课程中会深入讲解常用到的各种库，现在，我们先来象征性的学2个简单的。 sys 1234567891011#!/usr/bin/env python# -*- coding: utf-8 -*- import sys print(sys.argv) #输出$ python test.py helo world[&apos;test.py&apos;, &apos;helo&apos;, &apos;world&apos;] #把执行脚本时传递的参数获取到了 os 123456#!/usr/bin/env python# -*- coding: utf-8 -*- import os os.system(&quot;df -h&quot;) #调用系统命令 完全结合一下 123import os,sys os.system(&apos;&apos;.join(sys.argv[1:])) #把用户的输入的参数当作一条命令交给os.system来执行 自己写个模块 python tab补全模块 for mac 123456781 import sys2 import readline3 import rlcompleter4 5 if sys.platform == &apos;darwin&apos; and sys.version_info[0] == 2:6 readline.parse_and_bind(&quot;bind ^I rl_complete&quot;)7 else:8 readline.parse_and_bind(&quot;tab: complete&quot;) # linux and python3 on mac for Linux 1234567891011121314151617 1 #!/usr/bin/env python 2 # python startup file 3 import sys 4 import readline 5 import rlcompleter 6 import atexit 7 import os 8 # tab completion 9 readline.parse_and_bind(&apos;tab: complete&apos;)10 # history file 11 histfile = os.path.join(os.environ[&apos;HOME&apos;], &apos;.pythonhistory&apos;)12 try:13 readline.read_history_file(histfile)14 except IOError:15 pass16 atexit.register(readline.write_history_file, histfile)17 del os, histfile, readline, rlcompleter 写完保存后就可以使用了 123456localhost:~ jieli$ pythonPython 2.7.10 (default, Oct 23 2015, 18:05:06)[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import tab你会发现，上面自己写的tab.py模块只能在当前目录下导入，如果想在系统的何何一个地方都使用怎么办呢？ 此时你就要把这个tab.py放到python全局环境变量目录里啦，基本一般都放在一个叫 Python/2.7/site-packages 目录下，这个目录在不同的OS里放的位置不一样，用 print(sys.path) 可以查看python环境变量列表 十、.pyc是个什么鬼？ Python是一门解释型语言？ 我初学Python时，听到的关于Python的第一句话就是，Python是一门解释性语言，我就这样一直相信下去，直到发现了.pyc文件的存在。如果是解释型语言，那么生成的.pyc文件是什么呢？c应该是compiled的缩写才对啊！ 为了防止其他学习Python的人也被这句话误解，那么我们就在文中来澄清下这个问题，并且把一些基础概念给理清。 解释型语言和编译型语言 计算机是不能够识别高级语言的，所以当我们运行一个高级语言程序的时候，就需要一个“翻译机”来从事把高级语言转变成计算机能读懂的机器语言的过程。这个过程分成两类，第一种是编译，第二种是解释。 编译型语言在程序执行之前，先会通过编译器对程序执行一个编译的过程，把程序转变成机器语言。运行时就不需要翻译，而直接执行就可以了。最典型的例子就是C语言。 解释型语言就没有这个编译的过程，而是在程序运行的时候，通过解释器对程序逐行作出解释，然后直接运行，最典型的例子是Ruby。 通过以上的例子，我们可以来总结一下解释型语言和编译型语言的优缺点，因为编译型语言在程序运行之前就已经对程序做出了“翻译”，所以在运行时就少掉了“翻译”的过程，所以效率比较高。但是我们也不能一概而论，一些解释型语言也可以通过解释器的优化来在对程序做出翻译时对整个程序做出优化，从而在效率上超过编译型语言。 此外，随着Java等基于虚拟机的语言的兴起，我们又不能把语言纯粹地分成解释型和编译型这两种。 用Java来举例，Java首先是通过编译器编译成字节码文件，然后在运行时通过解释器给解释成机器文件。所以我们说Java是一种先编译后解释的语言。 Python到底是什么 其实Python和Java/C#一样，也是一门基于虚拟机的语言，我们先来从表面上简单地了解一下Python程序的运行过程吧。 当我们在命令行中输入python hello.py时，其实是激活了Python的“解释器”，告诉“解释器”：你要开始工作了。可是在“解释”之前，其实执行的第一项工作和Java一样，是编译。 熟悉Java的同学可以想一下我们在命令行中如何执行一个Java的程序： javac hello.java java hello 只是我们在用Eclipse之类的IDE时，将这两部给融合成了一部而已。其实Python也一样，当我们执行python hello.py时，他也一样执行了这么一个过程，所以我们应该这样来描述Python，Python是一门先编译后解释的语言。 简述Python的运行过程 在说这个问题之前，我们先来说两个概念，PyCodeObject和pyc文件。 我们在硬盘上看到的pyc自然不必多说，而其实PyCodeObject则是Python编译器真正编译成的结果。我们先简单知道就可以了，继续向下看。 当python程序运行时，编译的结果则是保存在位于内存中的PyCodeObject中，当Python程序运行结束时，Python解释器则将PyCodeObject写回到pyc文件中。 当python程序第二次运行时，首先程序会在硬盘中寻找pyc文件，如果找到，则直接载入，否则就重复上面的过程。 所以我们应该这样来定位PyCodeObject和pyc文件，我们说pyc文件其实是PyCodeObject的一种持久化保存方式。 十一、数据类型初识1、数字 2 是一个整数的例子。 长整数 不过是大一些的整数。 3.23和52.3E-4是浮点数的例子。E标记表示10的幂。在这里，52.3E-4表示52.3 * 10-4。 (-5+4j)和(2.3-4.6j)是复数的例子，其中-5,4为实数，j为虚数，数学中表示复数是什么？。 12345678910111213int（整型） 在32位机器上，整数的位数为32位，取值范围为-2**31～2**31-1，即-2147483648～2147483647 在64位系统上，整数的位数为64位，取值范围为-2**63～2**63-1，即-9223372036854775808～9223372036854775807long（长整型） 跟C语言不同，Python的长整数没有指定位宽，即：Python没有限制长整数数值的大小，但实际上由于机器内存有限，我们使用的长整数数值不可能无限大。 注意，自从Python2.2起，如果整数发生溢出，Python会自动将整数数据转换为长整数，所以如今在长整数数据后面不加字母L也不会导致严重后果了。float（浮点型） 先扫盲 http://www.cnblogs.com/alex3714/articles/5895848.html 浮点数用来处理实数，即带有小数的数字。类似于C语言中的double类型，占8个字节（64位），其中52位表示底，11位表示指数，剩下的一位表示符号。complex（复数） 复数由实数部分和虚数部分组成，一般形式为x＋yj，其中的x是复数的实数部分，y是复数的虚数部分，这里的x和y都是实数。注：Python中存在小数字池：-5 ～ 257 2、布尔值 真或假 1 或 0 3、字符串 “hello world” 万恶的字符串拼接： python中的字符串在C语言中体现为是一个字符数组，每次创建字符串时候需要在内存中开辟一块连续的空，并且一旦需要修改字符串的话，就需要再次开辟空间，万恶的+号每出现一次就会在内从中重新开辟一块空间。 字符串格式化输出12345name = &quot;alex&quot;print &quot;i am %s &quot; % name #输出: i am alexPS: 字符串是 %s;整数 %d;浮点数%f 字符串常用功能： 移除空白 分割 长度 索引 切片 4、列表创建列表： 123name_list = [&apos;alex&apos;, &apos;seven&apos;, &apos;eric&apos;]或name_list ＝ list([&apos;alex&apos;, &apos;seven&apos;, &apos;eric&apos;]) 基本操作： 索引 切片 追加 删除 长度 循环 包含 5、元组(不可变列表) 创建元组： 123ages = (11, 22, 33, 44, 55)或ages = tuple((11, 22, 33, 44, 55)) 6、字典（无序） 创建字典： 123person = &#123;&quot;name&quot;: &quot;mr.wu&quot;, &apos;age&apos;: 18&#125;或person = dict(&#123;&quot;name&quot;: &quot;mr.wu&quot;, &apos;age&apos;: 18&#125;) 常用操作： 索引 新增 删除 键、值、键值对 循环 长度 十二、数据运算 成员运算 12in 如果在指定的序列中找到值返回ture，否则返回false 如:x在y的序列中，则返回truenot in 如果在指定的序列中没有找到值返回true，否则返回false 身份运算 12is 判断两个标识符是不是引用同一个对象 如:x is y 如果id(x)=id(y) 则返回结果1is not判断两个标识符不是引用同一个对象 如：x is not y 如果id(x)不等于id(y)则返回结果1 十三、表达式if … else场景一、用户登陆验证 1234567891011121314151617181920# 提示输入用户名和密码 # 验证用户名和密码# 如果错误，则输出用户名或密码错误# 如果成功，则输出 欢迎，XXX! #!/usr/bin/env python# -*- coding: encoding -*- import getpass name = raw_input(&apos;请输入用户名：&apos;)pwd = getpass.getpass(&apos;请输入密码：&apos;) if name == &quot;alex&quot; and pwd == &quot;cmd&quot;: print(&quot;欢迎，alex！&quot;)else: print(&quot;用户名和密码错误&quot;) 场景二、猜年龄游戏 在程序里设定好你的年龄，然后启动程序让用户猜测，用户输入后，根据他的输入提示用户输入的是否正确，如果错误，提示是猜大了还是小了 1234567891011121314#!/usr/bin/env python# -*- coding: utf-8 -*- my_age = 28 user_input = int(input(&quot;input your guess num:&quot;)) if user_input == my_age: print(&quot;Congratulations, you got it !&quot;)elif user_input &lt; my_age: print(&quot;Oops,think bigger!&quot;)else: print(&quot;think smaller!&quot;) 外层变量，可以被内层代码使用 内层变量，不应被外层代码使用 十四、表达式for loop最简单的循环10次 123456#_*_coding:utf-8_*___author__ = &apos;Alex Li&apos; for i in range(10): print(&quot;loop:&quot;, i ) 需求一：还是上面的程序，但是遇到小于5的循环次数就不走了，直接跳入下一次循环 1234for i in range(10): if i&lt;5: continue #不往下走了,直接进入下一次loop print(&quot;loop:&quot;, i ) 需求二：还是上面的程序，但是遇到大于5的循环次数就不走了，直接退出 1234for i in range(10): if i&gt;5: break #不往下走了,直接跳出整个loop print(&quot;loop:&quot;, i ) 十五、while loop 有一种循环叫死循环，一经触发，就运行个天荒地老、海枯石烂。 海枯石烂代码 12345678count = 0while True: print(&quot;你是风儿我是沙,缠缠绵绵到天涯...&quot;,count) count +=1``` 其实除了时间，没有什么是永恒的，死loop还是少写为好 上面的代码循环100次就退出吧 count = 0while True: print(“你是风儿我是沙,缠缠绵绵到天涯…”,count) count +=1 if count == 100: print(“去你妈的风和沙,你们这些脱了裤子是人,穿上裤子是鬼的臭男人..”) break 1回到上面for 循环的例子，如何实现让用户不断的猜年龄，但只给最多3次机会，再猜不对就退出程序。 #!/usr/bin/env python -- coding: utf-8 --my_age = 28 count = 0while count &lt; 3: user_input = int(input(“input your guess num:”)) if user_input == my_age: print(&quot;Congratulations, you got it !&quot;) break elif user_input &lt; my_age: print(&quot;Oops,think bigger!&quot;) else: print(&quot;think smaller!&quot;) count += 1 #每次loop 计数器+1else: print(“猜这么多次都不对,你个笨蛋.”) 12345678910111213141516171819## 入门知识拾遗### 一、bytes类型Python 3 新增了 bytes 类型，用于代表字节串（这是本教程创造的一个词，用来和字符串对应）。字符串（str）由多个字符组成，以字符为单位进行操作；字节串（bytes）由多个字节组成，以字节为单位进行操作。bytes 和 str 除操作的数据单元不同之外，它们支持的所有方法都基本相同，bytes 也是不可变序列。bytes 对象只负责以字节（二进制格式）序列来记录数据，至于这些数据到底表示什么内容，完全由程序决定。如果采用合适的字符集，字符串可以转换成字节串；反过来，字节串也可以恢复成对应的字符串。由于 bytes 保存的就是原始的字节（二进制格式）数据，因此 bytes 对象可用于在网络上传输数据，也可用于存储各种二进制格式的文件，比如图片、音乐等文件。如果希望将一个字符串转换成 bytes 对象，有如下三种方式：如果字符串内容都是 ASCII 字符，则可以通过直接在字符串之前添加 b 来构建字节串值。调用 bytes() 函数（其实是 bytes 的构造方法）将字符串按指定字符集转换成字节串，如果不指定字符集，默认使用 UTF-8 字符集。调用字符串本身的 encode() 方法将字符串按指定字符集转换成字节串，如果不指定字符集，默认使用 UTF-8 字符集。如果程序获得了 bytes 对象，也可调用 bytes 对象的 decode() 方法将其解码成字符串: #将bytes 对象解码成字符串，默认使用UTF-8进行解码st = b5.decode(‘utf-8’)print(st)#学习Python很有趣 12345678910python中，我们使用decode()和encode()来进行解码和编码### 二、三元运算result = 值1 if 条件 else 值2如果条件为真：result = 值1如果条件为假：result = 值2### 三、进制 二进制，01八进制，01234567十进制，0123456789十六进制，0123456789ABCDEF 二进制到16进制转换http://jingyan.baidu.com/album/47a29f24292608c0142399cb.html?picindex=1计算机内存地址和为什么用16进制？为什么用16进制1、计算机硬件是0101二进制的，16进制刚好是2的倍数，更容易表达一个命令或者数据。十六进制更简短，因为换算的时候一位16进制数可以顶4位2进制数，也就是一个字节（8位进制可以用两个16进制表示）2、最早规定ASCII字符集采用的就是8bit(后期扩展了,但是基础单位还是8bit)，8bit用2个16进制直接就能表达出来，不管阅读还是存储都比其他进制要方便3、计算机中CPU运算也是遵照ASCII字符集，以16、32、64的这样的方式在发展，因此数据交换的时候16进制也显得更好4、为了统一规范，CPU、内存、硬盘我们看到都是采用的16进制计算 16进制用在哪里 1、网络编程，数据交换的时候需要对字节进行解析都是一个byte一个byte的处理，1个byte可以用0xFF两个16进制来表达。通过网络抓包，可以看到数据是通过16进制传输的。 2、数据存储，存储到硬件中是0101的方式，存储到系统中的表达方式都是byte方式 3、一些常用值的定义，比如：我们经常用到的html中color表达，就是用的16进制方式，4个16进制位可以表达好几百万的颜色信息。 ### 四、 一切皆对象 对于Python，一切事物都是对象，对象基于类创建,并且是根据不同的类生成的对象。]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python基础之字符编码]]></title>
    <url>%2F2019%2F08%2F08%2Fpython%E5%9F%BA%E7%A1%80%E4%B9%8B%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%2F</url>
    <content type="text"><![CDATA[python基础之字符编码一. 什么是字符编码?计算机要想工作必须通电,即用‘电’驱使计算机干活,也就是说‘电’的特性决定了计算机的特性。电的特性即高低电平(人类从逻辑上将二进制数1对应高电平,二进制数0对应低电平)，关于磁盘的磁特性也是同样的道理。结论：计算机只认识数字。 很明显，我们平时在使用计算机时，用的都是人类能读懂的字符(用高级语言编程的结果也无非是在文件内写了一堆字符)，如何能让计算机读懂人类的字符?必须经过一个过程： 字符——–(翻译过程)——-&gt;数字，这个过程实际就是一个字符如何对应一个特定数字的标准，这个标准称之为字符编码。 二. 字符编码的发展史ASCII 记住一句话：计算机中的所有数据，不论是文字、图片、视频、还是音频文件，本质上最终都是按照类似 01010101 二进制存储的，再说简单点，计算机只懂二进制数字! 所以，目的明确了：如何将我们能识别的符号唯一的与一组二进制数字对应上?于是美利坚的同志想到通过一个电平的高低状态来代指0或1， 八个电平做为一组就可以表示出256种不同状态，每种状态就唯一对应一个字符，比如A—&gt;00010001,而英文只有26个字符，算上一些 特殊字符和数字，128个状态也够用了;每个电平称为一个比特为，约定8个比特位构成一个字节，这样计算机就可以用127个不同字节来存储英语的文字了。这就是ASCII编码。 扩展ANSI编码 刚才说了最开始，一个字节有八位，但是最高位没用上，默认为0;后来为了计算机也可以表示拉丁文，就将最后一位也用上了，从128到255的字符集对应拉丁文啦。至此，一个字节就用满了! GB2312 计算机漂洋过海来到中国后，问题来了，计算机不认识中文，当然也没法显示中文;而且一个字节所有状态都被占满了，万恶的帝国主义亡我之心不死啊!我党也是棒，自力更生，自己重写一张表，直接生猛地将扩展的第八位对应拉丁文全部删掉，规定一个小于127的字符的意义与原来相同，但两个大于127的字符连在一起时，就表示一个汉字，前面的一个字节(他称之为高字节)从0xA1用到0xF7，后面一个字节(低字节)从0xA1到0xFE，这样我们就可以组合出大约7000多个简体汉字了;这种汉字方案叫做 “GB2312”。GB2312 是对 ASCII 的中文扩展。 GBK 和 GB13030 但是汉字太多了，GB2312也不够用，于是规定：只要第一个字节是大于127就固定表示这是一个汉字的开始，不管后面跟的是不是扩展字符集里的内容。结果扩展之后的编码方案被称为 GBK 标准，GBK 包括了GB2312的所有内容，同时又增加了近20000个新的汉字(包括繁体字)和符号。 UNICODE编码 很多其它国家都搞出自己的编码标准，彼此间却相互不支持。这就带来了很多问题。于是，国际标谁化组织为了统一编码：提出了标准编码准 即：UNICODE，UNICODE是用两个字节来表示为一个字符，它总共可以组合出65535不同的字符，这足以覆盖世界上所有符号(包括甲骨文) UTF-8 都一统天下了，为什么还要有一个utf8的编码呢? 大家想，对于英文世界的人们来讲，一个字节完全够了，比如要存储A,本来00010001就可以了，现在吃上了unicode的大锅饭， 得用两个字节：00000000 00010001才行，浪费太严重! 基于此，美利坚的科学家们提出了天才的想法：utf8。 UTF-8(8-bit Unicode Transformation Format)是一种针对Unicode的可变长度字符编码，它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度，当字符在ASCII码的范围时，就用一个字节表示，所以是兼容ASCII编码的。 这样显著的好处是，虽然在我们内存中的数据都是unicode，但当数据要保存到磁盘或者用于网络传输时，直接使用unicode就远不如utf8省空间啦! 这也是为什么utf8是我们的推荐编码方式。 SCII编码和Unicode编码的区别：ASCII编码是1个字节，而Unicode编码通常是2个字节。 Unicode编码转化为“可变长编码”的UTF-8编码。UTF-8编码把一个Unicode字符根据不同的数字大小编码成1-6个字节，常用的英文字母被编码成1个字节，汉字通常是3个字节，只有很生僻的字符才会被编码成4-6个字节。如果你要传输的文本包含大量英文字符，用UTF-8编码就能节省空 unicode与utf8的关系： 一言以蔽之：Unicode是内存编码表示方案(是规范)，而UTF是如何保存和传输Unicode的方案(是实现)这也是UTF与Unicode的区别。 三. python2的string编码在python2中，有两种字符串类型：str类型和unicode类型;注意，这仅仅是两个名字，python定义的两个名字，关键是这两种数据类型在程序运行时存在内存地址的是什么? 我们来看一下： #coding:utf8s1=’苑’print type(s1) #print repr(s1) #’\xe8\x8b\x91s2=u’苑’print type(s2) #print repr(s2) # u’\u82d1’ 内置函数repr可以帮我们在这里显示存储内容。原来，str和unicode分别存的是字节数据和unicode数据;那么两种数据之间是什么关系呢?如何转换呢?这里就涉及到编码(encode)和解码(decode)了！ s1=u’苑’print repr(s1) #u’\u82d1’b=s1.encode(‘utf8’)print bprint type(b) #print repr(b) #’\xe8\x8b\x91’s2=’苑昊’u=s2.decode(‘utf8’)print u # 苑昊print type(u) #print repr(u) # u’\u82d1\u660a’ 无论是utf8还是gbk都只是一种编码规则，一种把unicode数据编码成字节数据的规则，所以utf8编码的字节一定要用utf8的规则解码，否则就会出现乱码或者报错的情况。 Python数据编码 四. python3的string编码python3也有两种数据类型：str和bytes;str类型存unicode数据，bytse类型存bytes数据，与python2比只是换了一下名字而已。 import jsons=’苑昊’print(type(s)) #print(json.dumps(s)) # “\u82d1\u660a”b=s.encode(‘utf8’)print(type(b)) #print(b) # b’\xe8\x8b\x91\xe6\x98\x8a’u=b.decode(‘utf8’)print(type(u)) #print(u) #苑昊print(json.dumps(u)) #”\u82d1\u660a”python3的string编码 五. 文件从磁盘到内存的编码 说到这，才来到我们的重点! 抛开执行执行程序，请问大家，文本编辑器大家都是用过吧，如果不懂是什么，那么word总用过吧，ok，当我们在word上编辑文字的时候，不管是中文还是英文，计算机都是不认识的，那么在保存之前数据是通过什么形式存在内存的呢?yes，就是unicode数据，为什么要存unicode数据，这是因为它的名字最屌：万国码!解释起来就是无论英文，中文，日文，拉丁文，世界上的任何字符它都有唯一编码对应，所以兼容性是最好的。 好，那当我们保存了存到磁盘上的数据又是什么呢? 答案是通过某种编码方式编码的bytes字节串。比如utf8—一种可变长编码，很好的节省了空间;当然还有历史产物的gbk编码等等。于是，在我们的文本编辑器软件都有默认的保存文件的编码方式，比如utf8，比如gbk。当我们点击保存的时候，这些编辑软件已经”默默地”帮我们做了编码工作。 那当我们再打开这个文件时，软件又默默地给我们做了解码的工作，将数据再解码成unicode,然后就可以呈现明文给用户了!所以，unicode是离用户更近的数据，bytes是离计算机更近的数据。 说了这么多，和我们程序执行有什么关系呢? 先明确一个概念：py解释器本身就是一个软件，一个类似于文本编辑器一样的软件! 现在让我们一起还原一个py文件从创建到执行的编码过程： 打开pycharm，创建hello.py文件，写入 ret=1+1s=’苑昊’print(s) 当我们保存的的时候，hello.py文件就以pycharm默认的编码方式保存到了磁盘;关闭文件后再打开，pycharm就再以默认的编码方式对该文件打开后读到的内容进行解码，转成unicode到内存我们就看到了我们的明文; 而如果我们点击运行按钮或者在命令行运行该文件时，py解释器这个软件就会被调用，打开文件，然后解码存在磁盘上的bytes数据成unicode数据，这个过程和编辑器是一样的，不同的是解释器会再将这些unicode数据翻译成C代码再转成二进制的数据流，最后通过控制操作系统调用cpu来执行这些二进制数据，整个过程才算结束。 那么问题来了，我们的文本编辑器有自己默认的编码解码方式，我们的解释器有吗? 当然有啦，py2默认ASCII码，py3默认的utf8，可以通过如下方式查询 import sysprint(sys.getdefaultencoding()) 大家还记得这个声明吗? #coding:utf8 是的，这就是因为如果py2解释器去执行一个utf8编码的文件，就会以默认地ASCII去解码utf8，一旦程序中有中文，自然就解码错误了，所以我们在文件开头位置声明coding:utf8，其实就是告诉解释器，你不要以默认的编码方式去解码这个文件，而是以utf8来解码。而py3的解释器因为默认utf8编码，所以就方便很多了。]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用nginx反向代理后如何在web应用获取用户ip及原理解释]]></title>
    <url>%2F2019%2F08%2F07%2F%E4%BD%BF%E7%94%A8nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E5%90%8E%E5%A6%82%E4%BD%95%E5%9C%A8web%E5%BA%94%E7%94%A8%E8%8E%B7%E5%8F%96%E7%94%A8%E6%88%B7ip%E5%8F%8A%E5%8E%9F%E7%90%86%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[使用nginx反向代理后如何在web应用获取用户ip及原理解释问题描述在实际实用中，我们可能需要获取用户的ip地址，比如做异地登陆的判断，或者统计ip访问次数等，通常情况下我们使用request.getRemoteAddr()就可以获取到时客户端ip，但是当我们使用了nginx作为反向代理后，使用request.getRemoteAddr()获取到的就一直是nginx服务器的ip的地址，这时应该怎么办? 解决方案1经过反向代理后，由于在客户端和web服务器之间增加了中间层，因此web服务器无法直接拿到客户端的ip，通过$remote_addr变量拿到的将是反向代理服务器的ip地址。这句话的意思是说，当你使用了nginx反向服务器后，在web端使用request.getRemoteAddr()(本质是就是获取$remote_addr),取得的是nginx的地址，即$remote_addr变量中封装的是nginx的地址，当然是没法获得用户的真实ip的，但是，nginx是可以获得用户的真实ip的，也就是说nginx使用$remote_addr变量时获得的是用户的真实ip，如果我们想要在web端获得用户的真实ip，就必须在nginx这里作一个赋值操作，如下： proxy_set_header X-real-ip $remote_addr; 其中X-real-ip是一个自定义的变量名，名字可以随意取，用户的真实ip就被放在X-real-ip这个变量里了，然后，在web端可以这样获取: request.getRemoteAddr(“X-real-ip”) 解决方案2通常我们会看到有这样一些配置 1234567891011121314151617181920212223242526272829server &#123; listen 88; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location /&#123; root html; index index.html index.htm; proxy_pass http://backend; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-real-ip $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # proxy_set_header X-Forwarded-For $http_x_forwarded_for; &#125; proxy_set_header X-real-ip $remote_addr; 有了这句就可以在web服务器端获得用户的真实ip，但是，实际上要获得用户的真实ip，不是只有这一个方法。还有其实方法，下面我们继续看。 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 首先这里有个X-Forwarded-For变量，这是一个squid开发的，用于识别通过http代理或负载均衡器原始ip一个连接到web服务器的客户机地址的非rfc标准，如果有做X-Forwarded-For设置的话，每次经过proxy转发都会有记录，格式是client1,proxy1,proxy2以逗号隔开各个地址，由于他是非rfc标准，所以默认是没有的，需要强制添加，在默认情况下经过proxy转发的请求，在后端看来远程地址都是proxy端的ip。也就是说在默认情况下我们使用request.getRemoteAddr(“X-Forwarded-For”)获取不到用户的ip，如果我们想要通过这个变量获得用户的ip，我们需要自己在nginx添加如下配置: proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 意思是增加一个$proxy_add_x_forwarded_for到X-Forwarded-For里去，注意是增加，而不是覆盖，当然由于默认的X-Forwarded-For值是空的，所以我们总感觉X-Forwarded-For的值就等于$proxy_add_x_forwarded_for的值，实际上当你搭建两台nginx在不同的ip上，并且都使用了这段配置，那你会发现在web服务器端通过request.getAttribute(“X-Forwarded-For”)获得的将会是客户端的ip和第一台nginx的ip。这样就清楚了吧。 最后我们看到还有一个$http_x_forwarded_for变量，这个变量就是X-Forwarded-For，由于之前我们说了，默认的这个X-Forwarded-For是为空的，所以当我们直接使用proxy_set_header X-Forwarded-For $http_x_forwarded_for时会发现，web服务器端使用request.getAttribute(“X-Forwarded-For”)获得的值是null。如果想要通过request.getAttribute(“X-Forwarded-For”)获得用户ip，就必须先使用proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;这样就可以获得用户真实ip。]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx反向代理配置和工作原理]]></title>
    <url>%2F2019%2F08%2F07%2Fnginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E9%85%8D%E7%BD%AE%E5%92%8C%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[nginx反向代理配置和工作原理nginx是一款面向性能设计的http服务器，其性能相对于其他服务器表现优异。内部使用异步的事件处理模型，比如linux平台的epoll事件模型，unix平台的kqueue事件模型等。在Nginx源码的src/event/modules目录下，其对各个平台不同的异步模型进行了二次封装。此外，Nginx在代码实现的时候，会考虑到众多细节优化。比如：根据CPU亲缘性来分配进程和事件，避免CPU级的缓存失效；比如字符串比较时，四字节转换为整数来进行快速指令级比较，等等。 nginx反向代理配置说明反向代理指以代理服务器来接受internet上的连接请求，它是一种通过在繁忙的web服务器和外部网络之间增加的一个高速web缓冲服务器，用来降低实际的web服务器的负载的一种技术。反向代理是针对web服务提高加速功能，所有外部网络要访问服务器时所有请求都要通过它，这样反向代理服务器负责接收客户端的请求，然后到源服务器上获取内容，把内容返回给用户，已减少后端web服务器的压力，提高响应速度。因此nginx还具有缓存功能。 nginx反向代理的设置: 1234567891011121314151617181920212223242526272829303132upstream cc_001 &#123; server 192.168.1.101:80; server 192.168.1.102:80; healthcheck_enabled; healthcheck_delay 3000; healthcheck_timeout 1000; healthcheck_failcount 2; healthcheck_send &apos;GET /healthcheck.html HTTP/1.0&apos; &apos;Host: local.com&apos; &apos;Connection: close&apos;; &#125; server &#123; listen 192.168.1.100:80; server_name cc.local.com; proxy_buffers 64 4k; location = / &#123; proxy_pass http://cc_001/bm/index.htm; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; location / &#123; proxy_pass http://cc_001; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125; upstream配置块其中upstream是一个非常重要的配置。nginx所有对于动态请求的处理，基本上都需要用upstream配置模块。nginx的两个重要功能，反向代理和负载均衡，都需要通过配置对应的upstream来完成。 在nginx中，有一个基础模块叫handler，这个模块可以接受来自客户端/用户端的请求，然后处理并产生对应的响应内容返回过去。因此，我们那些静态资源，前端页面什么的，都是使用handler模块来完成响应的。但是，众所知周，一般的核心服务都是后台动态产生的，这些资源就不可以方便使用handler去完成内容的生成和响应动作（当然也是可以使用开发自定义handler来完成的，比如各种xxxcgi之流，但是一般还是用来处理静态资源）。 那么，upstream就出现了。其接收到用户的请求，然后转发到后端服务器拿到对应的响应资源，再返回给请求端。在整个处理过程中，其本身不会产生自己的响应内容，这是和handler模块唯一的区别。 upstream的特性，决定了在其配置块中，设置一些后端服务器的地址和端口，就ok了。 配置项说明:upstream中的server项:表明后台的一台服务器和端口。当客户端有请求到nginx服务器的时候，upstream模块根据这里配置的server，该对应的请求转发到这些server服务上，由这些server来处理请求，然后把响应结果告知upstream模块。 healthcheck_enabled项：healthcheck健康监控功能，并不是原生nginx自带的。所以如果使用这个功能，必须要安装第三方插件：ngx_http_healthcheck_module。healthcheck_enabled表示启动健康检查模块功能。 healthcheck_delay项：对同一台后端服务器两次检测之间的时间间隔，单位毫秒，默认为1000。 healthcheck_timeout项：进行一次健康检测的超时时间，单位为毫秒，默认值2000。 healthcheck_failcount项：对一台后端服务器检测成功或失败多少次之后方才确定其为成功或失败，并实现启用或禁用此服务器。 healthcheck_send项：为了检测后端服务器的健康状态所发送的检测请求。然后根据各个服务器的响应情况来判断服务器是否存活。上面的配置表面，各个后台服务器上都存在healthcheck.html静态页面，然后nginx会get这个页面，根据是否status为200来判断是否服务器存活。 server配置块在nginx中，不管怎么样的配置，都会有一个server配置块。http服务上支持若干虚拟主机。每个虚拟主机会有一个对应的server配置项，配置项里面包含该虚拟主机相关的配置。在提供mail服务的代理时，也可以建立若干server.每个server通过监听的地址来区分。 Server其实就是一个虚拟主机。因为在nginx中可以配置多个server，这样就使得nginx可以在一台服务器上配置多个域名 在nginx的Server虚拟主机中，它只会处理与之对应的域名请求。并且，如果在listen中设置了ip地址，则该虚拟主机只会处理从该服务器的指定ip端口进来的请求，才会去处理。 配置项说明： listen项:监听ip和端口。当nginx服务器的该ip端口有请求访问，则调用该server的配置来处理该请求。 server_name：域名。nginx对进入该虚拟主机的请求，检查其请求host头是否匹配设置的server_name，如果是，则继续处理该请求。 location块选项:location在nginx中是一个非常重要的指令。对于http请求，其被用来详细匹配uri和设置的location path。一般这个uri path会是字符串或者正则表达式形式。 1234567location匹配，语法规则:location [=|~|~*|^~|@] /uri/ &#123; ... &#125; =：表示精确匹配，如果找到，立即停止搜索并立即处理此请求。 ~：表示区分大小写匹配。 ~*：表示不区分大小写匹配。 ^~：表示只匹配字符串不查询正则表达式。 @：指定一个命名的location，一般只用于内部重定向请求。 location中proxy_pass项:代理转发。配置了该项，当匹配location path请求进来后，会根据upstream设置，请求后台服务器上的proxy_pass的请求。例如，上面的配置，当有请求cc.local.com时，由于精确匹配=/，则根据proxy_pass配置，则会反向代理，请求192.168.1.101:80/bm/index.htm. location中的proxy_set_header项:设置代理请求头。由于经过了反向代理服务器，所以后台服务器不能获取真正的客户端请求地址信息，这样，就需要把这些ip地址，设置回请求头部中。然后，我们在后台服务上，可以使用request.get(“X-Real-IP”)或者request.get(“X-Forwarded-For”)获取真实的请求ip地址。获取host也是如此。]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx之proxy_pass详解]]></title>
    <url>%2F2019%2F08%2F07%2Fnginx%E4%B9%8Bproxy_pass%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[nginx之proxy_pass详解在nginx中配置proxy_pass代理转发时，如果在proxy_pass后面的url加/,表示绝对根路径;如果没有/，表示相对路径，把匹配的路径部分也给代理走。 如:用下面四种情况来访问http://192.168.1.1/proxy/test.html 1234567891011121314151617181920第一种：location /proxy/ &#123;proxy_pass http://127.0.0.1/;&#125;代理到URL：http://127.0.0.1/test.html第二种（相对于第一种，最后少一个 / ）location /proxy/ &#123;proxy_pass http://127.0.0.1;&#125;代理到URL：http://127.0.0.1/proxy/test.html第三种：location /proxy/ &#123;proxy_pass http://127.0.0.1/aaa/;&#125;代理到URL：http://127.0.0.1/aaa/test.html第四种（相对于第三种，最后少一个 / ）location /proxy/ &#123;proxy_pass http://127.0.0.1/aaa;&#125;代理到URL：http://127.0.0.1/aaatest.html nginx中有两个模块都有proxy_pass指令。 ngx_http_proxy_module的proxy_pass(四层代理): 语法: proxy_pass URL;场景: location, if in location, limit_except说明: 设置后端代理服务器的协议(protocol)和地址(address),以及location中可以匹配的一个可选的URI。协议可以是”http”或”https”。地址可以是一个域名或ip地址和端口，或者一个 unix-domain socket 路径。 详见官方文档: http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_passURI的匹配. ngx_stream_proxy_module的proxy_pass(七层代理)： 语法: proxy_pass address;场景: server说明: 设置后端代理服务器的地址。这个地址(address)可以是一个域名或ip地址和端口，或者一个 unix-domain socket路径。 详见官方文档: http://nginx.org/en/docs/stream/ngx_stream_proxy_module.html#proxy_pass 二、两个proxy_pass的关系与区别在两个模块中，两个proxy_pass都是用来做后端代理的指令。 ngx_stream_proxy_module模块的proxy_pass指令的只能在server段使用，只需要提供域名或ip地址和端口。可以理解为端口转发，可以是tcp端口，也可以是udp端口。 ngx_http_proxy_module模块的proxy_pass指令需要在location段，locaton中的if段，limit_except段中使用，处理需要提供域名或ip地址和端口外，还需要提供协议，如http或https，还有一个可选的uri可以配置。 三、proxy_pass的具体用法ngx_stream_proxy_module模块的proxy_pass指令 1234567891011121314151617181920212223server &#123; listen 127.0.0.1:12345; proxy_pass 127.0.0.1:8080;&#125; server &#123; listen 12345; proxy_connect_timeout 1s; proxy_timeout 1m; proxy_pass example.com:12345;&#125; server &#123; listen 53 udp; proxy_responses 1; proxy_timeout 20s; proxy_pass dns.example.com:53;&#125; server &#123; listen [::1]:12345; proxy_pass unix:/tmp/stream.socket;&#125; ngx_http_proxy_module模块的proxy_pass指令 1234567891011121314151617181920212223242526272829303132server &#123; listen 80; server_name www.test.com; # 正常代理，不修改后端url的 location /some/path/ &#123; proxy_pass http://127.0.0.1; &#125; # 修改后端url地址的代理（本例后端地址中，最后带了一个斜线) location /testb &#123; proxy_pass http://www.other.com:8801/; &#125; # 使用 if in location location /google &#123; if ( $geoip_country_code ~ (RU|CN) ) &#123; proxy_pass http://www.google.hk; &#125; &#125; location /yongfu/ &#123; # 没有匹配 limit_except 的，代理到 unix:/tmp/backend.socket:/uri/ proxy_pass http://unix:/tmp/backend.socket:/uri/;; # 匹配到请求方法为: PUT or DELETE, 代理到9080 limit_except PUT DELETE &#123; proxy_pass http://127.0.0.1:9080; &#125; &#125; &#125; 四、proxy_pass后，后端服务器的url(request_uri)情况分析12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182server &#123; listen 80; server_name www.test.com; # 情形A # 访问 http://www.test.com/testa/aaaa # 后端的request_uri为: /testa/aaaa location ^~ /testa/ &#123; proxy_pass http://127.0.0.1:8801; &#125; # 情形B # 访问 http://www.test.com/testb/bbbb # 后端的request_uri为: /bbbb location ^~ /testb/ &#123; proxy_pass http://127.0.0.1:8801/; &#125; # 情形C # 下面这段location是正确的 location ~ /testc &#123; proxy_pass http://127.0.0.1:8801; &#125; # 情形D # 下面这段location是错误的 # # nginx -t 时，会报如下错误: # # nginx: [emerg] &quot;proxy_pass&quot; cannot have URI part in location given by regular # expression, or inside named location, or inside &quot;if&quot; statement, or inside # &quot;limit_except&quot; block in /opt/app/nginx/conf/vhost/test.conf:17 # # 当location为正则表达式时，proxy_pass 不能包含URI部分。本例中包含了&quot;/&quot; location ~ /testd &#123; proxy_pass http://127.0.0.1:8801/; # 记住，location为正则表达式时，不能这样写！！！ &#125; # 情形E # 访问 http://www.test.com/ccc/bbbb # 后端的request_uri为: /aaa/ccc/bbbb location /ccc/ &#123; proxy_pass http://127.0.0.1:8801/aaa$request_uri; &#125; # 情形F # 访问 http://www.test.com/namea/ddd # 后端的request_uri为: /yongfu?namea=ddd location /namea/ &#123; rewrite /namea/([^/]+) /yongfu?namea=$1 break; proxy_pass http://127.0.0.1:8801; &#125; # 情形G # 访问 http://www.test.com/nameb/eee # 后端的request_uri为: /yongfu?nameb=eee location /nameb/ &#123; rewrite /nameb/([^/]+) /yongfu?nameb=$1 break; proxy_pass http://127.0.0.1:8801/; &#125; access_log /data/logs/www/www.test.com.log;&#125; server &#123; listen 8801; server_name www.test.com; root /data/www/test; index index.php index.html; rewrite ^(.*)$ /test.php?u=$1 last; location ~ \.php$ &#123; try_files $uri =404; fastcgi_pass unix:/tmp/php-cgi.sock; fastcgi_index index.php; include fastcgi.conf; &#125; access_log /data/logs/www/www.test.com.8801.log;&#125; 文件: /data/www/test/test.php 12&lt;?phpecho &apos;$_SERVER[REQUEST_URI]:&apos; . $_SERVER[&apos;REQUEST_URI&apos;]; 通过查看 $_SERVER[‘REQUEST_URI’] 的值，我们可以看到每次请求的后端的request_uri的值，进行验证。 小结 1234情形A和情形B进行对比，可以知道proxy_pass后带一个URI,可以是斜杠(/)也可以是其他uri，对后端request_uri变量的影响。情形D说明，当location为正则表达式时，proxy_pass不能包含URI部分。情形E通过变量($request_uri, 也可以是其他变量)，对后端的request_uri进行改写。情形F和情形G通过rewrite配合break标志,对url进行改写，并改写后端的request_uri。需要注意，proxy_pass地址的URI部分在情形G中无效，不管如何设置，都会被忽略]]></content>
      <categories>
        <category>nginx模块</category>
      </categories>
      <tags>
        <tag>nginx模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[系统状态统计和查看]]></title>
    <url>%2F2019%2F08%2F02%2F%E7%B3%BB%E7%BB%9F%E7%8A%B6%E6%80%81%E7%BB%9F%E8%AE%A1%E5%92%8C%E6%9F%A5%E7%9C%8B%2F</url>
    <content type="text"><![CDATA[统计和查看linux的系统状态/proc的意义及说明在linux中查看各种状态，其实质是查看内核中相关进程的数据结构中的项，通过工具将其格式化后输出出来。但是内核的数据是绝对不能随意查看或更改的，至少不能直接去修改。所以在linux上出现了伪文件系统/proc，这是内核中各属性或状态向外提供访问或修改的接口。 在/proc下，记录了内核自己的数据信息，各进程独立的数据信息，统计信息等。绝大多数文件都是只读不可改的。即使对root也一样，但/proc/sys除外。 其中数字命名的目录对应的是各进程的pid号，其内的文件记录的都是该进程当前的数据信息，且都是只读的，例如记录命令信息的cmdline文件，进程使用哪颗cpu信息cpuset，进程占用内存的信息mem文件，进程IO信息io文件等其他各种信息文件。 12345[root@xuexi ~]# ls /proc/6982attr clear_refs cpuset fd loginuid mounts numa_maps pagemap schedstat stat taskautogroup cmdline cwd fdinfo maps mountstats oom_adj personality sessionid statm wchanauxv comm environ io mem net oom_score root smaps statuscgroup coredump_filter exe limits mountinfo ns oom_score_adj sched stack syscall 非数字命名的目录各有用途，例如bus表示总线信息，driver表示驱动信息，fs表示文件系统特殊信息，net表示网络信息，tty表示跟物理终端有关的信息，最特殊的两个是/proc/self和/proc/sys。 先说/proc/self目录，它表示的是当前正在访问/proc目录的进程，因为/proc目录是内核数据向外记录的接口，所以当前访问/proc目录的进程表示的就是当前cpu正在执行的进程。如果执行cat /proc/self/cmdline，会发现其结果总是该命令本身，因为cat是手动敲入的命令，它是重要性进程，cpu会立即执行该命令。 再说/proc/sys这个目录，该目录是为管理员提供用来修改内核运行参数的，所以该目录中的文件对root都是可写的，例如管理数据包转发功能的/proc/sys/net/ipv4/ip_forward文件。使用sysctl命令修改内核运行参数，其本质也是修改/proc/sys目录中的文件。 查看进程信息 pstree命令pstree命令将以树的形式显示进程信息，默认树的分支是收拢的，也不显示pid，要显示这些信息需要指定对应的选项。 12345678pstree [-a] [-c] [-h] [-l] [-p] [pid]选项说明：-a：显示进程的命令行-c：展开分支-h：高亮当前正在运行的进程及其父进程-p：显示进程pid，此选项也将展开分支-l：允许显示长格式进程。默认在显示结果中超过132个字符时将截断后面的字符。 ps命令ps命令查看当前这一刻的进程信息，注意查看的是静态进程信息，要查看随时刷新的动态进程信息(如windows的进程管理器那样，每秒刷新一次)，使用top或htop命令。 这个命令的man文档及其复杂，它同时支持3种类型的选项：GUN/BSD/UNIX，不同类型的选项其展示的信息格式不一样。有些加了”-“的是SysV风格 的选项，不加”-“的是BSD选项，加不加”-“它们的意义是不一样的，例如ps aux 和ps -aux是不同的。 其实只需掌握少数几个选项即可，关键的是要了解ps显示出的进程信息中每一列代表什么属性。 对于BSD风格的选项，只需知道一个用法ps aux足以，选项”a”表示列出依赖于终端的进程，选项”x”表示列出不依赖于终端的进程，所以两者结合就表示列出所有进程，选项”u”表示展现的进程信息是以用户为导向的，不用管它什么是以用户为导向，用ps aux就没错。 123456789101112[root@server2 ~]# ps aux | tailUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1340 0.0 0.0 27176 588 ? Ss 20:30 0:00 /usr/sbin/xinetd -stayalive -pidfile /var/run/xinetd.pidroot 2266 0.0 0.1 93212 2140 ? Ss 20:30 0:00 /usr/libexec/postfix/master -wpostfix 2268 0.0 0.2 93384 3992 ? S 20:30 0:00 qmgr -l -t unix -upostfix 2306 0.0 0.2 93316 3972 ? S 20:31 0:00 pickup -l -t unix -uroot 2307 0.0 0.2 145552 5528 ? Ss 20:31 0:00 sshd: root@pts/0root 2309 0.0 0.0 0 0 ? S&lt; 20:31 0:00 [kworker/3:1H]root 2310 0.0 0.1 116568 3184 pts/0 Ss 20:31 0:00 -bashroot 2352 0.0 0.0 0 0 ? S&lt; 20:31 0:00 [kworker/1:2H]root 2355 0.0 0.0 139492 1632 pts/0 R+ 20:34 0:00 ps auxroot 2356 0.0 0.0 107928 676 pts/0 R+ 20:34 0:00 tail 各列的意义： 12345678910111213141516171819202122%CPU：表示CPU占用百分比，注意，CPU的衡量方式是占用时间，所以百分比的计算方式是&quot;进程占用cpu时间/cpu总时间&quot;，而不是cpu工作强度的状态。%MEM：表示各进程所占物理内存百分比。VSZ：表示各进程占用的虚拟内存，也就是其在线性地址空间中实际占用的内存。单位为kb。RSS：表示各进程占用的实际物理内存。单位为Kb。TTY：表示属于哪个终端的进程，&quot;?&quot;表示不依赖于终端的进程。STAT：进程所处的状态。 D：不可中断睡眠 R：运行中或等待队列中的进程(running/runnable) S：可中断睡眠 T：进程处于stopped状态 Z：僵尸进程 对于BSD风格的ps选项，进程的状态还会显示下面几个组合信息。 &lt;：高优先级进程 N：低优先级进程 L：该进程在内存中有被锁定的页 s：表示该进程是session leader，即进程组的首进程。例如管道左边的进程，shell脚本中的shell进程 l：表示该进程是一个线程 +：表示是前端进程。前端进程一般来说都是依赖于终端的START：表示进程是何时被创建的TIME：表示各进程占用的CPU时间COMMAND：表示进程的命令行。如果是内核线程，则使用方括号&quot;[]&quot;包围 注意到了没，ps aux没有显示出ppid。 另外常用的ps选项是ps -elf。其中”-e”表示输出全部进程信息，”-f”和”-l”分别表示全格式输出和长格式输出。全格式会输出cmd的全部参数。 123456789101112[root@server2 ~]# ps -lfF S UID PID PPID C PRI NI ADDR SZ WCHAN STIME TTY TIME CMD4 S postfix 2306 2266 0 80 0 - 23329 ep_pol 20:31 ? 00:00:00 pickup -l -t unix -u4 S root 2307 1141 0 80 0 - 36388 poll_s 20:31 ? 00:00:00 sshd: root@pts/01 S root 2309 2 0 60 -20 - 0 worker 20:31 ? 00:00:00 [kworker/3:1H]4 S root 2310 2307 0 80 0 - 29142 wait 20:31 pts/0 00:00:00 -bash1 S root 2433 2 0 60 -20 - 0 worker 21:21 ? 00:00:00 [kworker/1:1H]1 S root 2479 2 0 80 0 - 0 worker 21:25 ? 00:00:00 [kworker/1:0]1 S root 2503 2 0 60 -20 - 0 worker 21:28 ? 00:00:00 [kworker/1:2H]1 S root 2532 2 0 80 0 - 0 worker 21:30 ? 00:00:00 [kworker/1:1]0 R root 2539 2310 0 80 0 - 34873 - 21:33 pts/0 00:00:00 ps -elf0 S root 2540 2310 0 80 0 - 26982 pipe_w 21:33 pts/0 00:00:00 tail 各列的意义： 123456789F：程序的标志位。0表示该程序只有普通权限，4表示具有root超级管理员权限，1表示该进程被创建的时候只进行了fork，没有进行execS：进程的状态位，注意ps选项加了&quot;-&quot;的是非BSD风格选项，不会有&quot;s&quot;&quot;&lt;&quot;&quot;N&quot;&quot;+&quot;等的状态标识位C：CPU的百分比，注意衡量方式是时间PRI：进程的优先级，值越小，优先级越高，越早被调度类选中运行NI：进程的NICE值，值为-20到19，影响优先级的方式是PRI(new)=PRI(old)+NI，所以NI为负数的时候，越小将导致进程优先级越高。 ：但要注意，NICE值只能影响非实时进程。ADDR：进程在物理内存中哪个地方。SZ：进程占用的实际物理内存WCHAN：若进程处于睡眠状态，将显示其对应内核线程的名称，若进程为R状态，则显示&quot;-&quot; ps后grep问题在ps后加上grep筛选目标进程时，总会发现grep自身进程也被显示出来。先解释下为何会如此。 123[root@xuexi ~]# ps aux | grep &quot;crond&quot;root 1425 0.0 0.1 117332 1276 ? Ss Jun10 0:00 crondroot 8275 0.0 0.0 103256 856 pts/2 S+ 17:07 0:00 grep crond 通过管道将ps结果传递给grep时，管道协调了ps和grep两进程间通信，但管道的本质是进程间数据传递。管道左边的输出数据放入内存，由管道右边的进程读取。假如划分的内存不足以完全存放输出数据，则管道左边的进程将一直等待，直到管道右边取出内存中一部分的数据以让管道左边的进程继续输出，而管道右边的进程在管道左边的进程启动后也立刻启动了(实际上，管道左右两边的进程没有启动的先后顺序，谁先启动完全看内核对这两个进程的调度顺序，换句话说，看人品)，但是它一直处于等待状态，等待接收管道传递来的数据（就像是平时执行命令时不给输入文件将会一直等待输入一样）。 也就是说，管道左右两端的进程是同时被创建的(不考虑父进程创建进程消耗的那点时间)，但数据传输是有先后顺序的，左边先传，右边后收，所以可能会造成交叉的情况，左边还没执行完，就捕获到了右边的进程信息。在此处体现在ps还没有统计完进程信息时，grep进程就已经被ps抓到了。 要将grep自身进程排除在结果之外，方法有二： 12345[root@xuexi ~]# ps aux | grep &quot;crond&quot; | grep -v &quot;grep&quot; # 使用-v将grep自己筛选掉root 1425 0.0 0.1 117332 1276 ? Ss Jun10 0:00 crond [root@xuexi ~]# ps aux | grep &quot;cron[d]&quot;root 1425 0.0 0.1 117332 1276 ? Ss Jun10 0:00 crond 第二种方法能成功是因为grep进程被ps捕获时的结果是”grep cron[d]”，而使用cron[d]匹配时，它将只能匹配crond，所以”grep cron[d]”被筛选掉了。其实加上其他字符将更容易理解。 12[root@xuexi ~]# ps aux | grep &quot;cron[dabc]&quot;root 1425 0.0 0.1 117332 1276 ? Ss Jun10 0:00 crond uptime命令 12[root@xuexi ~]# uptime 08:38:11 up 22:35, 2 users, load average: 0.00, 0.01, 0.05 显示当前时间，已开机运行多少时间，当前有多少用户已登录系统，以及3个平均负载值。 所谓负载率(load)，即特定时间长度内，cpu运行队列中的平均进程数(包括线程)，一般平均每分钟每核的进程数小于3都认为正常，大于5时负载已经非常高。在UNIX系统中，运行队列包括cpu正在执行的进程和等待cpu的进程(即所谓的可运行runable)。在Linux系统中，还包括不可中断睡眠态(IO等待)的进程。运行队列中每出现一个进程，load就加1，进程每退出运行队列，Load就减1。如果是多核cpu，则还要除以核数。 详细信息见man uptime和https://en.wikipedia.org/wiki/Load_(computing) 例如，单核cpu上的负载值为”1.73 0.60 7.98”时，表示： 最近1分钟：1.73表示平均可运行的进程数，这一分钟要一直不断地执行这1.73个进程。0.73个进程等待该核cpu。 最近5分钟：平均进程数还不足1，表示该核cpu在过去5分钟空闲了40%的时间。 最近15分钟：7.98表示平均可运行的进程数，这15分钟要一直不断地执行这7.98个进程。 结合前5分钟的结果，说明前15-前10分钟时间间隔内，该核cpu的负载非常高。 如果是多核cpu，则还要将结果除以核数。例如4核时，某个最近一分钟的负载值为3.73，则意味着有3.73个进程在运行队列中，这些进程可被调度至4核中的任何一个核上运行。最近1分钟的负载值为1.6，表示这一分钟内每核cpu都空闲(1-1.6/4)=60%的时间。所以，load的理想值是正好等于CPU的核数，小于核数的时候表示cpu有空闲，超出核数的时候表示有进程在等待cpu，即系统资源不足。 top、htop以及iftop命令top命令查看动态进程状态，默认每5秒刷新一次。 1234567891011121314151617181920top选项说明：-d：指定top刷新的时间间隔，默认是5 秒-b：批处理模式，每次刷新分批显示-n：指定top刷新几次就退出，可以配合-b使用-p：指定监控的pid，指定方式为-pN1 -pN2 ...或-pN1, N2 [,...]-u：指定要监控的用户的进程，可以是uid也可以是user_name在top动态模式下，按下各种键可以进行不同操作。使用&quot;h&quot;或&quot;?&quot;可以查看相关键的说明。 1 ：(数字一)表示是否要在top的头部显示出多个cpu信息 H ：表示是否要显示线程，默认不显示 c,S ： c表示是否要展开进程的命令行，S表示显示的cpu时间是否是累积模式，cpu累积模式下已死去的子进程cpu时间会累积到父进程中 x,y ：x高亮排序的列，y表示高亮running进程 u ：仅显示指定用户的进程 n or #：设置要显示最大的进程数量 k ：杀进程 q ：退出top P ：以CPU 的使用资源排序显示 M ：以Memory 的使用资源排序显示 N ：以PID 来排序 以下是top的一次结果。 12345678910111213141516[root@xuexi ~]# toptop - 17:43:44 up 1 day, 14:16, 2 users, load average: 0.10, 0.06, 0.01Tasks: 156 total, 1 running, 155 sleeping, 0 stopped, 0 zombieCpu0 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu1 : 0.0%us, 0.0%sy, 0.0%ni, 99.7%id, 0.0%wa, 0.0%hi, 0.3%si, 0.0%stCpu2 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu3 : 0.3%us, 0.0%sy, 0.0%ni, 99.7%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stMem: 1004348k total, 417928k used, 586420k free, 52340k buffersSwap: 2047996k total, 0k used, 2047996k free, 243800k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 19364 1444 1132 S 0.0 0.1 0:00.96 init 2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd 3 root RT 0 0 0 0 S 0.0 0.0 0:01.28 migration/0 4 root 20 0 0 0 0 S 0.0 0.0 0:00.59 ksoftirqd/0 5 root RT 0 0 0 0 S 0.0 0.0 0:00.00 stopper/0 第1行：和w命令的第一行一样，也和uptime命令的结果一样。此行各列分别表示”当前时间”、”已开机时长”、”当前在线用户”、”前1、5、15分钟平均负载率”。 第2行：分别表示总进程数、running状态的进程数、睡眠状态的进程数、停止状态进程数、僵尸进程数。 第3-6行：每颗cpu的状况。 us = user mode sy = system mode ni = low priority user mode (nice)(用户空间中低优先级进程的cpu占用百分比) id = idle task wa = I/O waiting hi = servicing IRQs(不可中断睡眠，hard interruptible) si = servicing soft IRQs(可中断睡眠，soft interruptible) st = steal (time given to other DomU instances)(被偷走的cpu时间，一般被虚拟化软件偷走)第7-8行：从字面意思理解即可。 VIRT：虚拟内存总量 RES：实际内存总量 SHR：共享内存量 TIME：进程占用的cpu时间(若开启了时间累积模式，则此处显示的是累积时间) top命令虽然非常强大，但是太老了。所以有了新生代的top命令htop。htop默认没有安装，需要手动安装。 1[root@xuexi ~]# yum -y install htop iftop用于动态显示网络接口的数据流量。用法也很简单，按下h键即可获取帮助。 负载高、CPU空闲，说明当前正在执行的任务基本不消耗CPU资源，大量的负载进程都在IO等待中。 可以从ps的进程状态中获取哪些进程是正在运行或运行队列中的(状态为R)，哪些进程是在不可中断睡眠中的(状态为D)。 12[root@xuexi src]# ps -eTo stat,pid,ppid,comm --no-header |grep -E &quot;^(D|R)&quot;R+ 11864 9624 ps vmstat命令注意vmstat的第一次统计是自开机起的平均值信息，从第二次开始的统计才是指定刷新时间间隔内的资源利用信息，若不指定刷新时间间隔，则默认只显示一次统计信息。 12vmstat [-d] [delay [ count]]vmstat [-f] 12345678910111213141516171819202122232425262728293031323334353637383940414243选项说明：-f：统计自开机起fork的次数。包括fork、clone、vfork的次数。但不包括exec次数。-d：显示磁盘统计信息。delay：刷新时间间隔，若不指定，则只统计一次信息就退出vmstat。count：总共要统计的次数。例如，只统计一次信息。[root@xuexi ~]# vmstatprocs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 583692 52684 244200 0 0 5 3 4 5 0 0 100 0 0其中各列的意义如下：Procs r: 等待队列中的进程数 b: 不可中断睡眠的进程数Memory swpd: 虚拟内存使用总量 free: 空闲内存量 buff: buffer占用的内存量(buffer用于缓冲) cache: cache占用的内存量(cache用于缓存)Swap si:从磁盘加载到swap分区的数据流量，单位为&quot;kb/s&quot; so: 从swap分区写到磁盘的数据流量，单位为&quot;kb/s&quot;IO bi: 从块设备接受到数据的速率，单位为blocks/s bo: 发送数据到块设备的速率，单位为blocks/sSystem in: 每秒中断数，包括时钟中断数量 cs: 每秒上下文切换次数CPU：统计的是cpu时间百分比，具体信息和top的cpu统计列一样 us: Time spent running non-kernel code. (user time, including nice time) sy: Time spent running kernel code. (system time) id: Time spent idle. Prior to Linux 2.5.41, this includes IO-wait time. wa: Time spent waiting for IO. Prior to Linux 2.5.41, included in idle. st: Time stolen from a virtual machine. Prior to Linux 2.6.11, unknown.还可以统计磁盘的IO信息。统计信息的结果很容易看懂，所以略过。 iostat命令123456789101112131415161718192021222324252627282930313233iostat主要统计磁盘或分区的整体使用情况。也可以输出cpu信息，甚至是NFS网络文件系统的信息。同vmstat/sar一样，第一次统计的都是自系统开机起的平均统计信息。iostat [ -c ] [ -d ] [ -n -h ][ -k | -m ] [ -p [device][,...] ] [ interval [ count ] ]选项说明：-c：统计cpu信息-d：统计磁盘信息-n：统计NFS文件系统信息-h：使NFS统计信息更人类可读化-k：指定以kb/s为单位显示-m：指定以mb/s为单位显示-p：指定要统计的设备名称-y：指定不显示第一次统计信息，即不显示自开机起的统计信息。interval：刷新时间间隔count：总统计次数例如：[root@xuexi ~]# iostatLinux 2.6.32-504.el6.x86_64 (xuexi.longshuai.com) 06/11/2017 _x86_64_ (4 CPU) avg-cpu: %user %nice %system %iowait %steal %idle 0.01 0.00 0.03 0.01 0.00 99.96Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnsda 0.58 39.44 23.14 5557194 3259968sdb 0.00 0.03 0.00 4256 0各列的意义都很清晰，从字面即可理解。tps：每秒transfer速率(transfers per second),一次对物理设备的IO请求为一个transfer，但多个逻辑请求可能只组成一个transferBlk_read/s：每秒读取的block数量Blk_wrtn/s：每秒写入的block总数Blk_read：读取的总block数量Blk_wrtn：写入的总block数量 free命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051free用于查看内存使用情况。CentOS 6和CentOS 7上显示格式不太一样。free [options]选项说明：-h：人类可读方式显式单位-m：以MB为显示单位-w：将buffers和cache分开单独显示。只对CentOS 7上有效-s：动态查看内存信息时的刷新时间间隔-c：一共要刷新多少次退出free以下以CentOS 7上的free结果说明各列的意义。[root@server2 ~]# free -m total used free shared buff/cache availableMem: 1824 131 1286 8 407 1511Swap: 1999 0 1999Mem和Swap分别表示物理内存和交换分区的使用情况。total：总内存空间used：已使用的内存空间。该值是total-free-buffers-cache的结果free：未使用的内存空间shared：/tmpfs总用的内存空间。对内核版本有要求，若版本不够，则显示为0。buff/cache：buffers和cache的总占用空间available：可用的内存空间。即程序启动时，将认为可用空间有这么多。可用的内存空间为free+buffers+cache。所以available才是真正需要关注的可使用内存空间量。使用-w可以将buffers/cache分开显示。[root@server2 ~]# free -w -m total used free shared buffers cache availableMem: 1824 131 1286 8 0 406 1511Swap: 1999 0 1999还可以动态统计内存信息，例如每秒统计一次，统计2次。[root@server2 ~]# free -w -m -s 1 -c 2 total used free shared buffers cache availableMem: 1824 130 1287 8 0 406 1512Swap: 1999 0 1999 total used free shared buffers cache availableMem: 1824 130 1287 8 0 406 1512Swap: 1999 0 1999以下是CentOS 6上的free结果。[root@xuexi ~]# free -m total used free shared buffers cachedMem: 980 415 565 0 53 239-/+ buffers/cache: 121 859Swap: 1999 0 1999在此结果中，&quot;-/+ buffers/cache&quot;的free列才是真正可用的内存空间了，即CentOS 7上的available列。一般来说，内存可用量的范围低于20%应该要引起注意了。 sar命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137sar是一个非常强大的性能分析工具，它可以获取系统的cpu/等待队列/磁盘IO/内存/网络等性能指标。功能多的必然结果是选项多，应用复杂，但只要知道一些常用的选项足以。sar [options] [-o filename] [delay [count] ]选项说明：-A：显示系统所有资源运行状况-b：显示磁盘IO和tranfer速率信息，和iostat的信息一样，是总体IO统计信息-d：显示磁盘在刷新时间间隔内的活跃情况，可以指定一个或多个设备，和-b不同的是，它显示的是单设备的IO、transfer信息。 ：建议配合-p使用显示友好的设备名，否则默认显示带主次设备号的设备名-P：显示指定的某颗或某几颗cpu的使用情况。指定方式为，-P 0,1,2,3或ALL。-u：显示每颗cpu整体平均使用情况。-u和-P的区别通过下面的示例很容易区分。-r：显示内存在刷新时间间隔内的使用情况-n：显示网络运行状态。后可接DEV/NFS/NFSD/ALL等多种参数。 ：DEV表示显示网路接口信息，NFS和NFSD分别表示显示NFS客户端服务端的流量信息，ALL表示显示所有信息。-q：显示等待队列大小-o filename：将结果存入到文件中delay：状态刷新时间间隔count：总共刷新几次10.5.1 统计cpu使用情况[root@server2 ~]# sar -P ALL 1 2Linux 3.10.0-327.el7.x86_64 (server2.longshuai.com) 06/20/2017 _x86_64_ (4 CPU)01:18:49 AM CPU %user %nice %system %iowait %steal %idle01:18:50 AM all 0.00 0.00 0.25 0.00 0.00 99.7501:18:50 AM 0 0.00 0.00 0.00 0.00 0.00 100.0001:18:50 AM 1 0.00 0.00 0.00 0.00 0.00 100.0001:18:50 AM 2 0.00 0.00 0.00 0.00 0.00 100.0001:18:50 AM 3 0.00 0.00 0.00 0.00 0.00 100.00 01:18:50 AM CPU %user %nice %system %iowait %steal %idle01:18:51 AM all 0.00 0.00 0.00 0.00 0.00 100.0001:18:51 AM 0 0.00 0.00 0.00 0.00 0.00 100.0001:18:51 AM 1 0.00 0.00 0.99 0.00 0.00 99.0101:18:51 AM 2 0.00 0.00 0.00 0.00 0.00 100.0001:18:51 AM 3 0.00 0.00 0.00 0.00 0.00 100.00 Average: CPU %user %nice %system %iowait %steal %idleAverage: all 0.00 0.00 0.12 0.00 0.00 99.88Average: 0 0.00 0.00 0.00 0.00 0.00 100.00Average: 1 0.00 0.00 0.50 0.00 0.00 99.50Average: 2 0.00 0.00 0.00 0.00 0.00 100.00Average: 3 0.00 0.00 0.00 0.00 0.00 100.00各列的意义就不再赘述了，在前面几个信息查看命令已经解释过多次了。在上面的例子中，统计了所有cpu(0,1,2,3共4颗)每秒的状态信息，每秒还进行了一次汇总，即all，最后还对每颗cpu和汇总all计算了平均值。而我们真正需要关注的是最后的average部分的idle值，idle越小，说明cpu处于空闲时间越少，该颗或整体cpu使用率就越高。或者直接对整体进行统计。如下：[root@server2 ~]# sar -u 1 2Linux 3.10.0-327.el7.x86_64 (server2.longshuai.com) 06/20/2017 _x86_64_ (4 CPU)01:18:37 AM CPU %user %nice %system %iowait %steal %idle01:18:39 AM all 0.00 0.00 0.00 0.00 0.00 100.0001:18:40 AM all 0.00 0.00 0.23 0.00 0.00 99.77Average: all 0.00 0.00 0.12 0.00 0.00 99.8810.5.2 统计内存使用情况其中kbdirty表示内存中脏页的大小，即内存中还有多少应该刷新到磁盘的数据。10.5.3 统计网络流量第一种方法是查看/proc/net/dev文件。关注列：receive和transmit分别表示收包和发包，关注每个网卡的bytes即可获得网卡的情况。写一个脚本计算每秒的差值即为网络流量。或者使用sar -n命令统计网卡接口的数据。[root@server2 ~]# sar -n DEV 1 2Linux 3.10.0-327.el7.x86_64 (server2.longshuai.com) 06/20/2017 _x86_64_ (4 CPU)01:51:11 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s01:51:12 AM eth0 0.00 0.00 0.00 0.00 0.00 0.00 0.0001:51:12 AM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.0001:51:12 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s01:51:13 AM eth0 0.99 0.99 0.06 0.41 0.00 0.00 0.0001:51:13 AM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/sAverage: eth0 0.50 0.50 0.03 0.21 0.00 0.00 0.00Average: lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00各列的意义如下：rxpck/s：每秒收到的包数量txpck/s：每秒发送的包数量rxkB/s：每秒收到的数据，单位为kbtxkB/s：每秒发送的数据，单位为kbrxcmp/s：每秒收到的压缩后的包数量txcmp/s：每秒发送的压缩后的包数量rxmcst/s：每秒收到的多播包数量10.5.4 查看队列情况[root@server2 ~]# sar -qLinux 3.10.0-327.el7.x86_64 (server2.longshuai.com) 06/20/2017 _x86_64_ (4 CPU)12:00:01 AM runq-sz plist-sz ldavg-1 ldavg-5 ldavg-15 blocked12:10:01 AM 0 446 0.01 0.02 0.05 012:20:01 AM 0 445 0.02 0.03 0.05 012:30:01 AM 0 446 0.00 0.01 0.05 0Average: 0 446 0.01 0.02 0.05 0每列意义解释：runq-sz：等待队列的长度，不包括正在运行的进程plist-sz：任务列表中的进程数量，即总任务数ldavg-N：过去1分钟、5分钟、15分钟内系统的平均哎blocked：当前因为IO等待被阻塞的任务数量10.5.5 统计磁盘IO情况[root@server2 ~]# sar -d -p 1 2Linux 3.10.0-327.el7.x86_64 (server2.longshuai.com) 06/20/2017 _x86_64_ (4 CPU) 12:53:06 AM DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util12:53:07 AM sda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 12:53:07 AM DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util12:53:08 AM sda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %utilAverage: sda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00分别统计的是12:53:06到12:53:07和12:53:07到12:53:08这两秒的IO使用情况。各列的意义如下：tps：transfer per second，每秒的transfer速率，一次物理IO请求算一次transfer，但多次逻辑IO请求可能组合起来才算一次transfer。rd_sec/s：每秒读取的扇区数，扇区大小为512字节。wr_sec/s：每秒写入的扇区数。avgrq-sz：请求写入设备的平均大小，单位为扇区。(The average size (in sectors) of the requests that were issued to the device)avgqu-sz：请求写入设备的平均队列长度。(The average queue length of the requests that were issued to the device.)await：写入设备的IO请求的平均(消耗)时间，单位微秒(The average time for I/O requests issued to the device to be served.)svctm：不可信的列，该列未来将被移除，所以不用管%util：最重要的一列，显示的是设备的带宽情况。该列若接近100%，说明磁盘速率饱和了。]]></content>
      <categories>
        <category>linux基础</category>
      </categories>
      <tags>
        <tag>linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程与信号分析]]></title>
    <url>%2F2019%2F07%2F31%2F%E8%BF%9B%E7%A8%8B%E4%B8%8E%E4%BF%A1%E5%8F%B7%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/f-ck-need-u/p/7048359.html linux进程和信号分析进程与程序的区别程序是二进制文件，是静态存放在磁盘上的，不会占用系统运行资源(cpu/内存)。 进程是用户执行程序或者触发程序的结果，可以认为进程是程序的一个运行实例。进程是动态的，会申请和使用系统资源，并与所作系统内核进行交互。 多任务和cpu时间片现在所有的操作系统都能同时运行多个进程，也就是多任务或者说是并行执行。但实际上这是一种错觉，一颗物理cpu在同一个时刻只能运行一个进程，只有多颗物理cpu才能真正意义上实现多任务。 之所有会产生错觉，以为操作系统能并行做几件事情，这是通过在极短时间内进程进程间切换实现的，因为时间极短，前一刻执行是进程A,下一刻切换到进程B,不断的在多个进程间进行切换，使得我们以为在同时处理多件事情。 不过，cpu如何选择下一个要执行的进程，这是一件非常复杂的事情。在linux上，决定下一个要运行的进程是通过”调度类”(调试程度)来实现的。程度何时运行，由进程的优先级决定，但要注意，优先级值越低，就越快被调试类选中。除此之外，优先级还影响分配给进程的时间片长短。在linux中，改变进程的nice值，可以影响某类进程的优先级值。 有些进程比较重要，要让其尽快完成，有些进程则比较次要，早点或晚点完成不会有太大影响，所以操作系统要能够知道哪些进程比较重要。比较重要的进程，应该多给它分配一些cpu的执行时间，让其尽快完成。 由此可以知道，所有的进程都有机会运行，但重要的进程总是会获得更多的cpu时间，这种方式是”抢占式多任务处理”：内核可以强制在时间片耗尽的情况下收回cpu使用权，并将cpu交给调试类选中的进程，此外，在某些情况下也可以直接抢占当前运行的进程，随着时间的流逝，分配给进程的时间也会被逐渐消耗，当分配时间消耗完毕时，内核收回此进程的控制权，并让下一个进程运行。但因为前面的进程还没有完成，在未来某个时候调试类还是会选中它，所以内核应该将每个进程临时停止时的运行环境(寄存器中的内容和页表)保存下来(保存位置为内核占用的内存)，这称为保护现场，在下次进程恢复运行时，将原来的运行时环境加载到cpu上，这称为恢复现场，这样cpu可以在当初的运行时环境下继续执行。 调度类选中了下一个要执行的进程后，要进行底层的任务切换，也就是上下文切换，这一过程需要和cpu进程紧密的交互。进程切换不应太频繁，也不应太慢。切换太频繁将导致cpu闲置在保护和恢复现场的时间过长，保护和恢复现场对人类或者进程来说是没有产生生产力的(因为它没有在执行程序)。切换太慢将导致进程调度切换慢，很可能下一个进程要等待很久才能轮到它执行，直白的说，如果你发出一个ls命令，你可能要等半天，这显然是不允许的。 至此，也就知道了cpu的衡量单位是时间，就像内存的衡量单位是空间大小一样。进程占用的cpu时间长，说明cpu运行在它身上的时间就长。注意，cpu的百分比值不是其工作强度或频率高低，而是”进程占用cpu时间/cpu总时间”，这个衡量概念一定不要搞错。 父子进程及创建进程的方式根据执行程序的用户UID以及其他标准，会为每一个进程分配一个唯一的PID。 父子进程的概念，简单来说，在某进程(父进程)的环境下执行或调用程序，这个程序触发的进程就是子进程，而进程的PPID表示的是该进程的父进程的PID。由此也知道了，子进程总是由父进程创建。 在Linux，父子进程以树型结构的方式存在，父进程创建出来的多个子进程之间称为兄弟进程。CentOS 6上，init进程是所有进程的父进程，CentOS 7上则为systemd。 Linux上创建子进程的方式有三种(极其重要的概念)：一种是fork出来的进程，一种是exec出来的进程，一种是clone出来的进程。 (1).fork是复制进程，它会复制当前进程的副本(不考虑写时复制的模式)，以适当的方式将这些资源交给子进程。所以子进程掌握的资源和父进程是一样的，包括内存中的内容，所以也包括环境变量和变量。但父子进程是完全独立的，它们是一个程序的两个实例。 (2).exec是加载另一个应用程序，替代当前运行的进程，也就是说在不创建新进程的情况下加载一个新程序。exec还有一个动作，在进程执行完毕后，退出exec所在环境(实际上是进程直接跳转到exec上，执行完exec就直接退出。而非exec加载程序的方式是：父进程睡眠，然后执行子进程，执行完后回到父进程，所以不会立即退出当前环境)。所以为了保证进程安全，若要形成新的且独立的子进程，都会先fork一份当前进程，然后在fork出来的子进程上调用exec来加载新程序替代该子进程。例如在bash下执行cp命令，会先fork出一个bash，然后再exec加载cp程序覆盖子bash进程变成cp进程。但要注意，fork进程时会复制所有内存页，但使用exec加载新程序时会初始化地址空间，意味着复制动作完全是多余的操作，当然，有了写时复制技术不用过多考虑这个问题。 (3).clone用于实现线程。clone的工作原理和fork相同，但clone出来的新进程不独立于父进程，它只会和父进程共享某些资源，在clone进程的时候，可以指定要共享的是哪些资源。 题外知识：如何创建一个子进程？ 每次fork一个进程的时候，虽然调用一次fork()，会分别为两个进程返回两个值：对子进程的返回值为0，对父进程的返回值是子进程的pid。所以，可以使用下面的shell伪代码来描述运行一个ls命令时的过程： 123456fpid=`fork()`if [ $fpid = 0 ]&#123; exec(ls) || echo &quot;Can&apos;t exec ls&quot; exit&#125;wait($fpid) 假设上面是在shell脚本中执行ls命令，那么fork的是shell脚本进程。fork后，父进程将继续执行，且if语句判断失败，于是执行wait；而子进程执行时将检测到fpid=0，于是执行exec(ls)，当ls执行结束，子进程因为exec的原因将退出。于是父进程的wait等待完成，继续执行后面的代码。 如果在这个shell脚本中某个位置，执行exec命令(exec命令调用的其实就是exec家族函数)，shell脚本进程直接切换到exec命令上，执行完exec命令，就表示进程终止，于是exec命令后面的所有命令都不会再执行。 一般情况下，兄弟进程之间是相互独立、互不可见的，但有时候通过特殊手段，它们会实现进程间通信。例如管道协调了两边的进程，两边的进程属于同一个进程组，它们的PPID是一样的，管道使得它们可以以”管道”的方式传递数据。 进程是有所有者的，也就是它的发起者，某个用户如果它非进程发起者、非父进程发起者、非root用户，那么它无法杀死进程。且杀死父进程(非终端进程)，会导致子进程变成孤儿进程，孤儿进程的父进程总是init/systemd。 进程的状态进程并非总是处于运行中，至少cpu没运行在它身上时它就是非运行的。进程有几种状态，不同的状态之间可以实现状态切换。 运行态：进程正在运行，也即是cpu正在它身上。 就绪(等待)态：进程可以运行，已经处于等待队列中，也就是说调度类下次可能会选中它 睡眠(阻塞)态：进程睡眠了，不可运行。 各状态之间的转换方式为：(也许可能不太好理解，可以结合稍后的例子) (1)新状态－＞就绪态：当等待队列允许接纳新进程时，内核便把新进程移入等待队列。 (2)就绪态－＞运行态：调度类选中等待队列中的某个进程，该进程进入运行态。 (3)运行态－＞睡眠态：正在运行的进程因需要等待某事件(如IO等待、信号等待等)的出现而无法执行，进入睡眠态。 (4)睡眠态－＞就绪态：进程所等待的事件发生了，进程就从睡眠态排入等待队列，等待下次被选中执行。 (5)运行态－＞就绪态：正在执行的进程因时间片用完而被暂停执行；或者在抢占式调度方式中，高优先级进程强制抢占了正在执行的低优先级进程。 (6)运行态－＞终止态：一个进程已完成或发生某种特殊事件，进程将变为终止状态。对于命令来说，一般都会返回退出状态码。 没有”就绪–&gt;睡眠”和”睡眠–&gt;运行”的状态切换。这很容易理解。对于”就绪–&gt;睡眠”,等待中的进程本就已经进入了等待队列，表示可运行，而进入睡眠态表示暂时不可运行，这本身就是冲突的；对于”睡眠–&gt;运行”这也是行不通的，因为调度类只会从等待队列中挑出下一次要运行的进程。 再说说运行态–&gt;睡眠态。从运行态到睡眠态一般是等待某事件的出现，例如等待信号通知，等待IO完成。信号通知很容易理解，而对于IO等待，程序要运行起来，cpu就要执行该程序的指令，同时还需要输入数据，可能是变量数据、键盘输入数据或磁盘文件中的数据，后两种数据相对cpu来说，都是极慢极慢的。但不管怎样，如果cpu在需要数据的那一刻却得不到数据，cpu就只能闲置下来，这肯定是不应该的，因为cpu是极其珍贵的资源，所以内核应该让正在运行且需要数据的进程暂时进入睡眠，等它的数据都准备好了再回到等待队列等待被调度类选中。这就是IO等待。 僵尸态。僵尸态进程表示的是进程已经转为终止态，它已经完成了它的使命并消逝了，但是内核还没有来得及将它在进程列表中的项删除，也就是说内核没给它料理后事，这就造成了一个进程是死的也是活着的假象，说它死了是因为它不再消耗额外资源(但可能会占用未释放的资源)，调度类也不可能选中它并让它运行，说它活着是因为在进程列表中还存在对应的表项，可以被捕捉到。僵尸态进程并不一定会占用多少资源(除非fork出来的大量进程都占用了未释放的资源且成了僵尸进程)，正常情况下的大多数僵尸进程仅在进程列表中占用一点点的内存，大多数僵尸进程的出现都是因为进程正常终止(包括kill -9)，但父进程没有确认该进程已经终止，内核也不知道该进程已经终止了。僵尸进程更具体说明见后文。 另外，睡眠态是一个非常宽泛的概念，分为可中断睡眠和不可中断睡眠。可中断睡眠是允许接收外界信号和内核信号而被唤醒的睡眠，绝大多数睡眠都是可中断睡眠，能ps或top捕捉到的睡眠也几乎总是可中断睡眠；不可中断睡眠只能由内核发起信号来唤醒，外界无法通过信号来唤醒，主要表现在和硬件交互的时候。例如cat一个文件时，从硬盘上加载数据到内存中，在和硬件交互的那一小段时间一定是不可中断的，否则在加载数据的时候突然被人为发送的信号手动唤醒，而被唤醒时和硬件交互的过程又还没完成，所以即使唤醒了也没法将cpu交给它运行，所以cat一个文件的时候不可能只显示一部分内容。而且，不可中断睡眠若能被人为唤醒，更严重的后果是硬件崩溃。由此可知，不可中断睡眠是为了保护某些重要进程，也是为了让cpu不被浪费。 其实只要发现进程存在，且非僵尸态进程，还不占用cpu资源，那么它就是睡眠的。包括后文中出现的暂停态、追踪态，它们也都是睡眠态。 举例分析进程状态转换过程进程间状态的转换情况可能很复杂，这里举一个例子，尽可能详细地描述它们。 以在bash下执行cp命令为例。在当前bash环境下，处于可运行状态(即就绪态)时，当执行cp命令时，首先fork出一个bash子进程，然后在子bash上exec加载cp程序，cp子进程进入等待队列，由于在命令行下敲的命令，所以优先级较高，调度类很快选中它。在cp这个子进程执行过程中，父进程bash会进入睡眠状态(不仅是因为cpu只有一颗的情况下一次只能执行一个进程，还因为进程等待)，并等待被唤醒，此刻bash无法和人类交互。当cp命令执行完毕，它将自己的退出状态码告知父进程，此次复制是成功还是失败，然后cp进程自己消逝掉，父进程bash被唤醒再次进入等待队列，并且此时bash已经获得了cp退出状态码。根据状态码这个”信号”，父进程bash知道了子进程已经终止，所以通告给内核，内核收到通知后将进程列表中的cp进程项删除。至此，整个cp进程正常完成。 假如cp这个子进程复制的是一个大文件，一个cpu时间片无法完成复制，那么在一个cpu时间片消耗尽的时候它将进入等待队列。 假如cp这个子进程复制文件时，目标位置已经有了同名文件，那么默认会询问是否覆盖，发出询问时它等待yes或no的信号，所以它进入了睡眠状态(可中断睡眠)，当在键盘上敲入yes或no信号给cp的时候，cp收到信号，从睡眠态转入就绪态，等待调度类选中它完成cp进程。 在cp复制时，它需要和磁盘交互，在和硬件交互的短暂过程中，cp将处于不可中断睡眠。 假如cp进程结束了，但是结束的过程出现了某种意外，使得bash这个父进程不知道它已经结束了(此例中是不可能出现这种情况的)，那么bash就不会通知内核回收进程列表中的cp表项，cp此时就成了僵尸进程。 进程结构和子shell前台进程：一般命令(如cp命令)在执行时都会fork子进程来执行，在子进程执行过程中，父进程会进入睡眠，这类是前台进程。前台进程执行时，其父进程睡眠，因为cpu只有一颗，即使是多颗cpu，也会因为执行流(进程等待)的原因而只能执行一个进程，要想实现真正的多任务，应该使用进程内多线程实现多个执行流。后台进程：若在执行命令时，在命令的结尾加上符号”&amp;”，它会进入后台。将命令放入后台，会立即返回父进程，并返回该后台进程的的jobid和pid，所以后台进程的父进程不会进入睡眠。当后台进程出错，或者执行完成，总之后台进程终止时，父进程会收到信号。所以，通过在命令后加上”&amp;”，再在”&amp;”后给定另一个要执行的命令，可以实现”伪并行”执行的方式，例如”cp /etc/fstab /tmp &amp; cat /etc/fstab”。bash内置命令：bash内置命令是非常特殊的，父进程不会创建子进程来执行这些命令，而是直接在当前bash进程中执行。但如果将内置命令放在管道后，则此内置命令将和管道左边的进程同属于一个进程组，所以仍然会创建子进程。说到这了，应该解释下子shell，这个特殊的子进程。 一般fork出来的子进程，内容和父进程是一样的，包括变量，例如执行cp命令时也能获取到父进程的变量。但是cp命令是在哪里执行的呢？在子shell中。执行cp命令敲入回车后，当前的bash进程fork出一个子bash，然后子bash通过exec加载cp程序替代子bash。请不要在此纠结子bash和子shell，如果搞不清它们的关系，就当它是同一种东西好了。 那是否可以理解为所有命令、脚本其运行环境都是在子shell中呢？显然，上面所说的bash内置命令不是在子shell中运行的。其他的所有方式，都是在子shell中完成，只不过方式不尽相同。 分为几种情况(只列出几种比较能说明问题的例子，还有其它很多种会进入子shell的情况)： ①.执行bash内置命令：bash内置命令是非常特殊的，父进程不会创建子进程来执行这些命令，而是直接在当前bash进程中执行。但如果将内置命令放在管道后，则此内置命令将和管道左边的进程同属于一个进程组，所以仍然会创建子进程，但却不一定是子shell。请先阅读完下面的几种情况再来考虑此项。②.显然它会进入子shell环境，它的绝大多数环境都是新配置的，因为会加载一些环境配置文件。事实上fork出来的bash子进程内容完全继承父shell，但因重新加载了环境配置项，所以子shell没有继承普通变量，更准确的说是覆盖了从父shell中继承的变量。不妨试试在/etc/bashrc文件中定义一个变量，再在父shell中export名称相同值却不同的环境变量，然后到子shell中看看该变量的值为何？其实执行bash命令，既可以认为进入了子shell，也可以认为没有进入子shell。在执行bash命令后从变量$BASH_SUBSHELL的值为0可以认为它没有进入子shell。但从执行bash命令后进入了新的shell环境来看，它有其父bash进程，且$BASHPID值和父shell不同，所以它算是进入了子shell。执行bash命令更应该被认为是进入了一个完全独立的、全新的shell环境，而不应该认为是进入了片面的子shell环境。③.执行shell脚本：因为脚本中第一行总是”#!/bin/bash”或者直接”bash xyz.sh”，所以这和上面的执行bash进入子shell其实是一回事，都是使用bash命令进入子shell。只不过此时的bash命令和情况②中直接执行bash命令所隐含的选项不一样，所以继承和加载的shell环境也不一样。事实也确实如此，shell脚本只会继承父shell的一项属性：父进程所存储的各命令的路径。另外，执行shell脚本有一个动作：命令执行完毕后自动退出子shell。④.执行非bash内置命令：例如执行cp命令、grep命令等，它们直接fork一份bash进程，然后使用exec加载程序替代该子bash。此类子进程会继承所有父bash的环境。但严格地说，这已经不是子shell，因为exec加载的程序已经把子bash进程替换掉了，这意味着丢失了很多bash环境。⑤.非内置命令的命令替换：当命令行中包含了命令替换部分时，将开启一个子shell先执行这部分内容，再将执行结果返回给当前命令。因为这次的子shell不是通过bash命令进入的子shell，所以它会继承父shell的所有变量内容。这也就解释了”$(echo $$)”中”$$”的结果是当前bash的pid号，而不是子shell的pid号，因为它不是使用bash命令进入的子shell。⑥.使用括号()组合一系列命令：例如(ls;date;echo haha)，独立的括号将会开启一个子shell来执行括号内的命令。这种情况等同于情况⑤。最后需要说明的是，子shell的环境设置不会粘滞到父shell环境，也就是说子shell的变量等不会影响父shell。 还有两种特殊的脚本调用方式：exec和source。 exec：exec是加载程序替换当前进程，所以它不开启子shell，而是直接在当前shell中执行命令或脚本，执行完exec后直接退出exec所在的shell。这就解释了为何bash下执行cp命令时，cp执行完毕后会自动退出cp所在的子shell。source：source一般用来加载环境配置类脚本。它也不会开启子shell，直接在当前shell中执行调用脚本且执行脚本后不退出当前shell，所以脚本会继承当前已有的变量，且脚本执行完毕后加载的环境变量会粘滞给当前shell，在当前shell生效。 job任务大部分进程都能将其放入后台，这时它就是一个后台任务，所以常称为job，每个开启的shell会维护一个job table，后台中的每个job都在job table中对应一个Job项。 手动将命令或脚本放入后台运行的方式是在命令行后加上”&amp;”符号。例如： 12[root@server2 ~]# cp /etc/fstab /tmp/ &amp;[1] 8701 将进程放入后台后，会立即返回其父进程，一般对于手动放入后台的进程都是在bash下进行的，所以立即返回bash环境。在返回父进程的同时，还会返回给父进程其jobid和pid。未来要引用jobid，都应该在jobid前加上百分号”%”，其中”%%”表示当前job，例如”kill -9 %1”表示杀掉jobid为1的后台进程，如果不加百分号，完了，把Init进程给杀了。 通过jobs命令可以查看后台job信息。 123456789101112jobs [-lrs] [jobid]选项说明：-l：jobs默认不会列出后台工作的PID，加上-l会列出进程的PID-r：显示后台工作处于run状态的jobs-s：显示后台工作处于stopped状态的jobs通过&quot;&amp;&quot;放入后台的任务，在后台中仍会处于运行中。当然，对于那种交互式如vim类的命令，将转入暂停运行状态。[root@server2 ~]# sleep 10 &amp;[1] 8710[root@server2 ~]# jobs[1]+ Running sleep 10 &amp; 一定要注意，此处看到的是running和ps或top显示的R状态，它们并不总是表示正在运行，处于等待队列的进程也属于running。它们都属于task_running标识。 另一种手动加入后台的方式是按下CTRL+Z键，这可以将正在运行中的进程加入到后台，但这样加入后台的进程会在后台暂停运行。 123456[root@server2 ~]# sleep 10^Z[1]+ Stopped sleep 10[root@server2 ~]# jobs[1]+ Stopped sleep 10 从jobs信息也看到了在每个jobid的后面有个”+”号，还有”-“，或者不带符号。 12345678[root@server2 ~]# sleep 30&amp;vim /etc/my.cnf&amp;sleep 50&amp;[1] 8915[2] 8916[3] 8917[root@server2 ~]# jobs[1] Running sleep 30 &amp;[2]+ Stopped vim /etc/my.cnf[3]- Running sleep 50 &amp; 发现vim的进程后是加号，”+”表示执行中的任务，也就是说cpu正在它身上，”-“表示被调度类选中的下个要执行的任务，从第三个任务开始不会再对其标注。从jobs的状态可以分析出来，后台任务表中running但没有”+”的表示处于等待队列，running且带有”+”的表示正在执行，stopped状态的表示处于睡眠状态。但不能认为job列表中任务一直是这样的状态，因为每个任务分配到的时间片实际上都很短，在很短的时间内执行完这一次时间片长度的任务，立刻切换到下一个任务并执行。只不过实际过程中，因为切换速度和每个任务的时间片都极短，所以任务列表较小时，显示出来的顺序可能不怎么会出现变动。 就上面的例子而言，下一个要执行的任务是vim，但它是stop的，难道因为这个第一顺位的进程stop，其他进程就不执行吗？显然不是这样的。事实上，过不了多久，会发现另外两个sleep任务已经完成了，但vim仍处于stop状态。 1234[root@server2 ~]# jobs[1] Done sleep 30[2]+ Stopped vim /etc/my.cnf[3]- Done sleep 50 通过这个job例子，是不是更深入的理解了一点内核调度进程的方式呢？ 回归正题。既然能手动将进程放入后台，那肯定能调回到前台，调到前台查看了下执行进度，又想调入后台，这肯定也得有方法，总不能使用CTRL+Z以暂停方式加到后台吧。 fg和bg命令分别是foreground和background的缩写，也就是放入前台和放入后台，严格的说，是以运行状态放入前台和后台，即使原来任务是stopped状态的。 操作方式也很简单，直接在命令后加上jobid即可(即[fg|bg] [%jobid])，不给定jobid时操作的将是当前任务，即带有”+”的任务项。 1234567891011[root@server2 ~]# sleep 20^Z # 按下CTRL+Z进入暂停并放入后台[3]+ Stopped sleep 20[root@server2 ~]# jobs[2]- Stopped vim /etc/my.cnf[3]+ Stopped sleep 20 # 此时为stopped状态[root@server2 ~]# bg %3 # 使用bg或fg可以让暂停状态的进程变会运行态[3]+ sleep 20 &amp;[root@server2 ~]# jobs[2]+ Stopped vim /etc/my.cnf[3]- Running sleep 20 &amp; # 已经变成运行态 使用disown命令可以从job table中直接移除一个job，仅仅只是移出job table，并非是结束任务。而且移除job table后，任务将挂在init/systemd进程下，使其不依赖于终端。 123456disown [-ar] [-h] [%jobid ...]选项说明：-h：给定该选项，将不从job table中移除job，而是将其设置为不接受shell发送的sighup信号。具体说明见&quot;信号&quot;小节。-a：如果没有给定jobid，该选项表示针对Job table中的所有job进行操作。-r：如果没有给定jobid，该选项严格限定为只对running状态的job进行操作如果不给定任何选项，该shell中所有的job都会被移除，移除是disown的默认操作，如果也没给定jobid，而且也没给定-a或-r，则表示只针对当前任务即带有&quot;+&quot;号的任务项。 终端和进程的关系使用pstree命令查看下当前的进程，不难发现在某个终端执行的进程其父进程或上几个级别的父进程总是会是终端的连接程序。 例如下面筛选出了两个终端下的父子进程关系，第一个行是tty终端(即直接在虚拟机中)中执行的进程情况，第二行和第三行是ssh连接到Linux上执行的进程。 1234[root@server2 ~]# pstree -c | grep bash |-login---bash---bash---vim |-sshd-+-sshd---bash | `-sshd---bash-+-grep 正常情况下杀死父进程会导致子进程变为孤儿进程，即其PPID改变，但是杀掉终端这种特殊的进程，会导致该终端上的所有进程都被杀掉。这在很多执行长时间任务的时候是很不方便的。比如要下班了，但是你连接的终端上还在执行数据库备份脚本，这可能会花掉很长时间，如果直接退出终端，备份就终止了。所以应该保证一种安全的退出方法。 一般的方法也是最简单的方法是使用nohup命令带上要执行的命令或脚本放入后台，这样任务就脱离了终端的关联。当终端退出时，该任务将自动挂到init(或systemd)进程下执行。如： shell&gt; nohup tar rf a.tar.gz /tmp/*.txt &amp;另一种方法是使用screen这个工具，该工具可以模拟多个物理终端，虽然模拟后screen进程仍然挂在其所在的终端上的，但同nohup一样，当其所在终端退出后将自动挂到init/systemd进程下继续存在，只要screen进程仍存在，其所模拟的物理终端就会一直存在，这样就保证了模拟终端中的进程继续执行。它的实现方式其实和nohup差不多，只不过它花样更多，管理方式也更多。一般对于简单的后台持续运行进程，使用nohup足以。 另外，在子shell中的后台进程在终端被关闭时也会脱离终端，因此也不受shell和终端的控制。例如shell脚本中的后台进程，再如”(sleep 10 &amp;)”。 可能你已经发现了，很多进程是和终端无关的，也就是不依赖于终端，这类进程一般是内核类进程/线程以及daemon类进程，若它们也依赖于终端，则终端一被终止，这类进程也立即被终止，这是绝对不允许的。 信号信号在操作系统中控制着进程的绝大多数动作，信号可以让进程知道某个事件发生了，也指示着进程下一步要做出什么动作。信号的来源可以是硬件信号(如按下键盘或其他硬件故障)，也可以是软件信号(如kill信号，还有内核发送的信号)。不过，很多可以感受到的信号都是从进程所在的控制终端发送出去的。 需知道的信号Linux中支持非常多种信号，它们都以SIG字符串开头，SIG字符串后的才是真正的信号名称，信号还有对应的数值，其实数值才是操作系统真正认识的信号。但由于不少信号在不同架构的计算机上数值不同(例如CTRL+Z发送的SIGSTP信号就有三种值18,20,24)，所以在不确定信号数值是否唯一的时候，最好指定其字符名称。 以下是需要了解的信号。 123456789101112131415161718Signal Value Comment─────────────────────────────SIGHUP 1 终端退出时，此终端内的进程都将被终止SIGINT 2 中断进程，可被捕捉和忽略，几乎等同于sigterm，所以也会尽可能的释放执行clean-up，释放资源，保存状态等(CTRL+C)SIGQUIT 3 从键盘发出杀死(终止)进程的信号SIGKILL 9 强制杀死进程，该信号不可被捕捉和忽略，进程收到该信号后不会执行任何clean-up行为，所以资源不会释放，状态不会保存SIGTERM 15 杀死(终止)进程，可被捕捉和忽略，几乎等同于sigint信号，会尽可能的释放执行clean-up，释放资源，保存状态等SIGCHLD 17 当子进程中断或退出时，发送该信号告知父进程自己已完成，父进程收到信号将告知内核清理进程列表。所以该信号可以解除僵尸进 程，也可以让非正常退出的进程工作得以正常的clean-up，释放资源，保存状态等。 SIGSTOP 19 该信号是不可被捕捉和忽略的进程停止信息，收到信号后会进入stopped状态SIGTSTP 20 该信号是可被忽略的进程停止信号(CTRL+Z)SIGCONT 18 发送此信号使得stopped进程进入running，该信号主要用于jobs，例如bg &amp; fg 都会发送该信号。 可以直接发送此信号给stopped进程使其运行起来 SIGUSR1 10 用户自定义信号1SIGUSR2 12 用户自定义信号2 除了这些信号外，还需要知道一个特殊信号：代码为0的信号。此信号为EXIT信号，表示直接退出。如果kill发送的信号是0(即kill -0)则表示不做任何处理直接退出，但执行错误检查：当检查发现给定的pid进程存在，则返回0，否则返回1。也就是说，0信号可以用来检测进程是否存在，可以代替 ps aux | grep proc_name 。(man kill中的原文为：If sig is 0, then no signal is sent, but error checking is still performed。而man bash的trap小节中有如下描述：If a sigspec is EXIT (0)，这说明0信号就是EXIT信号) 以上所列的信号中，只有SIGKILL和SIGSTOP这两个信号是不可被捕捉且不可被忽略的信号，其他所有信号都可以通过trap或其他编程手段捕捉到或忽略掉。 此外，经常看到有些服务程序(如httpd/nginx)的启动脚本中使用WINCH和USR1这两个信号，发送这两个信号时它们分别表示graceful stop和graceful restart。所谓的graceful，译为优雅，不过使用这两个字去描述这种环境实在有点不伦不类。它对于后台服务程序而言，传达了几个意思：(1)当前已经运行的进程不再接受新请求(2)给当前正在运行的进程足够多的时间去完成正在处理的事情(3)允许启动新进程接受新请求(4)可能还有日志文件是否应该滚动、pid文件是否修改的可能，这要看服务程序对信号的具体实现。 再来说说，为什么后台服务程序可以使用这两个信号。以httpd的为例，在其头文件mpm_common.h中有如下几行代码： 12345/* Signal used to gracefully restart */#define AP_SIG_GRACEFUL SIGUSR1/* Signal used to gracefully stop */#define AP_SIG_GRACEFUL_STOP SIGWINCH 这说明注册了对应信号的处理函数，它们分别表示将接收到信号时，执行对应的GRACEFUL函数。 注意，SIGWINCH是窗口程序的尺寸改变时发送改信号，如vim的窗口改变了就会发送该信号。但是对于后台服务程序，它们根本就没有窗口，所以WINCH信号对它们来说是没有任何作用的。因此，大概是约定俗成的，大家都喜欢用它来作为后台服务程序的GRACEFUL信号。但注意，WINCH信号对前台程序可能是有影响的，不要乱发这种信号。同理，USR1和USR2也是一样的，如果源代码中明确为这两个信号注册了对应函数，那么发送这两个信号就可以实现对应的功能，反之，如果没有注册，则这两个信号对进程来说是错误信号。 SIGHUP(1).当控制终端退出时，会向该终端中的进程发送sighup信号，因此该终端上运行的shell进程、其他普通进程以及任务都会收到sighup而导致进程终止。 多种方式可以改变因终端中断发送sighup而导致子进程也被结束的行为，这里仅介绍比较常见的三种：一是使用nohup命令启动进程，它会忽略所有的sighup信号，使得该进程不会随着终端退出而结束；二是将待执行命令放入子shell中并放入后台运行，例如”(sleep 10 &amp;)”；三是使用disown，将任务列表中的任务移除出job table或者直接使用disown -h的功能设置其不接收终端发送的sighup信号。但不管是何种实现方式，终端退出后未被终止的进程将只能挂靠在init/systemd下。 (2).对于daemon类的程序(即服务性进程)，这类程序不依赖于终端(它们的父进程都是init或systemd)，它们收到sighup信号时会重读配置文件并重新打开日志文件，使得服务程序可以不用重启就可以加载配置文件。 僵尸进程和SIGCHLD一个编程完善的程序，在子进程终止、退出的时候，内核会发送SIGCHLD信号给其父进程，父进程收到信号就会对该子进程进行善后(接收子进程的退出状态、释放未关闭的资源)，同时内核也会进行一些善后操作(比如清理进程表项、关闭打开的文件等)。 在子进程死亡的那一刹那，子进程的状态就是僵尸进程，但因为发出了SIGCHLD信号给父进程，父进程只要收到该信号，子进程就会被清理也就不再是僵尸进程。所以正常情况下，所有终止的进程都会有一小段时间处于僵尸态(发送SIGCHLD信号到父进程收到该信号之间)，只不过这种僵尸进程存在时间极短(倒霉的僵尸)，几乎是不可被ps或top这类的程序捕捉到的。 如果在特殊情况下，子进程终止了，但父进程没收到SIGCHLD信号，没收到这信号的原因可能是多种的，不管如何，此时子进程已经成了永存的僵尸，能轻易的被ps或top捕捉到。僵尸不倒霉，人类就要倒霉，但是僵尸爸爸并不知道它儿子已经变成了僵尸，因为有僵尸爸爸的掩护，僵尸道长即内核见不到小僵尸，所以也没法收尸。悲催的是，人类能力不足，直接发送信号(如kill)给僵尸进程是无效的，因为僵尸进程本就是终结了的进程，它收不到信号，只有内核从进程列表中将僵尸进程表项移除才算完成收尸。 要解决掉永存的僵尸有几种方法： (1).杀死僵尸进程的父进程。没有了僵尸爸爸的掩护，小僵尸就暴露给了僵尸道长的直系弟子init/systemd，init/systemd会定期清理它下面的各种僵尸进程。所以这种方法有点不讲道理，僵尸爸爸是正常的啊，不过如果僵尸爸爸下面有很多僵尸儿子，这僵尸爸爸肯定是有问题的，比如编程不完善，杀掉是应该的。 (2).手动发送SIGCHLD信号给僵尸进程的父进程。僵尸道长找不到僵尸，但被僵尸祸害的人类能发现僵尸，所以人类主动通知僵尸爸爸，让僵尸爸爸知道自己的儿子死而不僵，然后通知内核来收尸。 当然，第二种手动发送SIGCHLD信号的方法要求父进程能收到信号，而SIGCHLD信号默认是被忽略的，所以应该显式地在程序中加上获取信号的代码。也就是人类主动通知僵尸爸爸的时候，默认僵尸爸爸是不搭理人类的，所以要强制让僵尸爸爸收到通知。不过一般daemon类的程序在编程上都是很完善的，发送SIGCHLD总是会收到，不用担心。 手动发送信号(kill命令)使用kill命令可以手动发送信号给指定的进程。 123kill [-s signal] pid...kill [-signal] pid...kill -l 使用kill -l可以列出Linux中支持的信号，有64种之多，但绝大多数非编程人员都用不上。 使用-s或-signal都可以发送信号，不给定发送的信号时，默认为TREM信号，即kill -15。 123shell&gt; kill -9 pid1 pid2...shell&gt; kill -TREM pid1 pid2...shell&gt; kill -s TREM pid1 pid2... pkill和killall这两个命令都可以直接指定进程名来发送信号，不指定信号时，默认信号都是TERM。 (1).pkill pkill和pgrep命令是同族命令，都是先通过给定的匹配模式搜索到指定的进程，然后发送信号(pkill)或列出匹配的进程(pgrep)，pgrep就不介绍了。 pkill能够指定模式匹配，所以可以使用进程名来删除，想要删除指定pid的进程，反而还要使用”-s”选项来指定。默认发送的信号是SIGTERM即数值为15的信号。 123456789101112131415161718192021222324pkill [-signal] [-v] [-P ppid,...] [-s pid,...][-U uid,...] [-t term,...] [pattern]选项说明：-P ppid,... ：匹配PPID为指定值的进程-s pid,... ：匹配PID为指定值的进程-U uid,... ：匹配UID为指定值的进程，可以使用数值UID，也可以使用用户名称-t term,... ：匹配给定终端，终端名称不能带上&quot;/dev/&quot;前缀，其实&quot;w&quot;命令获得终端名就满足此处条件了，所以pkill可以直接杀掉整个终端-v ：反向匹配-signal ：指定发送的信号，可以是数值也可以是字符代表的信号-f ：默认情况下，pgrep/pkill只会匹配进程名。使用-f将匹配命令行在CentOS 7上，还有两个好用的新功能选项。-F, --pidfile file：匹配进程时，读取进程的pid文件从中获取进程的pid值。这样就不用去写获取进程pid命令的匹配模式-L, --logpidfile ：如果&quot;-F&quot;选项读取的pid文件未加锁，则pkill或pgrep将匹配失败。例如：[root@xuexi ~]# ps x | grep ssh[d] 1291 ? Ss 0:00 /usr/sbin/sshd 13193 ? Ss 0:02 sshd: root@pts/1,pts/3,pts/0现在想匹配/usr/sbin/sshd。[root@xuexi ~]# pgrep bin/sshd[root@xuexi ~]# pgrep -f bin/sshd1291 可以看到第一个什么也不返回。因为不加-f选项时，pgrep只能匹配进程名，而进程名指的是sshd，而非/usr/sbin/sshd，所以匹配失败。加上-f后，就能匹配成功。所以，当pgrep或pkill匹配不到进程时，考虑加上-f选项。 踢出终端： 1shell&gt; pkill -t pts/0 killallkillall主要用于杀死一批进程，例如杀死整个进程组。其强大之处还体现在可以通过指定文件来搜索哪个进程打开了该文件，然后对该进程发送信号，在这一点上，fuser和lsof命令也一样能实现。 123456789killall [-r,--regexp] [-s,--signal signal] [-u,--user user] [-v,--verbose] [-w,--wait] [-I,--ignore-case] [--] name ...选项说明：-I ：匹配时不区分大小写-r ：使用扩展正则表达式进行模式匹配-s, --signal ：发送信号的方式可以是-HUP或-SIGHUP，或数值的&quot;-1&quot;，或使用&quot;-s&quot;选项指定信号-u, --user ：匹配该用户的进程-v, ：给出详细信息-w, --wait ：等待直到该杀的进程完全死透了才返回。默认killall每秒检查一次该杀的进程是否还存在，只有不存在了才会给出退出状态码。 如果一个进程忽略了发送的信号、信号未产生效果、或者是僵尸进程将永久等待下去 fuser和lsoffuser可以查看文件或目录所属进程的pid，即由此知道该文件或目录被哪个进程使用。例如，umount的时候提示the device busy可以判断出来哪个进程在使用。而lsof则反过来，它是通过进程来查看进程打开了哪些文件，但要注意的是，一切皆文件，包括普通文件、目录、链接文件、块设备、字符设备、套接字文件、管道文件，所以lsof出来的结果可能会非常多。 fuser12345678910111213141516fuser [-ki] [-signal] file/dir-k：找出文件或目录的pid，并试图kill掉该pid。发送的信号是SIGKILL-i：一般和-k一起使用，指的是在kill掉pid之前询问。-signal：发送信号，如-1 -15，如果不写，默认-9，即kill -9不加选项：直接显示出文件或目录的pid在不加选项时，显示结果中文件或目录的pid后会带上一个修饰符： c:在当前目录下 e:可被执行的 f:是一个被开启的文件或目录 F:被打开且正在写入的文件或目录 r:代表root directory 例如： 123456[root@xuexi ~]# fuser /usr/sbin/crond/usr/sbin/crond: 1425e表示/usr/sbin/crond被1425这个进程打开了，后面的修饰符e表示该文件是一个可执行文件。[root@xuexi ~]# ps aux | grep 142[5]root 1425 0.0 0.1 117332 1276 ? Ss Jun10 0:00 crond lsof输出信息中各列意义： 1234567891011121314151617181920212223242526272829303132333435 COMMAND：进程的名称 PID：进程标识符 USER：进程所有者 FD：文件描述符，应用程序通过文件描述符识别该文件。如cwd、txt等 TYPE：文件类型，如DIR、REG等 DEVICE：指定磁盘的名称 SIZE/OFF：文件的大小或文件的偏移量(单位kb)(size and offset) NODE：索引节点（文件在磁盘上的标识） NAME：打开文件的确切名称lsof的各种用法：lsof /path/to/somefile：显示打开指定文件的所有进程之列表；建议配合grep使用lsof -c string：显示其COMMAND列中包含指定字符(string)的进程所有打开的文件；可多次使用该选项lsof -p PID：查看该进程打开了哪些文件lsof -U：列出套接字类型的文件。一般和其他条件一起使用。如lsof -u root -a -Ulsof -u uid/name：显示指定用户的进程打开的文件；可使用脱字符&quot;^&quot;取反，如&quot;lsof -u ^root&quot;将显示非root用户打开的所有文件lsof +d /DIR/：显示指定目录下被进程打开的文件lsof +D /DIR/：基本功能同上，但lsof会对指定目录进行递归查找，注意这个参数要比grep版本慢lsof -a：按&quot;与&quot;组合多个条件，如lsof -a -c apache -u apachelsof -N：列出所有NFS（网络文件系统）文件lsof -n：不反解IP至HOSTNAMElsof -i：用以显示符合条件的进程情况lsof -i[46] [protocol][@host][:service|port] 46：IPv4或IPv6 protocol：TCP or UDP host：host name或ip地址，表示搜索哪台主机上的进程信息 service：服务名称(可以不只一个) port：端口号 (可以不只一个)大概&quot;-i&quot;是使用最多的了，而&quot;-i&quot;中使用最多的又是服务名或端口了。[root@www ~]# lsof -i :22COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 1390 root 3u IPv4 13050 0t0 TCP *:ssh (LISTEN)sshd 1390 root 4u IPv6 13056 0t0 TCP *:ssh (LISTEN)sshd 36454 root 3r IPv4 94352 0t0 TCP xuexi:ssh-&gt;172.16.0.1:50018 (ESTABLISHED)]]></content>
      <categories>
        <category>linux基础</category>
      </categories>
      <tags>
        <tag>linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux文件权限管理]]></title>
    <url>%2F2019%2F07%2F29%2Flinux%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/f-ck-need-u/p/7011971.html 文件/目录的权限文件的权限每个文件都有其所有者(u:user)、所属组(g:group)和其他人(o:other)对它的操作权限，a:all则同时代表这3者。权限包括读(r:read)、写(w:write)、执行(x:execute)。在不同类型的文件上读、写、执行权限的体现有所不同，所以目录权限和普通文件权限要区分开来。 在普通文件上：12345r：可读，可以使用类似cat等命令查看文件内容；读是文件的最基本权限，没有读权限，普通文件的一切操作行为都被限制。w：可写，可以编辑此文件；x：可执行，表示文件可由特定的解释器解释并运行。可以理解为windows中的可执行程序或批处理脚本，双击就能运行起来的文件。 在目录上：12345r：可以对目录执行ls以列出目录内的所有文件；读是文件的最基本权限，没有读权限，目录的一切操作行为都被限制。w：可以在此目录创建或删除文件/子目录；x：可进入此目录，可使用ls -l查看文件的详细信息。可以理解为windows中双击就进入目录的动作。 如果目录没有x权限，其他人将无法查看目录内文件属性(只能查看到文件类型和文件名，至于为什么，见后文)，所以一般目录都要有x权限。而如果只有执行却没有读权限，则权限拒绝。 一般来说，普通文件的默认权限是644(没有执行权限)，目录的默认权限是755(必须有执行权限，否则进不去)，链接文件的权限是777。当然，默认文件的权限设置方法是可以通过umask值来改变的。 权限的表示方式权限的模式有两种体现：数字体现方式和字符体现方式。 权限的数字表示：”-“代表没有权限,用0表示。 12345r-----4w-----2x-----1 例如：rwx rw- r–对应的数字权限是764，732代表的权限数值表示为rwx -wx -w-。 chmod修改权限能够修改权限的人只有文件所有者和超级管理员。 123chmod [OPTION]... MODE[,MODE]... FILE...chmod [OPTION]... num_mode FILE...chmod [OPTION]... --reference=RFILE FILE... 选项说明：–reference=RFILE：引用某文件的权限作为权限值-R：递归修改，只对当前已存在的文件有效(1). 使用数值方式修改权限 1shell&gt; chmod 755 /tmp/a.txt (2). 使用字符方式修改权限 由于权限属性附在文件所有者、所属组和其它上，它们三者都有独立的权限位，所有者使用字母”u”表示，所属组使用”g”来表示，其他使用”o”来表示，而字母”a”同时表示它们三者。所以使用字符方式修改权限时，需要指定操作谁的权限。 123chmod [ugoa][+ - =] [权限字符] 文件/目录名&quot;+&quot;是加上权限，&quot;-&quot;是减去权限，&quot;=&quot;是直接设置权限 12[root@xuexi tmp]# chmod u-x,g-x,o-x test # 将ugo都去掉x权限，等价于chmod -x test[root@xuexi tmp]# chmod a+x test # 为ugo都加上x权限，等价于chmod +x test chgrp更改文件和目录的所属组，要求组已经存在。 注意，对于链接文件而言，修改组的作用对象是链接的源文件，而非链接文件本身。 123456chgrp [OPTION]... GROUP FILE...chgrp [OPTION]... --reference=RFILE FILE..选项说明：-R：递归修改--reference=dest_file file_list：引用某文件的group作为文件列表的组,即将file文件列表的组改为dest_file的组 chownchown可以修改文件所有者和所属组。 注意，对于链接文件而言，默认不会穿过链接修改源文件，而是直接修改链接文件本身，这和chgrp的默认是不一样的。 123chown [OPTION]... [OWNER][:[GROUP]] FILE...chown [OPTION]... [OWNER][.[GROUP]] FILE...chown [OPTION]... --reference=RFILE FILE... 选项说明： 1234567891011121314--from=CURRENT_OWNER:CURRENT_GROUP：只修改当前所有者或所属组为此处指定的值的文件--reference=RFILE：引用某文件的所有者和所属组的值作为新的所有者和所属组-R：递归修改。注意，当指定-R时，且同时指定下面某一个选项时对链接文件有不同的行为 -H：如果chown的文件参数是一个链接到目录的链接文件，则穿过此链接文件修改其源目录的所有者和所属组 -L：目录中遇到的所有链接文件都穿越过去，修改它们的源文件的所有者和所属组 -P：不进行任何穿越，只修改链接文件本身的所有者和所属组。(这是默认值) 这3项若同时指定多项时，则最后一项生效chown指定所有者和所属组的方式有两种，使用冒号和点。shell&gt; chown root.root testshell&gt; chown root:root testshell&gt; chown root test # 只修改所有者shell&gt; chown :root test # 自修改组shell&gt; chown .root test 2 实现权限的本质涉及文件系统的知识点，若不理解，可以先看看文件系统的内容。此处是以ext4文件系统为例的，在其他文件系统上结果可能会有些不一样(centos 7上使用xfs文件系统时结果可能就不一样)，但本质是一样的。 不同的权限表示对文件具有不同能力，如读写执行(rwx)权限，但是它是怎么实现的呢？描述文件权限的数据放在哪里呢？ 首先，权限的元数据放在inode中，严格地说是放在inode table中，因为每个块组的所有inode组成一个inode table。在inode table中使用一列来存放数字型的权限，比如某文件的权限为644。每次用户要对文件进行操作时系统都会先查看权限，确认该用户是否有对应的权限来执行操作。当然，inode table一般都已经加载到内存中，所以每次查询权限的资源消耗是非常小的。 无论是读、写还是执行权限，所体现出来的能力究其本质都是因为它作用在对应文件的data block上。 读权限(r)对普通文件具有读权限表示的是具有读取该文件内容的能力，对目录具有读权限表示具有浏览该目录中文件或子目录的能力。其本质都是具有读取其data block的能力。 对于普通文件而言，能够读取文件的data block，而普通文件的data block存储的直接就是数据本身， 所以对普通文件具有读权限表示能够读取文件内容。 对于目录文件而言，能够读取目录的data block，而目录文件的data block存储的内容包括但不限于：目录中文件的inode号(并非直接存储，而是存储指向inode table中该inode号的指针)以及这些文件的文件类型、文件名。所以能够读取目录的data block表示仅能获取到这些信息。 目录的data block内容示例如下： 12345678910111213141516171819例如：shell&gt; mkdir -p /mydata/data/testdir/subdir # 创建testdir测试目录和其子目录subdirshell&gt; touch /mydata/data/testdir/a.log # 再在testdir下创建一个普通文件shell&gt; chmod 754 /mydata/data/testdir # 将testdir设置为对其他人只有读权限然后切换到普通用户查看testdir目录的内容。shell&gt; su - wangwushell&gt; ll -ai /mydata/data/testdir/ls: cannot access /mydata/data/testdir/..: Permission deniedls: cannot access /mydata/data/testdir/a.log: Permission deniedls: cannot access /mydata/data/testdir/subdir: Permission deniedls: cannot access /mydata/data/testdir/.: Permission deniedtotal 0? d????????? ? ? ? ? ? .? d????????? ? ? ? ? ? ..? -????????? ? ? ? ? ? a.log? d????????? ? ? ? ? ? subdir 从结果中看出，testdir下的文件名和文件类型是能够读取的，但是其他属性都不能读取到。而且也读取不到inode号，因为它并没有直接存储inode号，而是存储了指向Inode号的指针，要定位到指针的指向需要执行权限。 执行权限(x)执行权限表示的是能够执行。如何执行？执行这个词不是很好解释，可以简单的类比Windows中的双击行为。例如对目录双击就能进入到目录，对批处理文件双击就能运行(有专门的解释器解释)，对可执行程序双击就能运行等。 当然，读权限是文件的最基本权限，执行权限能正常运行必须得配有读权限。 对目录有执行权限，表示可以通过目录的data block中指向文件inode号的指针定位到inode table中该文件的inode信息，所以可以显示出这些文件的全部属性信息。 写权限(w)写权限很简单，就是能够将数据写入分配到的data block。 对目录文件具有写权限，表示能够创建和删除文件。目录的写操作实质是能够在目录的data block中创建或删除关于待操作文件的记录。它要求对目录具有执行权限，因为无论是创建还是删除其内文件，都需要将其data block中inode号和inode table中的inode信息关联或删除。 对普通文件具有写权限，实质是能够改写该文件的data block。 还是要说明的是，对文件有写权限不代表能够删除该文件，因为删除文件是要在目录的data block中删除该文件的记录，也就是说删除权限是在目录中定义的。 umask说明umask值用于设置用户在创建文件时的默认权限。对于root用户(实际上是UID小于200的user)，系统默认的umask值是022；对于普通用户和系统用户，系统默认的umask值是002。 默认它们的设置是写在/etc/profile和/etc/bashrc两个环境配置文件中。 1234shell&gt; grep -C 5 -R &apos;umask 002&apos; /etc | grep &apos;umask 022&apos; /etc/bashrc- umask 022/etc/csh.cshrc- umask 022/etc/profile- umask 022 相关设置项如下： 12345if [ $UID -gt 199 ] &amp;&amp; [ &quot;`id -gn`&quot; = &quot;`id -un`&quot; ]; then umask 002else umask 022fi 执行umask命令可以查看当前用户的umask值。 1234[root@xuexi tmp]# umask0022[longshuai@xuexi tmp]$ umask0002 执行umask num可以临时修改umask值为num，但这是临时的，要永久有效，需要写入到环境配置文件中，至于写入到/etc/profile、/etc/bashrc、/.bashrc还是/.bash_profile中，看你自己的需求了。不过一般来说，不会去永久修改umask值，只会在特殊条件下临时修改下umask值。 umask是如何决定创建文件的默认权限的呢？ 如果创建的是目录，则使用777-umask值，如root的umask=022，则root创建目录时该目录的默认权限为777-022=755，而普通用户创建目录时，权限为777-002=775. 如果创建的是普通文件，在Linux中，深入贯彻了一点：文件默认不应该有执行权限，否则是危险的。所以在计算时，可能会和想象中的结果不一样。如果umask的三位都为偶数，则直接使用666去减掉umask值，因为6减去一个偶数还是偶数，任何位都不可能会有执行权限。如root创建普通文件时默认权限为666-022=644，而普通用户创建普通文件时默认权限为666-002=664。 如果umask值某一位为奇数，则666减去umask值后再在奇数位上加1。如umask=021时，创建文件时默认权限为666-021=645，在奇数位上加1，则为646。 1234[longshuai@xuexi tmp]$ umask 021[longshuai@xuexi tmp]$ touch b.txt[longshuai@xuexi tmp]$ ls -l b.txt-rw-r--rw- 1 longshuai longshuai 0 Jun 7 12:02 b.txt 总之计算出后默认都是没有执行权限的。 文件的扩展ACL权限在计算机相关领域，所有的ACL(access control list)都表示访问控制列表。 文件的owner/group/others的权限就是一种ACL，它们是基本的ACL。很多时候，只通过这3个权限位是无法完全合理设置权限问题的，例如如何仅设置某单个用户具有什么权限。这时候需要使用扩展ACL。 扩展ACL是一种特殊权限，它是文件系统上功能，用于解决所有者、所属组和其他这三个权限位无法合理设置单个用户权限的问题。所以，扩展ACL可以针对单一使用者，单一档案或目录里的默认权限进行r,w,x的权限规范。 需要明确的是，扩展ACL是文件系统上的功能，且工作在内核，默认在ext4/xfs上都已开启。 在下文中，都直接以ACL来表示代替扩展ACL的称呼。 查看文件系统是否开启ACL功能对于ext家族的文件系统来说，要查看是否开启acl功能，使用dumpe2fs导出文件系统属性即可。 123shell&gt; dumpe2fs -h /dev/sda2 | grep -i acldumpe2fs 1.41.12 (17-May-2010)Default mount options: user_xattr acl 对于xfs文件系统，则没有直接的命令可以输出它的相关信息，需要使用dmesg来查看。其实无需关注它，因为默认xfs会开启acl功能。 123shell&gt; dmesg | grep -i acl[ 1.465903] systemd[1]: systemd 219 running in system mode. (+PAM +AUDIT +SELINUX +IMA -APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ -LZ4 -SECCOMP +BLKID +ELFUTILS +KMOD +IDN)[ 2.517705] SGI XFS with ACLs, security attributes, no debug enabled 开启ACL功能后，不代表就使用ACL功能。是否使用该功能，不同文件系统控制方法不一样，对于ext家族来说，通过mount挂载选项来控制，而对于xfs文件系统，mount命令根本不支持acl参数(xfs文件系统如何关闭或启用的方法本人也不知道)。 设置和查看ACL设置使用setfacl命令。 12345678910111213setfacl [options] u:[用户列表]:[rwx] 目录/文件名 # 对用户设置使用usetfacl [options] g:[组列表]:[rwx] 目录/文件名 # 对组设置使用g选项说明：-m：设定ACL权限(modify)-x：删除指定的ACL权限，可以指定用户、组和文件来删除(remove)-M：写了ACL条目的文件，将从此文件中读取ACL条目，需要配合-m，所以-M指定的是modify file-X：写了ACL条目的文件，将从此文件中读取ACL条目，需要配合-x，所以-X指定的是remove file-n：不重置mask-b：删除所有的ACL权限-d：设定默认ACL权限，只对目录有效，设置后子目录(文件)继承默认ACL，只对未来文件 有效-k：删除默认ACL权限-R：递归设定ACL权限，只对目录有效，只对已有文件有效 查看使用getfacl命令 1getfacl filename 案例：假设现有目录/data/videos专门存放视频，其中有一个a.avi的介绍性视频。该目录的权限是750。现在有一个新用户加入，但要求该用户对该目录只有查看的权限，且只能看其中一部视频a.avi，另外还要求该用户在此目录下没有创建和删除文件的权限。 1.准备相关环境。 123456shell&gt; mkdir -p /data/videosshell&gt; chmod 750 /data/videosshell&gt; touch /data/videos/&#123;a,b&#125;.avishell&gt; echo &quot;xxx&quot; &gt;/data/videos/a.avishell&gt; echo &quot;xxx&quot; &gt;/data/videos/b.avishell&gt; chown -R root.root /data/videos 2.首先设置用户longshuai对/data/videos目录有读和执行权限。 1shell&gt; setfacl -m u:longshuai:rx /data/videos 3.现在longshuai对/data/videos目录下的所有文件都有读权限，因为默认文件的权限为644。要设置longshuai只对a.avi有读权限，先设置所有文件的权限都为不可读。 1shell&gt; chmod 640 /data/videos/* 4.然后再单独设置a.avi的读权限。 1shell&gt; setfacl -m u:longshuai:r /data/videos/a.avi 到此就设置完成了。查看/data/videos/和/data/videos/a.avi上的ACL信息。 1234567891011121314151617181920shell&gt; getfacl /data/videos/getfacl: Removing leading &apos;/&apos; from absolute path names# file: data/videos/# owner: root# group: rootuser::rwxuser:longshuai:r-x # 用户longshuai在此文件上的权限是r-xgroup::r-xmask::r-xother::---shell&gt; getfacl /data/videos/a.avigetfacl: Removing leading &apos;/&apos; from absolute path names# file: data/videos/a.avi# owner: root# group: rootuser::rw-user:longshuai:r-- # 用户longshuai在此文件上的权限是r--group::r--mask::r--other::--- ACL:mask设置mask后会将mask权限与已有的acl权限进行与计算，计算后的结果会成为新的ACL权限。 设定mask的方式为： 1setfacl -m m:[rwx] 目录/文件名 注意：默认每次设置文件的acl都会重置mask为此次给定的用户的值。既然如此，要如何控制文件上的acl呢？如果一个文件上要设置多个用户的acl，重置mask后就会对已有用户的acl重新计算，而使得acl权限得不到有效的控制。使用setfacl的”-n”选项，它表示此次设置不会重置mask值。 例如： 当前的acl权限： 12345678910shell&gt; getfacl /data/videos getfacl: Removing leading &apos;/&apos; from absolute path names# file: data/videos# owner: root# group: rootuser::rwxuser:longshuai:rwxgroup::r-xmask::rwxother::--- 设置mask值为rx。 123456789101112shell&gt; setfacl -m m:rx /data/videosshell&gt; getfacl /data/videos getfacl: Removing leading &apos;/&apos; from absolute path names# file: data/videos# owner: root# group: rootuser::rwxuser:longshuai:rwx #effective:r-xgroup::r-xmask::r-xother::--- 设置mask后，它提示有效权限是r-x。这是rwx和r-x做与运算之后的结果。 再设置longshuai的acl为rwx，然后查看mask，会发现mask也被重置为rwx。 123456789101112shell&gt; setfacl -m u:longshuai:rwx /data/videosshell&gt; getfacl /data/videosgetfacl: Removing leading &apos;/&apos; from absolute path names# file: data/videos# owner: root# group: rootuser::rwxuser:longshuai:rwxgroup::r-xmask::rwxother::--- 所以，在设置文件的acl时，要使用-n选项来禁止重置mask。 1234567891011121314shell&gt; setfacl -m m:rx /data/videosshell&gt; setfacl -n -m u:longshuai:rwx /data/videosshell&gt; getfacl /data/videosgetfacl: Removing leading &apos;/&apos; from absolute path names# file: data/videos# owner: root# group: rootuser::rwxuser:longshuai:rwx #effective:r-xgroup::r-xmask::r-xother::--- 设置递归和默认ACL权限递归ACL权限只对目录里已有文件有效，默认权限只对未来目录里的文件有效。 设置递归ACL权限： 1setfacl -m u:username:[rwx] -R 目录名 设置默认ACL权限： 1setfacl -m d:u:username:[rwx] 目录名 删除ACL权限 123setfacl -x u:用户名 文件名 # 删除指定用户ACLsetfacl -x g:组名 文件名 # 删除指定组名ACLsetfacl -b 文件名 # 指定文件删除ACL，会删除所有ACL 文件隐藏属性chattr：change file attributes lsattr：list file attributes 1chattr [+ - =] [ai] 文件或目录名 常用的参数是a(append，追加)和i(immutable，不可更改)，其他参数略。 设置了a参数时，文件中将只能增加内容，不能删除数据，且不能打开文件进行任何编辑，哪怕是追加内容也不可以，所以像sed等需要打开文件的再写入数据的工具也无法操作成功。文件也不能被删除。只有root才能设置。 设置了i参数时，文件将被锁定，不能向其中增删改内容，也不能删除修改文件等各种动作。只有root才能设置。可以将其理解为设置了i后，文件将是永恒不变的了，谁都不能动它。 例如，对/etc/shadow文件设置i属性，任何用户包括root将不能修改密码，而且也不能创建用户。 1shell&gt; chattr +i /etc/shadow 此时如果新建一个用户。 12shell&gt; useradd newlongsuaishell&gt; useradd: cannot open /etc/shadow # 提示文件不能打开，被锁定了 lsattr查看文件设置的隐藏属性。 12shell&gt; lsattr /etc/shadow----i--------e- /etc/shadow # i属性说明被锁定了，e是另一种文件属性，忽略它 删除隐藏属性： 123shell&gt; chattr -i /etc/shadowshell&gt; lsattr /etc/shadow-------------e- /etc/shadow 再来一例： 1234shell&gt; chattr +a test1.txt # 对test1.txt设置a隐藏属性shell&gt; echo 1234&gt;&gt;test1.txt # 追加内容是允许的行为shell&gt; cat /dev/null &gt;test1.txt # 但是清空文件内容是不允许的-bash: test1.txt: Operation not permitted suid/sgid/sbit3.6.1 suidsuid只针对可执行文件，即二进制文件。它的作用是对某个命令(可执行文件)授予所有者的权限，命令执行完成权限就消失。一般是提权为root权限。 例如/etc/shadow文件所有人都没有权限(root除外)，其他用户连看都不允许。 12shell&gt; ls -l /etc/shadow----------. 1 root root 752 Apr 8 12:42 /etc/shadow 但是他们却能修改自己的密码，说明他们一定有一定的权限。这个权限就是suid控制的。 12shell&gt; ls -l /usr/bin/passwd-rwsr-xr-x. 1 root root 30768 Feb 22 2012 /usr/bin/passwd 其中的”s”权限就是suid，它出现在所有者位置上(是root)，其他用户执行passwd命令时，会暂时拥有所有者位的rwx权限，也就是root的权限，所以能向/etc/shadow写入数据。 suid必须和x配合，如果没有x配合，则该suid是空suid，仍然没有执行命令的权限，所有者都没有了x权限，suid依赖于它所以更不可能有x权限。空的suid权限使用大写的”S”表示。 数字4代表suid，如4755。 sgid针对二进制文件和目录。 针对二进制文件时，权限升级为命令的所属组权限。 针对目录时，目录中所建立的文件或子目录的组将继承默认父目录组，其本质还是提升为目录所属组的权限。此时目录应该要有rx权限，普通用户才能进入目录，如果普通用户有w权限，新建的文件和目录则以父目录组为默认组。 以2代表sgid，如2755，和suid组合如6755。 sbit只对目录有效。对目录设置sbit，将使得目录里的文件只有所有者能删除，即使其他用户在此目录上有rwx权限，即使是root用户。 以1代表sbit。 补充：suid/sgid/sbit的标志位都作用在x位，当原来的x位有x权限时，这些权限位则为s/s/t，如果没有x权限，则变为S/S/T。例如，/tmp目录的权限有个t位，使得该目录里的文件只有其所有者本身能删除。]]></content>
      <categories>
        <category>linux基础</category>
      </categories>
      <tags>
        <tag>linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[系统用户、组管理su和sudo]]></title>
    <url>%2F2019%2F07%2F29%2F%E7%B3%BB%E7%BB%9F%E7%94%A8%E6%88%B7%E3%80%81%E7%BB%84%E7%AE%A1%E7%90%86su%E5%92%8Csudo%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/f-ck-need-u/p/7011669.html 系统用户、组管理su和sudosu切换或以指定用户运行命令。 使用su可以指定运行命令的身份(user/group/uid/gid) 为了向后兼容，su默认不会改变当着目录，且仅设置HOMW和SHELL这两个环境变量(若目标用户非root,则还设置USER和LOGNAME环境变量)。推荐使用–login选项(即-选项)避免环境变量混乱。 1234567891011121314151617su [options...] [-] [user [args...]]选项说明：-c command：使用-c选项传递要指定的命令到shell上执行。使用-c执行命令会为每个su都分配新的会话环境-, -l, --login：启动shell作为登录的shell，模拟真正的登录环境。它会做下面几件事： 1.清除除了TERM外的所有环境变量 2.初始化HOME,SHELL,USER,LOGNAME,PATH环境变量 3.进入目标用户的家目录 4.设置argv[0]为&quot;-&quot;以便设置shell作为登录的shell 使用--login的su是交互式登录。不使用--login的su是非交互式登录(除不带任何参数的su外-m, -p, --preserve-environment：保留整个环境变量(不会重新设置HOME,SHELL,USER和LOGNAME)， 保留环境的方法是新用户shell上执行原用户的各配置文件，如~/.bashrc。 当设置了--login时，将忽略该选项-s SHELL：运行指定的shell而非默认shell，选择shell的顺序优先级如下： 1.--shell指定的shell 2.如果使用了--preserve-environment，选择SHELL环境变量的shell 3.选项目标用户在passwd文件中指定的shell 4./bin/sh 注意： (1). 若su没有给定任何参数，将默认以root身份运行交互式的shell(交互式，所以需要输入密码)，即切换到root用户，但只改变HOME和SHELL环境变量。 (2). su - username是交互式登录，要求密码，会重置整个环境变量，它实际上是在模拟真实的登录环境。 (3). su username是非交互登录，不会重置除HOME/SHELL外的环境变量。 例如：用户wangwu家目录为/home/wangwu，其shell为/bin/csh。 123shell&gt; head -1 /etc/passwd ; tail -1 /etc/passwdroot:x:0:0:root:/root:/bin/bashwangwu:x:2002:2002::/home/wangwu:/bin/csh 首先su到wangwu上，再执行一个完全不带参数的su。 123456789shell&gt; su - wangwu # 使用su - username后，以登录shell的方式模拟登录，会重新设置各环境变量。su - username是交互式登录shell&gt; env | egrep -i &apos;^home|^shell|^path|^logname|^user&apos;HOME=/home/wangwuSHELL=/bin/cshUSER=wangwuLOGNAME=wangwuPATH=/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbinPWD=/home/wangwu 123456789shell&gt; su # 不带任何参数的su，是交互式登录切换回root，但只会改变HOME和SHELL环境变量shell&gt; env | egrep -i &apos;^home|^shell|^path|^logname|^user|^pwd&apos;SHELL=/bin/bashUSER=wangwuPATH=/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbinPWD=/home/wangwuHOME=/rootLOGNAME=wangwu 123456789101112131415161718shell&gt; su - # su - 的方式切换回rootPassword:shell&gt; env | egrep -i &apos;^home|^shell|^path|^logname|^user|^pwd&apos;SHELL=/bin/bashUSER=rootPATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/binPWD=/rootHOME=/rootLOGNAME=rootshell&gt; su wangwu # 再直接su username，它只会重置SHELL和HOME两个环境变量，其他环境变量保持不变shell&gt; env | egrep -i &apos;^home|^shell|^path|^logname|^user|^pwd&apos;SHELL=/bin/cshUSER=wangwuPATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/binPWD=/rootHOME=/home/wangwuLOGNAME=wangwu 在某些环境下或脚本中，可能需要临时切换身份执行命令，注意这时候的环境变量是否会改变，否则很可能报错提示命令找不到。 sudosudo可以让一个用户以某个身份(如root或其他用户)执行某些命令，它隐含的执行方式是切换到指定用户再执行命令，因为涉及到了用户的切换，所以环境变量是否重置是需要设置的。 sudo支持插件实现安全策略。默认的安全策略插件是sudoers，它是通过/etc/sudoers或LDAP来配置的。 安全策略是控制用户使用sudo命令时具有什么权限，但要注意，安全策略可能需要用户进行身份认证，如密码认证的机制或其他认证机制，如果开启了认证要求，则在指定时间内未完成认证时sudo会退出，默认超时时间为5分钟。 安全策略支持对认证进行缓存，使得在一定时间内该用户无需再次认证就可以执行sudo命令，默认缓存时间为5分钟，sudo -v可以更新认证缓存。 sudo支持日志审核，可以记录下成功或失败的sudo。 /etc/sudoers文件该文件里主要配置sudo命令时指定的用户和对应的权限。 123456789101112131415161718192021shell&gt; visudo # 以下选取的是部分行## hostname or IP addresses instead. # 主机别名Host_Alias# Host_Alias FILESERVERS = fs1, fs2# Host_Alias MAILSERVERS = smtp, smtp2## User Aliases # 用户别名User_Alias# User_Alias ADMINS = jsmith, mikem## Command Aliases # 命令别名# Cmnd_Alias SERVICES = /sbin/service, /sbin/chkconfig# Cmnd_Alias LOCATE = /usr/bin/updatedb root ALL=(ALL) ALL # sudo权限的配置# %sys ALL = NETWORKING, SOFTWARE, SERVICES, STORAGE, DELEGATING, PROCESSES, LOCATE, DRIVERS## Allows people in group wheel to run all commands# %wheel ALL=(ALL) ALL## Same thing without a password# %wheel ALL=(ALL) NOPASSWD: ALL在这个文件里，主要有别名(用户别名，主机别名，命令别名)的配置和sudo权限的配置。安全策略配置格式为： 用户名 主机名=(可切换到的用户身份) 权限和命令 ① ② ③ ④ 123456789①用户名：可以用组，只需在组名前加个百分号%表示。②主机名：表示该用户可以在哪些主机上运行sudo，可以用hostname也可以用ip指定。③可切换的用户身份，即指定执行命令的用户，也可以用组。④权限和命令：允许执行和不允许执行的命令(多个命令间用逗号分隔)和特殊权限，命令可以带其选项及参数。命令要写绝对路径。不允许执行的命令需要在命令前加上&quot;!&quot;来表示。可以使用标签，如NOPASSWD标签表示切换或以指定用户执行该标签后的命令时不需要输入密码。一行写不下时可使用&quot;\&quot;续行。标签使用方法： NOPASSWD:/usr/sbin/useradd,PASSWD:/usr/sbin/userdel 12345它表示useradd命令不需要输入密码，而userdel需要输入密码。对于别名，相当于用户对于用户组。权限配置处都可以使用别名，即①②③④处都能使用别名来配置。例如，主机别名里设置多个主机，以后在②位置处直接使用主机别名。 FILESERVERS = fs1, fs2 123456以下是某设置示例：DEFAULT=/bin/*,/sbin/ldconfig,/sbin/ifconfig,/usr/sbin/useradd,/usr/sbin/userdel,/bin/rpm,/usr/bin/yum,/sbin/service,/sbin/chkconfig,sudoedit /etc/rc.local,sudoedit /etc/hosts,sudoedit /etc/ld.so.conf,/bin/mount,sudoedit /etc/exports,/usr/bin/passwd [!-]*,!/usr/bin/passwd root,/bin/su - [!-]*,!/bin/su - root,!/bin/su root, /bin/bash, /usr/sbin/dmidecode, /usr/sbin/lsof, /usr/bin/du, /usr/bin/python, /usr/sbin/xm,sudoedit /etc/profile,sudoedit /etc/bashrc,/usr/bin/make,sudoedit /etc/security/limits.conf,/etc/init.d/*,/usr/bin/rubyABC ALL=(ALL)NOPASSWD:DEFAULT 其中上面的”/usr/bin/passwd [!-]*”表示允许修改加参数的密码。”/bin/su - [!-]*”表示允许”su -“到某用户下，但必须给参数。 sudo和sudoedit命令当sudo执行指定的command时，它会调用fork函数，并设置命令的执行环境(如某些环境变量)，然后在子进程中执行command，sudo的主进程等待命令执行完毕，然后传递命令的退出状态码给安全策略并退出。 12345678910111213141516171819202122232425sudoedit等价于sudo -e，它是以sudo的方式执行文件编辑动作。sudo [options] [command]选项说明：-b ：(background)该选项告诉sudo在后台执行指定的命令。 注意，如果使用该选项，将无法使用任务计划(job)来控制维护这些后台进程， 需要交互的命令应该考虑是否真的要后台，因为可能会失败-l[l] [command]：当单独使用-l选项时，将列出(list)用户可执行和被禁止的命令。 当配合command时，且该command是被允许执行的命令，将列出命令的全路径及该命令参数。 如果command是不被允许执行的，则sudo直接以状态码-1退出。 可以指定多个字母&quot;l&quot;来显示更详细的格式-n ：使得sudo变成非交互模式，但如果安全策略是要求输入密码的，则sudo将报错-S ：(stdin)该选项使得sudo从标准输入而非终端设备上读取密码，给定的密码必须在尾部加上换行符-s [command] ：(shell)指定要切换到的shell，如果给定command，则在此shell上执行该命令-U user ：(other user)配合-l选项来指定要列出哪个用户的权限信息-u user ：(user)该选项明确指定要以此处指定的用户而非root来运行command。 若使用uid的方式指定用户，则需要使用&quot;#uid&quot;，但很多时候可能需要对&quot;#&quot;使用&quot;\&quot;转义，即使用&quot;\#uid&quot;-E ：(environment)该选项告诉sudo在执行命令时保留自己的环境变量，保留环境变量的方式是执行环境配置文件。 但因为跨了用户，所以很可能某些家目录下的环境配置文件会因为无权限而执行失败，此时sudo将报错 -k [command] ：当单独使用-k选项时，sudo将使得用户的认证缓存失效。下次执行sudo命令需要输入密码。 当配合command时，-k选项将忽略用户的缓存，所以sudo将要求用户输入密码，但这次输入密码不会更新认证缓存 但执行-k选项本身，不需要密码-K ：(sure kill)类似于-k选项，但它会完全移除用户的认证缓存，且不会配合command，执行-K本身不需要密码-v ：(validate)该选项使得sudo更新用户认证缓存-- ：暗示sudo命令行参数到此结束 在sudo上可以直接设置环境变量，它会传递为command的环境。设置的方式为var=value，如LD_LIBRARY_PATH=/usr/local/pkg/lib 由于sudo默认的安全策略插件是sudoers，所以当用户执行sudo时，系统会自动去寻找/etc/sudoers文件(该文件里被root配置了用户对应的权限，也即安全策略)，查看sudo要使用的用户是否有对应的权限，如果有则执行，如果没有权限就失败退出sudo。]]></content>
      <categories>
        <category>linux基础</category>
      </categories>
      <tags>
        <tag>linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux系统用户、组管理]]></title>
    <url>%2F2019%2F07%2F29%2Flinux%E7%B3%BB%E7%BB%9F%E7%94%A8%E6%88%B7%E3%80%81%E7%BB%84%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/f-ck-need-u/p/7011669.html linux系统用户/组管理用户和组的基本概念用户和组是操作系统中一种身份认证资源。 每个用户都有用户名、用户的唯一编号uid（user id）、所属组及默认的shell,可能还有密码、家目录、附属组、注释信息等。 每个组也有自己的名称、组唯一编号gid（group id）。一般来说，gid和uid是可以不相同的，但绝大多数都会让它们保持一致，一致属于约定俗成类的概念吧。 组分为主组(primary group)和辅助组(secondary group)两种，用户一定会属于某个主组，也可以同时加入多个辅助组。 在linux中，用户分为3类： (1).超级管理员 下级管理员是最高权限者，它的uid=0,默认超级管理员用户名为root,因为uid默认具有唯一性，所以超级管理员默认只能有一个(如何添加额外的超级管理员，见useradd命令)，但这一个超级管理员的名称并非一定要是root，但没人会去改root的名称，在后续非常非常多的程序中，都认为超级管理员名称为root，这里要是一改，牵一发而动全身。 (2).系统用户 有时候需要一类具有某些特权但又不需要登录操作系统的用户，这类用户称为系统用户。它们的uid范围从201到999（不包括 1000）有些老版本范围是1到499（centos6），出于安全考虑，它们一般不用来登录，所以它们的shell一般是/sbin/nologin,而且 大多数时候它们是没有家目录的。 (3). 普通用户 普通用户是权限受到限制的用户，默认只能执行/bin、/usr/bin、/usr/local/bin和自身家目录下的命令。它们的uid从500开始。尽管普通用户权限收到限制，但是它对自身家目录下的文件是有所有权限的。 超级管理员和其他类型的用户，它们的命令提示符是不一样的。uid=0的超级管理员，命令提示符是”#”，其他的为”$”。 默认root用户的家目录为/root，其他用户的家目录一般在/home下以用户名命名的目录中，如longshuai这个用户的家目录为/home/longshuai。当然，家目录是可以自定义位置和名称的。 用户和组管理相关的文件用户文件/etc/passwd /etc/passwd文件里记录的是操作系统中用户的信息，这里面记录了几行就表示系统中有几个系统用户。它的格式大致如下： 12345678910111213141516171819root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown halt:x:7:0:halt:/sbin:/sbin/halt ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin nobody:x:99:99:Nobody:/:/sbin/nologin sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin mysql:x:27:27:MySQL Server:/var/lib/mysql:/bin/bash nginx:x:498:499:Nginx web server:/var/lib/nginx:/sbin/nologin longshuai:x:1000:1000::/home/longshuai:/bin/bash 每一行表示一个用户，每一行的格式都是6个冒号共7列属性，其中有很多用户的某些列属性是留空的。 12345678用户名:x:uid:gid:用户注释信息:家目录:使用的shell类型 •第一列：用户名。注意两个个特殊的用户名，root、nobody •第二列：x。在以前老版本的系统上，第二列是存放用户密码的，但是密码和用户信息放在一起不便于管理(密钥要保证其特殊属性)，所以后来将密码单独放在另一个文件/etc/shadow中，这里就都写成x了•第三列：uid•第四列：gid•第五列：用户注释信息。•第六列：用户家目录。注意root用户的家目录为/root •第七列：用户的默认shell，虽然叫shell，但其实可以是任意一个可执行程序或脚本。例如上面的/bin/bash、/sbin/nologin、/sbin/shutdown 用户的默认shell表示的是用户登录(如果允许登录)时的环境或执行的命令。例如shell为/bin/bash时，表示登录时就执行/bin/bash命令进入bash环境；shell为/sbin/nologin表示该用户不能登录，之所以不能登录不是因为指定了这个特殊的程序，而是由/sbin/nologin这个程序的功能实现的，假如修改Linux的源代码，将/sbin/nologin这个程序变成可登录，那么shell为/sbin/nologin时也是可以登录的。 密码文件/etc/shadow/etc/shadow文件中存放的是用户的密码信息。该文件具有特殊属性，除了超级管理员，任何人都不能直接读取和修改该文件，而用户自身之所以能修改密码，则是因为passwd程序的suid属性，使得修改密码时临时提升为root权限。 该文件的格式大致如下： 123456789101112131415161718root:$6$hS4yqJu7WQfGlk0M$Xj/SCS5z4BWSZKN0raNncu6VMuWdUVbDScMYxOgB7mXUj./dXJN0zADAXQUMg0CuWVRyZUu6npPLWoyv8eXPA.::0:99999:7::: ftp:*:16659:0:99999:7::: nobody:*:16659:0:99999:7::: longshuai:$6$8LGe6Eh6$vox9.OF3J9nD0KtOYj2hE9DjfU3iRN.v3up4PbKKGWLOy3k1Up50bbo7Xii/Uti05hlqhktAf/dZFy2RrGp5W/:17323:0:99999:7:::每一行表示一个用户密码的属性，有8个冒号共9列属性。该文件更详细的信息看wiki：https://en.wikipedia.org/wiki/Passwd#Shadow_file。•第一列：用户名。•第二列：加密后的密码。但是这一列是有玄机的，有些特殊的字符表示特殊的意义。 ◦①.该列留空，即&quot;::&quot;，表示该用户没有密码。◦②.该列为&quot;!&quot;，即&quot;:!:&quot;，表示该用户被锁，被锁将无法登陆，但是可能其他的登录方式是不受限制的，如ssh key的方式，su的方式。◦③.该列为&quot;*&quot;，即&quot;:*:&quot;，也表示该用户被锁，和&quot;!&quot;效果是一样的。◦④.该列以&quot;!&quot;或&quot;!!&quot;开头，则也表示该用户被锁。◦⑤.该列为&quot;!!&quot;，即&quot;:!!:&quot;，表示该用户从来没设置过密码。◦⑥.如果格式为&quot;$id$salt$hashed&quot;，则表示该用户密码正常。其中$id$的id表示密码的加密算法，$1$表示使用MD5算法，$2a$表示使用Blowfish算法，&quot;$2y$&quot;是另一算法长度的Blowfish,&quot;$5$&quot;表示SHA-256算法，而&quot;$6$&quot;表示SHA-512算法，可见上面的结果中都是使用sha-512算法的。$5$和$6$这两种算法的破解难度远高于MD5。$salt$是加密时使用的salt，$hashed才是真正的密码部分。•第三列：从1970年1月1日到上次密码修改经过的时间(天数)。通过计算现在离1970年1月1日的天数减去这个值，结果就是上次修改密码到现在已经经过了多少天，即现在的密码已经使用了多少天。•第四列：密码最少使用期限(天数)。省略或者0表示不设置期限。例如，刚修改完密码又想修改，可以限制多久才能再次修改•第五列：密码最大使用期限(天数)。超过了它不一定密码就失效，可能下一个字段设置了过期后的宽限天数。设置为空时将永不过期，后面设置的提醒和警告将失效。root等一些用户的已经默认设置为了99999，表示永不过期。如果值设置小于最短使用期限，用户将不能修改密码。•第六列：密码过期前多少天就开始提醒用户密码将要过期。空或0将不提醒。•第七列：密码过期后宽限的天数，在宽限时间内用户无法使用原密码登录，必须改密码或者联系管理员。设置为空表示没有强制的宽限时间，可以过期后的任意时间内修改密码。•第八列：帐号过期时间。从1970年1月1日开始计算天数。设置为空帐号将永不过期，不能设置为0。不同于密码过期，密码过期后账户还有效，改密码后还能登录；帐号过期后帐号失效，修改密码重设密码都无法使用该帐号。•第九列：保留字段。 组文件/etc/group和/etc/gshadow大致知道有这么两个文件即可，至于文件中的内容无需关注。 /etc/group包含了组信息。每行一个组，每一行3个冒号共4列属性。 12345root:x:0: longshuai:x:500: xiaofang:x:501:zhangsan,lisi•第一列：组名。•第二列：占位符。•第三列：gid。•第四列：该组下的user列表，这些user成员以该组做为辅助组，多个成员使用逗号隔开。 /etc/gshadow包含了组密码信息 骨架目录/etc/skel骨架目录中的文件是每次新建用户时，都会复制到新用户家目录里的文件。默认只有3个环境配置文件，可以修改这里面的内容，或者添加几个文件在骨架目录中，以后新建用户时就会自动获取到这些环境和文件。 12345shell&gt; ls –l -A /etc/skeltotal 12-rw-r--r--. 1 root root 18 Oct 16 2014 .bash_logout-rw-r--r--. 1 root root 176 Oct 16 2014 .bash_profile-rw-r--r--. 1 root root 124 Oct 16 2014 .bashrc 删除家目录下这些文件，会导致某些设置出现问题。例如删除”.bashrc”这个文件，会导致提示符变异的问题,如-bash-4.2$ [linshuai@server2 ~]$要解决这个问题，只需拷贝一个正常的.bashrc文件到其家目录中即可。一般还会修改该文件的所有者和权限。 /etc/login.defs设置用户帐号限制的文件。该文件里的配置对root用户无效。 如果/etc/shadow文件里有相同的选项，则以/etc/shadow里的设置为准，也就是说/etc/shadow的配置优先级高于/etc/login.defs。 该文件有很多配置项，文件的默认内容只给出了一小部分，若想知道全部的配置项以及配个配置项的详细说明，可以”man 5 login.defs”查看。 123456789101112131415161718192021222324252627282930313233343536373839404142[root@xuexi ~]# less /etc/login.defs#QMAIL_DIR Maildir # QMAIL_DIR是Qmail邮件的目录，所以可以不设置它MAIL_DIR /var/spool/mail # 默认邮件根目录，即信箱#MAIL_FILE .mail # mail文件的格式是.mail# Password aging controls:PASS_MAX_DAYS 99999 # 密码最大有效期(天)PASS_MIN_DAYS 0 # 两次密码修改之间最小时间间隔PASS_MIN_LEN 5 # 密码最短长度PASS_WARN_AGE 7 # 密码过期前给警告信息的时间# 控制useradd创建用户时自动选择的uid范围# Min/max values for automatic uid selection in useraddUID_MIN 1000UID_MAX 60000# System accountsSYS_UID_MIN 201SYS_UID_MAX 999# 控制groupadd创建组时自动选择的gid范围# Min/max values for automatic gid selection in groupaddGID_MIN 1000GID_MAX 60000# System accountsSYS_GID_MIN 201SYS_GID_MAX 999# 设置此项后，在删除用户时，将自动删除用户拥有的at/cron/print等job#USERDEL_CMD /usr/sbin/userdel_local# 控制useradd添加用户时是否默认创建家目录，useradd -m选项会覆盖此处设置CREATE_HOME yes# 设置创建家目录时的umask值，若不指定则默认为022UMASK 077# 设置此项表示当组中没有成员时自动删除该组# 且useradd是否同时创建同用户名的主组。(该文件中并没有此项说明，来自于man useradd中-g选项的说明)USERGROUPS_ENAB yes# 设置用户和组密码的加密算法ENCRYPT_METHOD SHA512 注意，/etc/login.defs中的设置控制的是shadow-utils包中的组件，也就是说，该组件中的工具执行操作时会读取该文件中的配置。该组件中包含下面的程序： 12345678910111213141516171819202122/usr/bin/gpasswd ：administer /etc/group and /etc/gshadow/usr/bin/newgrp ：log in to a new group，可用来修改gid，哪怕是正在登陆的会话也可以修改/usr/bin/sg ：execute command as different group ID/usr/sbin/groupadd ：添加组/usr/sbin/groupdel ：删除组/usr/sbin/groupmems ：管理当前用户的主组中的成员，root用户则可以指定要管理的组/usr/sbin/groupmod ：modify a group definition on the system/usr/sbin/grpck ：verify integrity of group files/usr/sbin/grpconv ：无视它/usr/sbin/grpunconv ：无视它/usr/sbin/pwconv ：无视它/usr/sbin/pwunconv ：无视它/usr/sbin/adduser ：是useradd的一个软链接，添加用户/usr/sbin/chpasswd ：update passwords in batch mode/usr/sbin/newusers ：update and create new users in batch/usr/sbin/pwck ：verify integrity of passsword files/usr/sbin/useradd ：添加用户/usr/sbin/userdel ：删除用户/usr/sbin/usermod ：重定义用户信息/usr/sbin/vigr ：edit the group and shadow-group file/usr/sbin/vipw ：edit the password and shadow-password file/usr/bin/lastlog ：输出所有用户或给定用户最近登录信息 /etc/default/useradd创建用户时的默认配置。useradd -D修改的就是此文件。 12345678910[root@xuexi ~]# cat /etc/default/useradd # useradd defaults fileGROUP=100 # 在useradd使用-N或/etc/login.defs中USERGROUPS_ENAB=no时表示创建用户时不创建同用户名的主组(primary group)， # 此时新建的用户将默认以此组为主组，网上关于该设置的很多说明都是错的，具体可看man useradd的-g选项或useradd -D的-g选项HOME=/home # 把用户的家目录建在/home中INACTIVE=-1 # 是否启用帐号过期设置(是帐号过期不是密码过期)，-1表示不启用EXPIRE= # 帐号过期时间，不设置表示不启用SHELL=/bin/bash # 新建用户默认的shell类型SKEL=/etc/skel # 指定骨架目录，前文的/etc/skel就在这里CREATE_MAIL_SPOOL=yes # 是否创建用户mail缓冲 man useradd的useradd -D选项介绍部分说明了这些项的意义 用户和组管理命令useradd和adduser1234567891011121314151617181920212223242526272829303132333435adduser是useradd的一个软链接。useradd [options] login_name选项说明：-b：指定家目录的basedir，默认为/home目录-d：指定用户家目录，不写时默认为/home/user_name-m：要创建家目录时，若家目录不存在则自动创建，若不指定该项且/etc/login.defs中的CREATE_HOME未启用时将不会创建家目录-M：显式指明不要创建家目录，会覆盖/etc/login.defs中的CREATE_HOME设置 -g：指定用户主组，要求组已存在-G：指定用户的辅助组，多个组以逗号分隔-N：明确指明不要创建和用户名同名的组名-U：明确指明要创建一个和用户名同名的组，并将用户加入到此组中-o：允许创建一个重复UID的用户，只有和-u选项同时使用时才生效-r：创建一个系统用户。useradd命令不会为此选项的系统用户创建家目录，除非明确使用-m选项-s：指定用户登录的shell，默认留空。此时将选择/etc/default/useradd中的SHELL变量设置-u：指定用户uid，默认uid必须唯一，除非使用了-o选项-c：用户的注释信息 -k：指定骨架目录(skeleton)-K：修改/etc/login.defs文件中有关于用户的配置项，不能修改组相关的配置。设置方式为KEY=VALUE，如-K UID_MIN=100-D：修改useradd创建用户时的默认选项，就修改/etc/default/useradd文件-e：帐户过期时间，格式为&quot;YYYY-MM-DD&quot;-f：密码过期后，该账号还能存活多久才被禁用，设置为0表示密码过期立即禁用帐户，设置为-1表示禁用此功能-l：不要将用户的信息写入到lastlog和faillog文件中。默认情况下，用户信息会写入到这两个文件中useradd -D [options]修改/etc/default/useradd文件选项说明：不加任何选项时会列出默认属性-b, --base-dir BASE_DIR-e, --expiredate EXPIRE_DATE-f, --inactive INACTIVE-g, --gid GROUP-s, --shell SHELL 示例: 1234567891011121314151617181920[root@xuexi ~]# useradd -D -e &quot;2016-08-20&quot; # 设置用户2016-08-20过期[root@xuexi ~]# useradd -DGROUP=100HOME=/homeINACTIVE=-1EXPIRE=2016-08-20SHELL=/bin/bashSKEL=/etc/skelCREATE_MAIL_SPOOL=yes[root@xuexi ~]# cat /etc/default/useradd# useradd defaults fileGROUP=100HOME=/homeINACTIVE=-1EXPIRE=2016-08-20SHELL=/bin/bashSKEL=/etc/skelCREATE_MAIL_SPOOL=yes useradd创建用户时，默认会自动创建一个和用户名相同的用户组，这是/etc/login.defs中的USERGROUP_ENAB变量控制的。 useradd创建普通用户时，不加任何和家目录相关的选项时，是否创建家目录是由/etc/login.defs中的CREATE_HOME变量控制的。 批量创建用户newusersnewusers用于批量创建或修改已有用户信息。在创建用户时，它会读取/etc/login.defs文件中的配置项。 newusers [options] [file] newusers命令从file中或标准输入中读取要创建或修改用户的信息，文件中每行格式都一样，一行代表一个用户。格式如下： 1234567891011pw_name:pw_passwd:pw_uid:pw_gid:pw_gecos:pw_dir:pw_shell各列的意义如下：•pw_name：用户名，若不存在则新创建，否则修改已存在用户的信息•pw_passwd：用户密码，该项使用明文密码，在修改或创建用户时会按照指定的算法自动对其进行加密转换•pw_uid：指定uid，留空则自动选择uid。如果该项为已存在的用户名，则使用该用户的uid，但不建议这么做，uid应尽量保证唯一性•pw_gid：用户主组的gid或组名。若给定组不存在，则自动创建组。若留空，则创建同用户名的组，gid将自动选择•pw_gecos：用户注释信息•pw_dir：指定用户家目录，若不存在则自动创建。留空则不创建。：注意，newusers命令不会递归创建父目录，父目录不存在时将会给出信息，但newusers命令仍会继续执行：以完成创建剩下的用户，所以这些错误的用户家目录需要手动去创建。 •pw_shell：指定用户的默认shell newusers [options] [file] 选项说明： -c：指定加密方法，可选DES,MD5,NONE,SHA256和SHA512 -r：创建一个系统用户 newusers首先尝试创建或修改所有指定的用户，然后将信息写入到user和group的文件中。如果尝试创建或修改用户过程中发生错误，则所有动作都将回滚，但如果在写入过程中发生错误，则写入成功的不会回滚，这将可能导致文件的不一致性。要检查用户、组文件的一致性，可以使用showdow-utils包提供的grpck和pwck命令。 示例： 12345678910111213shell&gt; cat /tmp/userfilezhangsan:123456:2000:2000::/home/zhangsan:/bin/bashlisi:123456:::::/bin/bashshell&gt; newusers -c SHA512 /tmp/userfile shell&gt; tail -2 /etc/passwdzhangsan:x:2000:2000::/home/zhangsan:/bin/bashlisi:x:2001:2001:::/bin/bashshell&gt; tail -2 /etc/shadowzhangsan:$6$aI1Mk/krF$xN0TFOIRibrb/mYngJ/sV3M7g4zOxqOh8CWyDlI0uwmr5qNTzsmwauRFvCpfLtvtiJYZ/5bil.XfJMNB.sqDY1:17323:0:99999:7:::lisi:$6$bngXo/V6wWW$.TlQCJtEm9krBX0Oiep/iahS59a/BwVYcSc8F9lAnMGF55K6W5YoUZ2nK6WkMta3p7sihkxHm/AuNrrJ6hqNn1:17323:0:99999:7::: groupadd创建一个新组。 1234567groupadd [options] group选项说明：-f：如果要创建的组已经存在，默认会错误退出，使用该选项则强制创建且以正确状态退出，只不过gid可能会不受控制。-g：指定gid，默认gid必须唯一，除非使用了-o选项。-K：修改/etc/login.defs中关于组相关的配置项。配置方式为KEY=VALUE，例如-K GID_MIN=100 -K GID_MAX=499-o：允许创建一个非唯一gid的组-r：创建系统组 修改密码passwd修改密码的工具。默认passwd命令不允许为用户创建空密码。 passwd修改密码前会通过pam认证用户，pam配置文件中与此相关的设置项如下： 123passwd password requisite pam_cracklib.so retry=3passwd password required pam_unix.so use_authtok 命令的用法如下： 123456789101112passwd options [username]选项说明：-l：锁定指定用户的密码，在/etc/shadow的密码列加上前缀&quot;!&quot;或&quot;!!&quot;。这种锁定不是完全锁定，使用ssh公钥还是能登录。要完全锁定，使用chage -E 0来设置帐户过期。-u：解锁-l锁定的密码，解锁的方式是将/etc/shadow的密码列的前缀&quot;!&quot;或&quot;!!&quot;移除掉。但不能移除只有&quot;!&quot;或&quot;!!&quot;的项。--stdin：从标准输入中读取密码-d：删除用户密码，将/etc/shadow的密码列设置为空-f：指定强制操作-e：强制密码过期，下次登录将强制要求修改密码-n：密码最小使用天数-x：最大密码使用天数-w：过期前几天开始提示用户密码将要过期-i：设置密码过期后多少天，用户才过期。用户过期将被禁用，修改密码也无法登陆。 批量修改密码chpasswd以批处理模式从标准输入中获取提供的用户和密码来修改用户密码，可以一次修改多个用户密码。也就是说不用交互。适用于一次性创建了多个用户时为他们提供密码。 1234chpasswd [-e -c] &quot;user:passwd&quot;-c：指定加密算法，可选的算法有DES,MD5,NONE,SHA256和SHA512user:passwd为用户密码对，其中默认passwd是明文密码，可以指定多对，每行一个用户密码对。前提是用户是已存在的。-e：passwd默认使用的是明文密码，如果要使用密文，则使用-e选项。参见man chpasswd chpasswd会读取/etc/login.defs中的相关配置，修改成功后会将密码信息写入到密码文件中。 该命令的修改密码的处理方式是先在内存中修改，如果所有用户的密码都能设置成功，然后才写入到磁盘密码文件中。在内存中修改过程中出错，则所有修改都回滚，但若在写入密码文件过程中出错，则成功的不会回滚。 示例： 修改单个用户密码。 1shell&gt; echo &quot;user1:123456&quot; | chpasswd -c SHA512 修改多个用户密码，则提供的每个用户对都要分行。 1shell&gt; echo -e &apos;usertest:123456\nusertest2:123456&apos; | chpasswd 更方便的是写入到文件中，每行一个用户密码对。 12345shell&gt; cat /tmp/passwdfilezhangsan:123456lisi:123456shell&gt; chapasswd -c SHA512 &lt;/tmp/passwdfile chagechage命令主要修改或查看和密码时间相关的内容。具体的看man文档，可能用到的两个选项如下： -l：列出指定用户密码相关信息 -E：指定帐户(不是密码)过期时间，所以是强锁定，如果指定为0，则立即过期，即直接锁定该用户 12345678910111213141516171819[root@server2 ~]# chage -l zhangsanLast password change : Jun 06, 2017Password expires : neverPassword inactive : neverAccount expires : neverMinimum number of days between password change : 0Maximum number of days between password change : 99999Number of days of warning before password expires : 7[root@server2 ~]# chage -E 0 zhangsan[root@server2 ~]# chage -l zhangsan Last password change : Jun 06, 2017Password expires : neverPassword inactive : neverAccount expires : Jan 01, 1970Minimum number of days between password change : 0Maximum number of days between password change : 99999Number of days of warning before password expires : 7 删除用户和组 userdel命令用于删除用户。 123userdel [options] login_name-r：递归删除家目录，默认不删除家目录。-f：强制删除用户，即使这个用户正处于登录状态。同时也会强制删除家目录。 一般不直接删除家目录，即不用-r，可以vim /etc/passwd，将不需要的用户直接注释掉。 groupdel命令删除组。如果要删除的组是某用户的主组，需要先删除主组中的用户。 usermod修改帐户属性信息。必须要确保在执行该命令的时候，待修改的用户没有在执行进程。 1234567891011121314151617181920usermod [options] login选项说明：-l：修改用户名，仅仅只是改用户名，其他的一切都不会改动(uid、家目录等)-u：新的uid，新的uid必须唯一，除非同时使用了-o选项-g：修改用户主组，可以是以gid或组名。对于那些以旧组为所属组的文件(除原家目录)，需要重新手动修改其所属组-m：移动家目录内容到新的位置，该选项只在和-d选项一起使用时才生效-d：修改用户的家目录位置，若不存在则自动创建。默认旧的家目录不会删除 如果同时指定了-m选项，则旧的家目录中的内容会移到新家目录 如果当前用户家目录不存在或没有家目录，则也不会创建新的家目录-o：允许用户使用非唯一的UID-s：修改用的shell，留空则选择默认shell-c：修改用户注释信息-a：将用户以追加的方式加入到辅助组中，只能和-G选项一起使用-G：将用户加入指定的辅助组中，若此处未列出某组，而此前该用户又是该组成员，则会删除该组中此成员-L：锁定用户的密码，将在/etc/shadow的密码列加上前缀&quot;!&quot;或&quot;!!&quot;-U：解锁用户的密码，解锁的方式是移除shadow文件密码列的前缀&quot;!&quot;或&quot;!!&quot;-e：帐户过期时间，时间格式为&quot;YYYY-MM-DD&quot;，如果给一个空的参数，则立即禁用该帐户-f：密码过期后多少天，帐户才过期被禁用，0表示密码过期帐户立即禁用，-1表示禁用该功能 同样，还有groupmod修改组信息，用法非常简单，几乎也用不上，不多说了。 vipw和vigrvipw和vigr是编辑用户和组文件的工具，vipw可以修改/etc/passwd和/etc/shadow，vigr可以修改/etc/group和/etc/gshadow，用这两个工具比较安全，在修改的时候会检查文件的一致性。 删除用户出错时，提示用户正在被进程占用。可以使用vi编辑/etc/paswd和/etc/shadow文件将该用户对应的行删除掉。也可以使用vipw和vipw -s来分别编辑/etc/paswd和/etc/shadow文件。它们的作用是一样的。 手动创建用户手动创建用户的全过程：需要管理员权限。 •在/etc/group中添加用户所属组的相关信息。如果用户还有辅助组则在对应组中加入该用户作为成员。 •在/etc/passwd和/etc/shadow中添加用户相关信息。此时指定的家目录还不存在，密码不存在，所以/etc/shadow的密码位使用”!!”代替。 •创建家目录，并复制骨架目录中的文件到家目录中。 12shell&gt; mkdir /home/user_nameshell&gt; cp -r /etc/skel /home/user_name。 •修改家目录及子目录的所有者和属组。 1shell&gt; chown -R user_name:user_name /home/user_name •修改家目录及子目录的权限。例如设置组和其他用户无任何权限但所有者有。 1shell&gt; chmod -R 700 /home/user_name 到此为止，用户已经创建完成了，只是没有密码，所以只能su，不能登录。 •生成密码。 ◦使用openssl passwd生成密码。但openssl passwd生成的密码只能是MD5算法的，很容易被破解 123# 生成使用md5算法的密码，然后将其复制到/etc/shadow对应的密码位# 其中-1是指md5，-salt &apos;12345678&apos;是使用8位字符创建密码的杂项shell&gt; openssl passwd -1 -salt &apos;12345678&apos; &apos;123456&apos; ◦直接使用passwd命令创建密码 •测试手动创建的用户是否可以正确登录。 以下是全过程。 12345678shell&gt; mkdir /tmp/12;cp /etc/group /etc/passwd /etc/shadow /tmp/12/ # 备份这些文件shell&gt; echo &quot;userX:x:666&quot; &gt;&gt; /etc/groupshell&gt; echo &quot;userX:x:666:666::/home/userX:/bin/bash&quot; &gt;&gt; /etc/passwdshell&gt; echo &apos;userX:!!:17121:0:99999::::&apos; &gt;&gt; /etc/shadowshell&gt; cp -r /etc/skel /home/userXshell&gt; chown -R userX:userX /home/userXshell&gt; chmod -R go= /home/userXshell&gt; passwd --stdin userX &lt;&lt;&lt; &apos;123456&apos; 测试使用userX是否可以登录。 如果是使用openssl passwd创建的密码。那么使用下面的方法将这部分密码替换到/etc/shadow中。 123shell&gt; field=$(tail -1 /etc/shadow | cut -d&quot;:&quot; -f2)shell&gt; password=$(openssl passwd -1 -salt &apos;abcdefg&apos; 123456)shell&gt; sed -i &apos;$s%&apos;$field&apos;%&apos;$password&apos;%&apos; /etc/shadow 其他用户相关命令2.4.1 finger查看用户信息 从CentOS 6版本开始就没有该命令了，要先安装。 12345678910shell&gt; yum -y install fingershell&gt; useradd zhangsanshell&gt; finger zhangsanLogin: zhangsan Name:Directory: /home/zhangsan Shell: /bin/bashNever logged in.No mail.No Plan. 2.4.2 id 1234id username-u：得到uid-n：得到用户名而不是uid-z：无任何空白字符输出模式，不能在默认的格式下使用。 示例： 1234567891011shell&gt; id rootuid=0(root) gid=0(root) groups=0(root)shell&gt; id wangwuuid=500(wangwu) gid=500(wangwu) groups=500(wangwu)shell&gt; id -u wangwu500shell&gt; id -u -z wangwu2002[root@server2 ~]# 2.4.3 users 查看当前正在登陆的用户名。 2.4.4 last 查看最近登录的用户列表，其实last查看的是/var/log/wtmp文件。 -n 显示行数：列出最近几次登录的用户 12345678[root@xuexi ~]# last -4root pts/0 192.168.100.1 Wed Mar 30 15:16 still logged in root pts/1 192.168.100.1 Wed Mar 30 14:21 - 14:21 (00:00) root pts/1 192.168.100.1 Wed Mar 30 14:04 - 14:10 (00:06) root pts/0 192.168.100.1 Wed Mar 30 13:12 - 15:16 (02:04) wtmp begins Thu Feb 18 20:59:39 2016 2.4.5 lastb 查看谁尝试登陆过但没有登录成功的。即能够审核和查看谁曾经不断的登录，可能那就是黑客。 -n:只列出最近的n个尝试对象。 2.4.6 who和w 都是查看谁登录过，并干了什么事 w查看的信息比who多。 123456789shell&gt; whoroot tty1 2017-06-07 00:49root pts/0 2017-06-07 02:06 (192.168.100.1)shell&gt; w08:26:38 up 18:48, 2 users, load average: 0.00, 0.01, 0.05USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot tty1 00:49 7:36m 0.24s 0.24s -bashroot pts/0 192.168.100.1 02:06 6.00s 0.97s 0.02s w 其中w的第一行，分别表示当前时间，已开机时长，当前在线用户，过去1、5、15分钟的平均负载率。这一行和uptime命令获取的信息是完全一致的。 2.4.7 lastlog 可以查看登录的来源IP -u 指定查看用户 1234567891011shell&gt; lastlog|head -n 10Username Port From Latestroot pts/0 192.168.100.1 Wed Mar 30 15:16:25 +0800 2016bin **Never logged in**daemon **Never logged in**adm **Never logged in**lp **Never logged in**sync **Never logged in**shutdown **Never logged in**halt **Never logged in**mail **Never logged in**]]></content>
      <categories>
        <category>linux基础</category>
      </categories>
      <tags>
        <tag>linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux文件类基础命令]]></title>
    <url>%2F2019%2F07%2F28%2Flinux%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[本文原创地址：博客园骏马金龙https://www.cnblogs.com/f-ck-need-u/p/6995195.html linux文件类基础命令1.关于路径和通配符linux中分绝对路径和相对路径，绝对路径一定是从/开始写的，相对路径不从根开始写，还可能使用路径符号。 路径展开符号： 1234567. :一个点表示当前目录.. :两个点表示上一层目录- ：一个短横线表示上一次使用的目录,例如从/tmp直接切换到/etc下，&quot;-&quot;就表示/tmp~ ：(波浪符号)表示用户的家目录，例如&quot;~account&quot;表示account用户的家目录/dir/和/dir：一般都表示dir目录和dir目录中的文件。但在有些地方会严格区分是否加尾 随斜线，此时对于加了尾随斜线的表示此目录中的文件，不加尾随斜线的表示 该目录本身和此目录中的文件 切换路径用cd命令； 显示当前所在目录用pwd命令。若当前所在目录为链接目录，使用pwd显示的将是链接自身，使用-P选项将定位到链接的原始目录。 12345[root@ansible6_node1 ~]# ll ; cd tmp; pwd; pwd -Ptotal 0lrwxrwxrwx 1 root root 4 May 30 19:17 tmp -&gt; /tmp/root/tmp/tmp 获取文件名使用basename命令，获取文件所在目录使用dirname命令。注意，这两个命令其实不太完善，它不会检查文件或目录是否存在，只要写出来了就会去获取。 12345678910[root@xuexi tmp]# basename /etc/shadowshadow[root@xuexi tmp]# basename /etc/etc[root@xuexi tmp]# dirname /etc/shadow/etc[root@xuexi tmp]# dirname /etc/ # 对目录使用dirname获取的是上级目录/[root@server1 ~]# dirname /kalsldk/kdkskks/djfjdjdjsj # 获取不存在的目录/kalsldk/kdkskks bash shell通配符： 可以使用”*”、”?”、”[]”等的通配符来扩展路径或文件名。例如， ls *.log 将列出当前路径下所有以”.log”字符结尾的文件名(但不包括”.”开头的隐藏文件)。 默认情况下，bash提供的通配符规则比较弱，例如”*”无法匹配文件名开头的”.”，无法匹配路径分隔符号(即斜线”/“)，但可以通过set或shopt命令开启额外的通配功能，实现更完善的通配符规则。 例如，默认情况下，想要匹配目录/path下所有隐藏文件和非隐藏文件，如下： 1ls .* * 开启dotglob功能，”*”就可以匹配以”.”开头的文件： 12shopt -s dotglobls * 有时想要递归到目录内部，又想要匹配文件名，例如想要递归找出多层目录/path下所有的”.css”文件，这时可以开启globstar功能，使用”两星连珠”(**)就可以匹配匹配路径斜线。 12shopt -s globstar # 开启星号匹配模式ls /path/**/*.css # 开启后，使用两个星号**就会匹配斜线 必须要说明的是，对于非bash内置命令，有些可能也提供了自己的通配符匹配方式，它们的通配模式和shell提供的可能并不一样。例如find的”-name”选项就可以采用自己的通配符，它的星号”*”可以匹配以点开头的隐藏文件，如/var/log -name "*.log"。123456789#### 2. 查看目录内容(ls和tree)ls命令列出目录中的内容，和dir命令完全等价。tree命令按树状结构递归列出目录和子目录中的内容，而ls使用-R选项时才会递归列出。 注意：ls的结果中是以制表符分隔多个文件的。2.1 ls命令ls的各个选项说明如下： -l：(long)长格式显示，即显示属性等信息(包括mtime)。注意：显示的目录大小是节点所占大小。像win一样计算目录大小时包括文件大小要用du -sh -c：列出ctime -u：列出atime -d：(direcorty)查看目录本身属性信息，不查看目录里面的东西。不加-d会查看里面文件的信息 -a：会显示所有文件，包括两个相对路径的文件”.”和”..”以及以点开头的隐藏文件 -A：会列出绝大多数文件，即忽略两个相对路径的文件”.”和”..” -h：(human)人类可读的格式，将字节换成k,将K换成M，将M换成G -i：(inode)权限属性的前面加上一堆数字 -p：对目录加上/标识符以作区分 -F：对不同类型的文件加上不同标识符以作区分，对目录加的文件也是/ -t：按修改时间排序内容。不加任何改变顺序的选项时，ls默认按照字母顺序排序 -r：反转排序 -R：递归显示 -S：按文件大小排序，默认降序排序 –color：显示颜色 -m：使用逗号分隔各文件，当然，只适用于未使用长格式(ls -l)的情况 -1：(数值一)，以换行符分隔文件，当然，和-m或-l(小写字母)是冲突的 -I pattern：忽略被pattern匹配到的文件 12345注意，ls以-h显示文件大小时，一般显示的都是不带B的单位，如K/M/G，它们的转换比例是1024，如果显示的都是带了B的，如KB/MB/GB，则它们的转换比例为1000而非1024，一般很少显示带B的大小。 不得不说，ls本身不能显示出文件的全路径名是一大缺陷，不过好在使用find命令可以很简单的就获取到。以下是使用ls -l显示文件长格式的属性。 [root@xuexi ~]# ll /tmpdrwxr-xr-x 2 root root 4096 Mar 26 16:44 test1 #分析当为d则是目录，当为-则是文件，当为l则表示为链接文件这是文件的类型rwx，r-x,r-x分别表示所有者权限，所属组权限，其他人权限2表示硬链接数root所有者，root所属组4096文件大小(字节)-h可以转换显示方式Mar 26 16:44:最近一次修改的日期test1:表示文件(目录名) 123456#### 2.2tree命令有可能tree命令不存在，需要安装tree包才有(安装：yum -y install tree)。 tree命令的选项说明如下： 【 匹配选项：】 -L：用于指定递归显示的深度，指定的深度必须是大于0的整数。 -P：用于显示通配符匹配模式的目录和文件，但是不管是否匹配，目录一定显示。 -I：用于显示除被通配符匹配外的所有目录和文件。 1【 显示选项：】 -a：用于显示隐藏文件，默认不显示。 -d：指定只显示目录。 -f：指定显示全路径。 -i：不缩进显示。和-f一起使用很有用。 -p：用于显示权限位信息。 -h：用于显示大小。 -u：显示username或UID(当没有username时只能显示UID了)。 -g：显示groupname或GID。 -D：显示文件的最后一次Mtime。 –inodes：显示inode号。 –device：显示文件或目录所属的设备号。 -C：显示颜色。 1【 输出选项：】 -o filename：指定将tree的结果输出到filename文件中。 12345678910111213141516171819202122232425#### 3. 文件的时间戳(atime/ctime/mtime)文件的时间属性有三种：atime/ctime/mtime。atime是access time，即上一次的访问时间；mtime是modify time，是文件的修改时间；ctime是change time，也是文件的修改时间，只不过这个修改时间计算的inode修改时间，也就是元数据修改时间。文件还有一个创建时间(create time)，大多数unix系统上都认为这是个无用的属性，一般工具无法获取这个时间，但是对于ext家族文件系统，通过它的底层调试工具debugfs可以获取create time。但mtime只有修改文件内容才会改变，更准确的说是修改了它的data block部分；而ctime是修改文件属性时改变的，确切的说是修改了它的元数据部分，例如重命名文件，修改文件所有者，移动文件(移动文件没有改变datablock，只是改变了其inode指针，或文件名)等.当然，修改文件内容也一定会改变ctime(修改文件内容至少已经修改了inode记录上的mtime，这也是元数据)，也就是说mtime的改变一定会引起ctime的改变。 对目录而言，考虑目录文件的data block，可知在目录中创建、删除文件以及目录内其他任意文件操作都会改变mtime，因为目录里的任何东西都是目录中的内容；而目录的ctime，除了目录的mtime引起ctime改变之外，对目录本身的元数据修改也会改变ctime。 总结下： (1).atime只在文件被打开访问时才改变，若不是打开文件编辑内容(如重定向内容到文件中)，则ctime和mtime的改变不会引起atime的改变; (2).mtime的改变一定引起ctime的改变，而访问文件时(例如cat)，atime不一定会改变，所以atime&quot;改变&quot;(这个改变是假象，见下文分析)不一定会影响ctime。(见下面的relatime说明)#### 3.1 关于relatimeatime/ctime/mtime是Posix标准要求操作系统维护的时间戳信息。但是每次将atime、ctime和mtime写入到硬盘中(这些不会写入缓存，只要修改就是写入磁盘，即使从缓存读取文件内容也如此)效率很低。有多低？下图是写ctime消耗的时间，几乎总要花费零点几秒。mtime要被修改，必然是修改了文件内容，这时候将mtime写入到硬盘中是应该的。但是atime和ctime呢？很多情况下根本用不到atime和ctime，在频繁访问文件的时候，都要修改atime和ctime，这样效率会降低很多很多，所以mount有个noatime选项来避免这种负面影响。CentOS6引入了一个新的atime维护机制relatime：除非两次修改atime的时间超过1天(默认设置86400秒)，或者修改了mtime，否则访问文件的inode不会引起atime的改变。换句话说，当cat一个文件的时候，它的atime可能会改变，但是你稍后再cat，它不会再改变。由于cat文件的时候atime可能不会改变，所以可能也就不会引起ctime的改变。relatime维护的atime是可以控制的，详见man mount的relatime和redhat官方手册#### 4. 文件/目录的创建和删除##### 4.1 创建目录mkdir mkdir [-mp] 目录名-m：表示创建目录时直接设置权限-p：表示递归创建多层目录，即上层目录不存在时也会直接将其创建出来(parent) [root@xuexi ~]# mkdir /tmp/test1 # 在tmp目录中创建一个test1目录[root@xuexi ~]# mkdir -m 711 /tmp/test2 # 直接创建test2时就赋予权限711[root@xuexi ~]# mkdir -p /tmp/test3/test4/test5 # 创建test5，此时会将不存在的test3和test4目录也创建好 1##### 4.2 创建文件touch touch file_name [root@xuexi ~]# touch /tmp/test1/test1.txt[root@xuexi ~]# touch {1..10} # 创建文件名为1-10的文件 1234567多个&#123;&#125;还可以交换扩展。类似(a+b)(c+d)=ac+ad+bc+bd。[root@xuexi ~]# touch &#123;a,b&#125;_&#123;c,d&#125; # 创建a_c、a_d、b_c、b_d四个文件touch主要是修改文件的时间戳信息，当touch的文件不存在时就自动创建该文件。可以使用 touch –c 来取消创建动作。 touch可以更改最近一次访问时间(atime)，最近一次修改时间(mtime)，文件属性修改时间(ctime)，这些时间可以通过命令stat file来查看。其中ctime是文件属性上的更改，即元数据的更改，比如修改权限。 touch -a修改atime，-m修改mtime，没有修改ctime的选项。因为使用touch改变atime或mtime，同时也都会改变ctime，虽说atime并不总是会影响ctime(如cat文件时)。 -t选项表示使用”[[CC]YY]MMDDhhmm[.ss]”格式的时间替代当前时间。 shell&gt; touch -a -t 201212211212 file # 将file文件的atime修改为2012年12月21号12点12分 -d选项表示使用指定的字符串描述时间格式来替代当前时间，如”3 days ago”,”next Sunday”等很多种格式。 所以，touch命令选项说明如下： -c：强制不创建文件 -a：修改文件access time(atime) -m：修改文件modification time(mtime) -t：使用”[[CC]YY]MMDDhhmm[.ss]”格式的时间替代当前时间 -d：使用字符串描述的时间格式替代当前时间 1#### 4.3 删除文件/目录 rm [-rfi] file_name -r：表示递归删除，删除目录时需要加此参数 -i：询问是否删除(yes/no) -f：强制删除，不进行询问 [root@xuexi ~]# rm -rf /tmp/test2 123456789删除空目录时还可以使用rmdir。 在删除文件之前，一定一定要确定是否真的删除。最好使用rm -i(默认已经在~/.bashrc中定义了该别名)，除非在脚本中，否则不要轻易使用-f选项。已经有非常多的人不小心rm -rf *和rm -rf /NNNNN了。例如想删除&quot;rm –rf /abc*&quot;，结果习惯性的多敲了一个空格&quot;rm –rf /abc *&quot;，完了。 #### 5. 查看文件类型file命令这是一个简单查看文件类型的命令，查看文件是属于二进制文件还是数据文件还是ASCII文件。 [root@xuexi tmp]# file /etc/aliases.db/etc/aliases.db: Berkeley DB (Hash, version 9, native byte-order) # 数据文件 [root@xuexi tmp]# file ~/.bashrc/root/.bashrc: ASCII text # ASCII文件 [root@xuexi tmp]# file /bin/ls/bin/ls: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, stripped 12除了基本的查看文件类型的功能外，file还有一个&quot;-s&quot;选项，是一个超强力的选项，可以查看设备的文件系统类型。像有些分区工具如parted在分区时是可以指定文件系统的(虽然不建议这么做，CentOS 7的parted版本中已经取消了该功能)，但在分区后格式化前，一般是比较难查看该分区的文件系统类型的，但使用file可以查看到。 [root@server1 ~]# file -s /dev/sda1/dev/sda1: Linux rev 1.0 ext4 filesystem data (needs journal recovery) (extents) (huge files) [root@server1 ~]# file -s /dev/sda/dev/sda: x86 boot sector; GRand Unified Bootloader, stage1 version 0x3, boot drive 0x80, 1st sector stage2 0x7f86, GRUB version 0.94; partition 1: ID=0x83,active, starthead 32, startsector 2048, 512000 sectors; partition 2: ID=0x83, starthead 254, startsector 514048, 37332992 sectors; partition 3: ID=0x82,starthead 254, startsector 37847040, 4096000 sectors, code offset 0x48 12#### 6. 文件/目录复制和移动6.1 cp命令 cp [-apdriulfs] src dest # 复制单文件或单目录 cp [-apdriuslf] src1 src2 src3……dest_dir # 复制多文件、目录到一个目录下 1选项说明： -p： 文件的属性(权限、属组、时间戳)也复制过去。如果不指定p选项，谁执行复制动作，文件所有者和组就是谁。 -r或-R：递归复制，常用于复制非空目录。 -d：复制的源文件如果是链接文件，则复制链接文件而不是指向的文件本身。即保持链接属性，复制快捷方式本身。如果不指定-d，则复制的是链接所指向的文件。 -a：a=pdr三个选项。归档拷贝，常用于备份。 -i：复制时如果目标文件已经存在，询问是否替换。 -u：(update)若目标文件和源文件同名，但属性不一样(如修改时间，大小等)，则覆盖目标文件。 -f：强制复制，如果目标存在，不会进行-i选项的询问和-u选项的考虑，直接覆盖。 -l：在目标位置建立硬链接，而不是复制文件本身。 -s：在目标位置建立软链接，而不是复制文件本身(软链接或符号链接相当于windows的快捷方式)。 12345一般使用cp -a即可，对于目录加上-r选项即可。注意，bash内置命令在进行通配符匹配文件的时候，&quot;*&quot;、&quot;?&quot;、&quot;[]&quot;是无法匹配到以&quot;.&quot;开头的文件的，所以&quot;*&quot;不会匹配隐藏文件。要通配隐藏文件，使用&quot;.&quot;代替上述几种通配元字符即可，它能匹配除了&quot;.&quot;和&quot;..&quot;这两个特殊目录外的所有文件。它并非通配符，而是表示当前目录，显然直接复制目录，是可以将隐藏文件复制走的。例如，复制/etc/skel目录下所有文件包括隐藏文件到/tmp目录下。 cp -a /etc/skel/. /tmp 1如果有重复文件，则即使加上-f选项，也一样会交互式询问。解决方法可以是使用&quot;yes&quot;这个工具，它会不断的生成y字母直到进程被杀掉，当然也可以自行指定要生成的字符串。 yes | cp -a /etc/skel/. /tmp 1234567896.2 scp命令和执行过程分析scp是基于ssh的安全拷贝命令(security copy)，它是从古老的远程复制命令rcp改变而来，实现的是在host与host之间的拷贝，可以是本地到远程的、本地到本地的，甚至可以远程到远程复制。注意，scp可能会询问密码。 如果scp拷贝的源文件在目标位置上已经存在时(文件同名)，scp会替换已存在目标文件中的内容，但保持其inode号。 如果scp拷贝的源文件在目标位置上不存在，则会在目标位置上创建一个空文件，然后将源文件中的内容填充进去。 之所以解释上面的两句，是为了理解scp的机制，scp拷贝本质是只是填充内容的过程，它不会去修改目标文件的很多属性，对于从远程复制到另一远程时，其机制见后文。 scp [-12BCpqrv] [-l limit] [-o ssh_option] [-P port] [[user@]host1:]file1 … [[user@]host2:]file2 选项说明： -1：使用ssh v1版本，这是默认使用协议版本 -2：使用ssh v2版本 -C：拷贝时先压缩，节省带宽 -l limit：限制拷贝速度，Kbit/s. -o ssh_option：指定ssh连接时的特殊选项，一般用不上。偶尔在连接过程中等待提示输入密码较慢时，可以设置GSSAPIAuthentication为no -P port：指定目标主机上ssh端口，大写的字母P，默认是22端口 -p：拷贝时保持源文件的mtime,atime,owner,group,privileges -r：递归拷贝，用于拷贝目录。注意，scp拷贝遇到链接文件时，会拷贝链接的源文件内容填充到目标文件中(scp的本质就是填充而非拷贝) -v：输出详细信息，可以用来调试或查看scp的详细过程，分析scp的机制 1示例： 1.把本地文件/home/a.tar.tz拷贝到远程服务器192.168.0.2上的/home/tmp，连接时使用远程的root用户： scp /home/a.tar.tz root@192.168.0.2:/home/tmp/ 2.目标主机不写路径时，表示拷贝到对方的家目录下： scp /home/a.tar.tz root@192.168.0.2 3.把远程文件/home/a.tar.gz拷贝到本机： scp root@192.168.0.2:/home/a.tar.tz # 不接本地目录表示拷贝到当前目录 scp root@192.168.0.2:/home/a.tar.tz /tmp # 拷贝到本地/tmp目录下 4.拷贝远程机器的/home/目录到本地/tmp目录下。 scp -r root@192.168.0.2:/home/ /tmp 5.从远程主机192.168.100.60拷贝文件到另一台远程主机192.168.100.62上。 scp root@192.168.100.60:/tmp/copy.txt root@192.168.100.62:/tmp 1在远程复制到远程的过程中，例如在本地执行scp命令将A主机(192.168.100.60)上的/tmp/copy.txt复制到B主机(192.168.100.62)上的/tmp目录下，如果使用-v选项查看调试信息的话，会发现它的步骤类似是这样的。 以下是从结果中提取的过程首先输出本地要执行的命令Executing: /usr/bin/ssh -v -x -oClearAllForwardings yes -t -l root 192.168.100.60 scp -v /tmp/copy.txt root@192.168.100.62:/tmp 从本地连接到A主机debug1: Connecting to 192.168.100.60 [192.168.100.60] port 22. debug1: Connection established. 要求验证本地和A主机之间的连接debug1: Next authentication method: password root@192.168.100.60‘s password: 将scp命令行修改后发送到A主机上debug1: Sending command: scp -v /tmp/copy.txt root@192.168.100.62:/tmp 在A主机上执行scp命令Executing: program /usr/bin/ssh host 192.168.100.62, user root, command scp -v -t /tmp 验证A主机和B主机之间的连接debug1: Next authentication method: password root@192.168.100.62‘s password: 从A主机上拷贝源文件到最终的B主机上debug1: Sending command: scp -v -t /tmp Sending file modes: C0770 24 copy.txt Sink: C0770 24 copy.txt copy.txt 100% 24 0.0KB/s 关闭本地主机和A主机的连接Connection to 192.168.100.60 closed. 12345也就是说，远程主机A到远程主机B的复制，实际上是将scp命令行从本地传递到主机A上，由A自己去执行scp命令。也就是说，本地主机不会和主机B有任何交互行为，本地主机就像是一个代理执行者一样，只是帮助传送scp命令行以及帮助显示信息。其实从本地主机和主机A上的~/.ssh/know_hosts文件中可以看出，本地主机只是添加了主机A的信息，并没有添加主机B的信息，而在主机A上则添加了主机B的信息。#### 6.3 mv命令mv命令移动文件和目录，还可以用于重命名文件或目录。 mv [-iuf] src dest # 移动单个文件或目录 mv [-iuf] src1 src2 src3 dest_dir # 移动多个文件或目录 选项说明： –backup[=CONTROL]：如果目标文件已存在，则对该文件做一个备份，默认备份文件是在文件名后加上波浪线，如/b.txt~ -b：类似于–backup，但不接受参数, 默认备份文件是在文件名后加上波浪线，如/b.txt~ -f：如果目标文件已存在，则强制覆盖文件 -i：如果目标文件已存在，则提示是否要覆盖，这是alias mv的默认选项 -n：如果目标文件已存在，则不覆盖已存在的文件 如果同时指定了-f/-i/-n，则后指定的生效-u：(update)如果源文件和目标文件不同，则移动，否则不移动 12345mv默认已经是递归移动,不需要-r参数。#### 6.4 mv的一个经典问题(mv的本质)该问题涉及文件系统操作文件的机制，若不理解，请先深入学习文件系统。mv不能实现里层同名目录覆盖外层同名目录。如/tmp下有a目录，a目录里还有a目录，将不能实现/tmp/a/a移动到/tmp。 [root@toystory tmp]# tree -L 3 a -fC a└── a/a├── a/a/a2 directories, 1 file [root@toystory tmp]# mv a/* .mv: overwrite ./a&#39;? y mv: cannot movea/a’ to `./a’: Directory not empty [root@toystory tmp]# mv -f /tmp/a/* /tmpmv: cannot move /tmp/a/a&#39; to/tmp/a’: Directory not empty 123456789101112131415161718192021222324复制代码要解释为何会如此，先说明移动和覆盖动作的本质。同文件系统下移动文件实际上是修改目标文件所在目录的data block，向其中添加一行指向inode table中待移动文件的inode的指针，如果目标路径下有同名文件，则会提示是否覆盖，实际上是覆盖指向该同名文件的inode指针，由于同名文件的inode记录指针被覆盖，就无法再找到该文件的data block，所以该文件被标记为删除。跨文件系统移动文件的本质：如果目标路径下没有同名文件，则先为此文件分配一个inode号，并在目标目录的data block中添加一条指向该inode号的新记录(是全新的)，然后将文件复制到目标位置，复制成功则删除源文件，复制失败则保留源文件；如果目标路径下有同名文件，则提示是否要覆盖，如果选择覆盖，则将该同名文件的inode指针指向新分配的inode号，然后将文件复制到目标位置，复制成功则删除源文件，复制失败则保留源文件。也就是说，同文件系统下移动文件时，inode记录不变(如inode号)，当然，时间戳是一定会改变的，因为移动过程中修改了inode指向data block的指针。而跨文件系统下移动文件时，inode记录完全改变，它是新添加的记录。再考虑上面的问题，同文件系统下移动文件时先在目标位置/tmp的data block中添加一条记录，如果同名则提示覆盖，覆盖时会先删除/tmp的data block中的a对应的记录，再添加将要移动文件的记录。从上面的结果也可以看出是先提示覆盖再提示目录非空的错误。设想下，如果/tmp/a/a移动到/tmp下并重命名为b，则其动作是直接向/tmp的data block中添加b的记录，如果此时正好/tmp下已有b目录，则先删除/tmp的data block中b目录对应的记录，再添加移动后的b记录。但是现在不是重命名为b，而是覆盖/tmp/a，此时的动作按原理应该是先提示是否覆盖，如果是，则删除/tmp的data block中a对应的记录，但由于此时/tmp/a目录中还有文件，该记录无法删除(因为如果要删除了该记录，代表删除了/tmp/a整个目录，而删除整个/tmp/a目录需要删除里面所有的文件，在删除它们之前的一个动作是把/tmp/a中的所有目录和文件的inode号标记为未使用，但此刻要移动的源目录/tmp/a/a是在使用当中的)，所以提示目录非空而无法删除，这里所指的非空目录指的是/tmp/a，而非是/tmp/a/a非空。但是在Windows操作系统下，里层目录是可以直接覆盖外层同名目录的，这和文件系统的行为有关。#### 7. 查看文件内容7.1 cat命令输出一个或多个文件的内容。cat [OPTION]... [FILE]...选项说明 -n：显示所有行的行号 -b：显示非空行的行号 -E：在每行行尾加上$符号 -T：将TAB符号输出为”^I” -s：压缩连续空行为单个空行 123cat还有一个重要功能，允许将分行键入的内容输入到一个文件中去。首先测试&lt;&lt;eof，这表示将键入的内容追加到标准输入stdin中(不是从标准输入中读取)， eof可以随便使用其他符号代替。 [root@xuexi tmp]# cat &lt;&lt;eof abc.comeofabc.com 1再测试&lt;eof，发现没有输入的机会，并且此时只能使用eof作为符号，EOF或其他任何都不可以。因为&lt;eof是读取标准输入，会将eof当成输入文件处理。所以一定要使用&lt;&lt;eof，这表示here document，而两个eof正是document的起始和结束标志。 [root@xuexi tmp]# cat &lt;eof [root@xuexi tmp]# cat &lt;eox-bash: eox: No such file or directory [root@xuexi tmp]# cat &lt;EOF-bash: EOF: No such file or directory 123再进一步测试&lt;&lt;eof的功能，将键入的内容重定向到文件而非标准输入中。这时有两种书写方案：第一种方案：&gt;&gt;filename&lt;&lt;eof或&gt;filename&lt;&lt;eof [root@xuexi ~]# cat &gt;&gt;/tmp/test.txt&lt;&lt;EOF # 输入到这里按回车键继续输入下一行 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx # 按回车输入下一行yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy # 按回车输入下一行zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz # 按回车输入下一行EOF # 顶格写EOF结束输入 1第二种方案：&lt;&lt;eof&gt;filename或&lt;&lt;eof&gt;&gt;filename [root@xuexi tmp]# cat &lt;log.txt abc.comeof 12345两种方案结果是一样的，且总是使用&lt;&lt;eof，只不过所写的位置不同而已，不管写在哪个位置，它都表示将键入的内容追加到标准输入。然后再使用&gt;filename或&gt;&gt;filename控制重定向的方式，将标准输入中的内容重定向到filename文件中。7.2 tactac和cat字母正好是相反的，其作用也是和cat相反的，它会反向输出行，将最后一行放在第一行的位置输出，依此类推。但是，tac没有显示行号的参数。 shell&gt; echo -e ‘1\n2\n3\n4\n5’ | tac54321 123457.3 headhead打印前面的几行。head [-n num] | [-num] [-v] filename -n：显示前num行；如果num是负数，则显示除了最后|num|(绝对值)行的其余所有行，即显示前”总行数 - |num|” -v：会显示出文件名 -n num是显示文件的前num行，num可以是+/-或不加正负号的整数，如果是正整数或不写+号，则显示前num行。如果是负整数，则从后向前数num行，并打印除了这些行的前面所有的行，即打印除了最后num行的所有行，也即总行数减num的前正数行。不写-n时默认是前10行。正整数时”-n num”可以直接简写”-num”。 123不管怎么样，它取的都是前几行，哪怕是负整数也是前几行。示例： [root@xuexi ~]# echo -e ‘1\n2\n3\n4\n5’ | head # 取出默认前10行，但总共才有5行。12345 [root@xuexi ~]# echo -e ‘1\n2\n3\n4\n5’ | head -2 # 取出前2行12 1或者 [root@xuexi ~]# echo -e ‘1\n2\n3\n4\n5’ | head -n 2 # 取出前2行12 [root@xuexi ~]# echo -e ‘1\n2\n3\n4\n5’ | head -n -1 # 取出前5-1=4行1234 1237.4 tailtail和head相反，是显示后面的行，默认是后10行。 tail [OPTION]… [FILE]… 选项说明： -n：输出最后num行，如果使用-n +num则表示输出从第num行开始的所有行 -f：监控文件变化 –pid=PID：和-f一起使用，在给定PID的进程死亡后，终止文件监控 -v：显示文件名 “-n -num”或”-num”或”-n num”(num为正整数)表示输出最后的num行。使用”-n +num”(num为正整数)则表示输出从第num行开始的所有行。 [root@xuexi ~]# echo -e ‘1\n2\n3\n4\n5’ | tail -3 # 等价于 tail -n 3和tail -n -3345 [root@xuexi tmp]# seq 6 | tail -n +3 # 打印除了前3-1=2行的所有行3456 12345678tail还有一个重要的参数-f，监控文件的内容变化。当一个用户不断修改某个文件的尾部，另一个用户就可以通过这个命令来刷新并显示这些修改后的内容。7.5 nl以行号的方式查看内容。常用&quot;-b a&quot;，表示不论是否空行都显示行号，等价于cat -n；不写选项时，默认&quot;-b t&quot;，表示空行不显示行号，等价于cat -b。 [root@xuexi ~]# nl /etc/issue # 默认空行不显示行号1 CentOS release 6.6 (Final)2 Kernel \r on an \m [root@xuexi ~]# nl -b a /etc/issue1 CentOS release 6.6 (Final)2 Kernel \r on an \m3 12345678910111213141516177.6 more和less按页显示文件内容。使用more时，使用/搜索字符串，按下n或N键表示向下或向上继续搜索。使用less时，还多了一个搜索功能，使用?搜索字符串，同样，使用n或N键可以向上或向下继续搜索。7.7 比较文件内容shell&gt; diff file1 file2shell&gt; vimdiff file1 file28. 文件查找类命令搜索文件的路径在何处以及文件的名称为何。8.1 which显示命令或脚本的全路径，默认也会将命令的别名显示出来。 shell&gt; which mv alias mv=’mv -i’ /bin/mv 1238.2 whereis找出二进制文件、源文件和man文档文件。 shell&gt; whereis cd cd: /usr/bin/cd /usr/share/man/man1/cd.1.gz /usr/share/man/man1p/cd.1p.gz /usr/share/man/mann/cd.n.gz 1238.3 whatis列出给定命令(并非一定是命令)的man文档信息。 shell&gt; whatis passwd sslpasswd (1ssl) - compute password hashespasswd (1) - update user’s authentication tokenspasswd (5) - password file 1根据上面的结果，执行： man 1 passwd # 获取passwd命令的man文档 man 5 passwd # 获取password文件的man文档，文件类的man文档说明的是该文件中各配置项意义 man sslpasswd # 获取sslpasswd命令的man文档，实际上是openssl passwd的man文档]]></content>
      <categories>
        <category>linux基础</category>
      </categories>
      <tags>
        <tag>linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[透明代理,正向代理,反向代理区别]]></title>
    <url>%2F2019%2F07%2F26%2F%E9%80%8F%E6%98%8E%E4%BB%A3%E7%90%86-%E6%AD%A3%E5%90%91%E4%BB%A3%E7%90%86-%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[透明代理、正向代理、反向代理区别说明透明代理透明代理这个代理服务器是透明的，透明代理其实也叫做内网代理、拦截代理、以及强制代理。透明代理和正向代理的行为很相似，但细节上有所不同。透明代理将拦截客户端发送的请求，拦截后自己代为访问服务端，获取响应结果后再由透明代理交给客户端。一般公司内的上网行为管理软件就是透明代理。 例如：客户端要访问www.baidu.com，如果是正向代理的方式，客户端会指明它要交给正向代理服务，就像路由中说要交给网关一样。如果是透明代理的方式，则是发送出去，然后被透明代理拦截，客户端以为请求的这个过程是自己完成的，并不知道是透明代理完成的。 正向代理正向代理是转发代理。客户端将请求转发给正向代服务器，正向代理服务器再负责转发给服务端,响应服务端先响应给正向代理服务器，正向代理服务器再转发给对应的客户端。也就是说，正向代理可以但不限于为局域网内客户端做代理，它扮演的角色类似于NAT。如常用的vpn的shadowsocks中配置的代理目标就是正向代理。服务器IP就是正向代理的IP。 正向代理和透明代理的区别正向代理和透明代理的区别，细分起来还是有不少的，但主要几点： 1.正向代理时，客户端明确指明请求要交给正向代理服务，也就是说要设置。而透明代理对客户端是透明的，客户端不知道更不用设置透明代理，但是客户端发出的请求都会被透明代理拦截。 2.正向代理为了实现某些额外的需求，有可能会修改请求报文，担按时RFC文档的要求，透明代理不会修改请求报文。 3.正向代理可以内网也可以外网，但透明代理都是内网。 反向代理反向代理是为服务端转发请求，客户端将请求发送至反向代理服务器，反向代理服务器再将请求转发给真正的服务器以处理请求，响应时后端真正的服务器将处理结果发送给反向代理，再由反向代理构建响应给客户端。]]></content>
      <categories>
        <category>透明代理、正向代理、反向代理区别</category>
      </categories>
      <tags>
        <tag>透明代理、正向代理、反向代理区别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sort详解]]></title>
    <url>%2F2019%2F07%2F25%2Fsort%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[sort详解1.sort的工作原理sort将文件的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较最后将他们升序输出。 12345678910[rocrocket@rocrocket programming]$ cat seq.txtbananaapplepearorange[rocrocket@rocrocket programming]$ sort seq.txtapplebananaorangepear 2. sort的-u选项它的作用很简单，就是在输出行中去除重复行。 1234567891011121314151617[rocrocket@rocrocket programming]$ cat seq.txtbananaapplepearorangepear[rocrocket@rocrocket programming]$ sort seq.txtapplebananaorangepearpear[rocrocket@rocrocket programming]$ sort -u seq.txtapplebananaorangepear pear由于重复被-u选项无情的删除了。 3 sort的-r选项sort默认的排序方式是升序，如果想改成降序，就加个-r就搞定了。 123456789101112131415161718[rocrocket@rocrocket programming]$ cat number.txt13524[rocrocket@rocrocket programming]$ sort number.txt12345[rocrocket@rocrocket programming]$ sort -r number.txt54321 4 sort的-o选项由于sort默认是把结果输出到标准输出，所以需要用重定向才能将结果写入文件，形如sort filename &gt; newfile。 但是，如果你想把排序结果输出到原文件中，用重定向可就不行了。 123[rocrocket@rocrocket programming]$ sort -r number.txt &gt; number.txt[rocrocket@rocrocket programming]$ cat number.txt[rocrocket@rocrocket programming]$ 看，竟然将number清空了。 就在这个时候，-o选项出现了，它成功的解决了这个问题，让你放心的将结果写入原文件。这或许也是-o比重定向的唯一优势所在。 12345678910111213[rocrocket@rocrocket programming]$ cat number.txt13524[rocrocket@rocrocket programming]$ sort -r number.txt -o number.txt[rocrocket@rocrocket programming]$ cat number.txt54321 5 sort的-n选项你有没有遇到过10比2小的情况。我反正遇到过。出现这种情况是由于排序程序将这些数字按字符来排序了，排序程序会先比较1和2，显然1小，所以就将10放在2前面喽。这也是sort的一贯作风。 我们如果想改变这种现状，就要使用-n选项，来告诉sort，“要以数值来排序”！ 123456789101112131415161718192021[rocrocket@rocrocket programming]$ cat number.txt110191125[rocrocket@rocrocket programming]$ sort number.txt110111925[rocrocket@rocrocket programming]$ sort -n number.txt125101119 6 sort的-t选项和-k选项如果有一个文件的内容是这样： 12345[rocrocket@rocrocket programming]$ cat facebook.txtbanana:30:5.5apple:10:2.5pear:90:2.3orange:20:3.4 这个文件有三列，列与列之间用冒号隔开了，第一列表示水果类型，第二列表示水果数量，第三列表示水果价格。 那么我想以水果数量来排序，也就是以第二列来排序，如何利用sort实现？ 幸好，sort提供了-t选项，后面可以设定间隔符。（是不是想起了cut和paste的-d选项，共鸣～～） 指定了间隔符之后，就可以用-k来指定列数了。 12345[rocrocket@rocrocket programming]$ sort -n -k 2 -t : facebook.txtapple:10:2.5orange:20:3.4banana:30:5.5pear:90:2.3 我们使用冒号作为间隔符，并针对第二列来进行数值升序排序，结果很令人满意。 7 其他的sort常用选项-f会将小写字母都转换为大写字母来进行比较，亦即忽略大小写 -c会检查文件是否已排好序，如果乱序，则输出第一个乱序的行的相关信息，最后返回1 -C会检查文件是否已排好序，如果乱序，不输出内容，仅返回1 -M会以月份来排序，比如JAN小于FEB等等 -b会忽略每一行前面的所有空白部分，从第一个可见字符开始比较。 有时候学习脚本，你会发现sort命令后面跟了一堆类似-k1,2，或者-k1.2,-k3.4的东东，有些匪夷所思。今天，我们就来搞定它—-k选项！ 1 准备素材12345$ cat facebook.txtgoogle 110 5000baidu 100 5000guge 50 3000sohu 100 4500 第一个域是公司名称，第二个域是公司人数，第三个域是员工平均工资。 2 我想让这个文件按公司的字母顺序排序，也就是按第一个域进行排序：（这个facebook.txt文件有三个域）12345$ sort -t ‘ ‘ -k 1 facebook.txtbaidu 100 5000google 110 5000guge 50 3000sohu 100 4500 看到了吧，就直接用-k 1设定就可以了。（其实此处并不严格，稍后你就会知道） 3 我想让facebook.txt按照公司人数排序12345$ sort -n -t ‘ ‘ -k 2 facebook.txtguge 50 3000baidu 100 5000sohu 100 4500google 110 5000 不用解释，我相信你能懂。 但是，此处出现了问题，那就是baidu和sohu的公司人数相同，都是100人，这个时候怎么办呢？按照默认规矩，是从第一个域开始进行升序排序，因此baidu排在了sohu前面。 4 我想让facebook.txt按照公司人数排序 ，人数相同的按照员工平均工资升序排序：12345$ sort -n -t ‘ ‘ -k 2 -k 3 facebook.txtguge 50 3000sohu 100 4500baidu 100 5000google 110 5000 看，我们加了一个-k2 -k3就解决了问题。对滴，sort支持这种设定，就是说设定域排序的优先级，先以第2个域进行排序，如果相同，再以第3个域进行排序。（如果你愿意，可以一直这么写下去，设定很多个排序优先级） 5 我想让facebook.txt按照员工工资降序排序，如果员工人数相同的，则按照公司人数升序排序：（这个有点难度喽）12345$ sort -n -t ‘ ‘ -k 3r -k 2 facebook.txtbaidu 100 5000google 110 5000sohu 100 4500guge 50 3000 此处有使用了一些小技巧，你仔细看看，在-k 3后面偷偷加上了一个小写字母r。你想想，再结合我们上一篇文章，能得到答案么？揭晓：r和-r选项的作用是一样的，就是表示逆序。因为sort默认是按照升序排序的，所以此处需要加上r表示第三个域（员工平均工资）是按照降序排序。此处你还可以加上n，就表示对这个域进行排序时，要按照数值大小进行排序，举个例子吧： 12345$ sort -t ‘ ‘ -k 3nr -k 2n facebook.txtbaidu 100 5000google 110 5000sohu 100 4500guge 50 3000 看，我们去掉了最前面的-n选项，而是将它加入到了每一个-k选项中了 6 -k选项的具体语法格式要继续往下深入的话，就不得不来点理论知识。你需要了解-k选项的语法格式，如下： 1[ FStart [ .CStart ] ] [ Modifier ] [ , [ FEnd [ .CEnd ] ][ Modifier ] ] 这个语法格式可以被其中的逗号（“，”）分为两大部分，Start部分和End部分。 先给你灌输一个思想，那就是“如果不设定End部分，那么就认为End被设定为行尾”。这个概念很重要的，但往往你不会重视它。 Start部分也由三部分组成，其中的Modifier部分就是我们之前说过的类似n和r的选项部分。我们重点说说Start部分的FStart和C.Start。 C.Start也是可以省略的，省略的话就表示从本域的开头部分开始。之前例子中的-k 2和-k 3就是省略了C.Start的例子喽。 FStart.CStart，其中FStart就是表示使用的域，而CStart则表示在FStart域中从第几个字符开始算“排序首字符”。 同理，在End部分中，你可以设定FEnd.CEnd，如果你省略.CEnd，则表示结尾到“域尾”，即本域的最后一个字符。或者，如果你将CEnd设定为0(零)，也是表示结尾到“域尾”。 7 突发奇想，从公司英文名称的第二个字母开始进行排序：12345$ sort -t ‘ ‘ -k 1.2 facebook.txtbaidu 100 5000sohu 100 4500google 110 5000guge 50 3000 看，我们使用了-k 1.2，这就表示对第一个域的第二个字符开始到本域的最后一个字符为止的字符串进行排序。你会发现baidu因为第二个字母是a而名列榜首。sohu和 google第二个字符都是o，但sohu的h在google的o前面，所以两者分别排在第二和第三。guge只能屈居第四了。 8 又突发奇想，，只针对公司英文名称的第二个字母进行排序，如果相同的按照员工工资进行降序排序：12345$ sort -t ‘ ‘ -k 1.2,1.2 -k 3,3nr facebook.txtbaidu 100 5000google 110 5000sohu 100 4500guge 50 3000 由于只对第二个字母进行排序，所以我们使用了-k 1.2,1.2的表示方式，表示我们“只”对第二个字母进行排序。（如果你问“我使用-k 1.2怎么不行？”，当然不行，因为你省略了End部分，这就意味着你将对从第二个字母起到本域最后一个字符为止的字符串进行排序）。对于员工工资进行排 序，我们也使用了-k 3,3，这是最准确的表述，表示我们“只”对本域进行排序，因为如果你省略了后面的3，就变成了我们“对第3个域开始到最后一个域位置的内容进行排序” 了。 9 在modifier部分还可以用到哪些选项？可以用到b、d、f、i、n 或 r。 其中n和r你肯定已经很熟悉了。 b表示忽略本域的签到空白符号。 d表示对本域按照字典顺序排序（即，只考虑空白和字母）。 f表示对本域忽略大小写进行排序。 i表示忽略“不可打印字符”，只针对可打印字符进行排序。（有些ASCII就是不可打印字符，比如\a是报警，\b是退格，\n是换行，\r是回车等等） 10 思考思考关于-k和-u联合使用的例子：12345$ cat facebook.txtgoogle 110 5000baidu 100 5000guge 50 3000sohu 100 4500 这是最原始的facebook.txt文件。 12345678910$ sort -n -k 2 facebook.txtguge 50 3000baidu 100 5000sohu 100 4500google 110 5000$ sort -n -k 2 -u facebook.txtguge 50 3000baidu 100 5000google 110 5000 当设定以公司员工域进行数值排序，然后加-u后，sohu一行就被删除了！原来-u只识别用-k设定的域，发现相同，就将后续相同的行都删除。 12345678910$ sort -k 1 -u facebook.txtbaidu 100 5000google 110 5000guge 50 3000sohu 100 4500$ sort -k 1.1,1.1 -u facebook.txtbaidu 100 5000google 110 5000sohu 100 4500 这个例子也同理，开头字符是g的guge就没有幸免于难。 12345$ sort -n -k 2 -k 3 -u facebook.txtguge 50 3000sohu 100 4500baidu 100 5000google 110 5000 咦！这里设置了两层排序优先级的情况下，使用-u就没有删除任何行。原来-u是会权衡所有-k选项，将都相同的才会删除，只要其中有一级不同都不会轻易删除的:)（不信，你可以自己加一行sina 100 4500试试看）12 有时候在sort命令后会看到+1 -2这些符号，这是什么东东？ 12.关于这种语法，最新的sort是这么进行解释的：On older systems, sort’ supports an obsolete origin-zero syntax+POS1 [-POS2]‘ for specifying sort keys. POSIX 1003.1-2001 (*note Standards conformance::) does not allow this; use `-k’ instead. 原来，这种古老的表示方式已经被淘汰了，以后可以理直气壮的鄙视使用这种表示方法的脚本喽！ （为了防止古老脚本的存在，在这再说一下这种表示方法，加号表示Start部分，减号表示End部分。最最重要的一点是，这种方式方法是从0开始计数的，以前所说的第一个域，在此被表示为第0个域。以前的第2个字符，在此表示为第1个字符。明白？）]]></content>
      <categories>
        <category>linux基础</category>
      </categories>
      <tags>
        <tag>linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式符号]]></title>
    <url>%2F2019%2F07%2F25%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%AC%A6%E5%8F%B7%2F</url>
    <content type="text"><![CDATA[正则表达式正则表达式又称规则表达式，通常被用来检索、替换那些符合某个模式(规则)的文本。使用正则表达式可以让我们表达出某种规则，和我们的想法。正则就是规则，正则表达式就是能够让我们表达出自己想法的规则，只要学会了这种规则，我们就能够表达自己的想法。 位置匹配1234567^:表示锚定行首，此字符后面的任意内容必须出现在行首，才能匹配。$:表示锚定行尾，此字符前面的任意内容必须出现在行尾，才能匹配。^$:表示匹配空行，这里所描述的空行表示回车，而空格或tab等都不能算作此处所描述的空行。^abc$:表示abc独占一行时，会被匹配到。\&lt;或者\b:匹配单词边界，表示锚定词首，其后面的字符必须作为单词首部出现。\&gt;或者\b:匹配单词边界，表示锚定词尾，其前面的字符必须作为单词尾部出现。\B:匹配非单词边界，与\b正好相反。 连续次数的匹配123456789*:表示前面的字符连续出现任意次，包括0次。.:表示任意单个字符.*:表示任意长度的任意字符，与通配符中的*的意思相同。\?:表示匹配其前面的字符0或1次。\+:表示匹配其前面的字符至少1次，或者连续多次，连续次数上不封顶。\&#123;n\&#125;:表示前面的字符连续出现n次，将会被匹配到。如果字符连续出现的次数大于指定的次，也是可以匹配到的。\&#123;x,y\&#125;:表示之前的字符至少连续出现x次,最多连续出现y次，都能被匹配到，换句话说，只要之前的字符连续出现的次数在x与y之间，即可被匹配到。\&#123;,n\&#125;:表示之前的字符连续出现至多n次，最少0次，都会被匹配到。\&#123;n,\&#125;表示之前的字符连续出现至少n次，才会被匹配到。 常用符号123456789101112131415161718192021222324252627. :表示匹配任意单个字符* 表示匹配前面的字符任意次，包括0次[]:表示匹配指定范围内的任意单个字符[^ ]:表示匹配指定范围外的任意单个字符[[:alpha:]]：表示任意大小写字母[[:lower:]]表示任意小写字母[[:digit:]]表示0到9之间的任意单个数字（包括0和9）[[:upper:]]表示任意大写字母[[:alnum:]]表示任意数字或字母[[:space:]]表示任意空白字符，包括空格、tab键等。[[:punct:]]表示任意标点符号[0-9]与[[:digit:]]等效[a-z]与[[:lower:]]等效[A-Z]与[[:upper:]]等效[a-zA-Z]与[[:alpha:]]等效[a-zA-Z0-9]与[[:alnum:]]等效[^0-9]与[[:digit:]]等效#简短格式并非所有正则表达式解析器都可以识别\d:表示任意单个0到9的数字\D:表示任意单个非数字字符\t:表示匹配单个横向制表符(相当于一个tab键)\s:表示匹配单个空白字符，包括空格、tab制表符等。\S:表示匹配单个非空白字符\w:任意一个字母或数字或下划线，也就是A~Z,a~z,0~9,_ 中任意一个 分组与后向引用1234\(\)：表示分组，我们可以将其中的内容当做一个整体，分组可以嵌套，规则是从左向右的，第1个分组是最左边的括号\(ab\):表示将ab当做一个整体去处理。\1：表示引用整个表达式中第1个分组中的正则匹配到的结果。\2：表示引用整个表达式中第2个分组中的正则匹配到的结果，依次类推。 转义符\与正则中的符号结合在一起时，就表示这个符号本身的含义。如果是?和+只需要把前面\去掉就表示本身 扩展正则表达式扩展正则表达式中多了一个|表示或者其他不同的?,+,{n},{x,y},{,n},{n,},()，(ab)去掉了反斜线就是扩展正则表达式 grep默认不支持扩展正则表达式，要加上-E来支持扩展正则表达式。]]></content>
      <categories>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shadowsocks搭建与一键安装脚本]]></title>
    <url>%2F2019%2F07%2F24%2Fshadowsocks%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[centos搭建shadowsocks步骤安装pip(pip是python的包管理工具)执行如下命令: 123curl "https://bootstrap.pypa.io/get-pip.py" -o "get-pip.py"python get-pip.py 安装配置shadowsocks12pip install --upgrade pippip install shadowsocks 安装完成之后创建shadowsocks配置文件/etc/shadowsocks.json： 1234567891011&#123; &quot;server&quot;: &quot;0.0.0.0&quot;, &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;: 1080, &quot;port_password&quot;: &#123; &quot;8080&quot;: &quot;填写密码&quot;, &quot;8081&quot;: &quot;填写密码&quot; &#125;, &quot;timeout&quot;: 600, &quot;method&quot;: &quot;aes-256-cfb&quot;&#125; 说明： method为加密方法，可选aes-128-cfb, aes-192-cfb, aes-256-cfb, bf-cfb, cast5-cfb, des-cfb, rc4-md5, chacha20, salsa20, rc4, table port_password为端口对应的密码，可使用密码生成工具生成一个随机密码 如果你不需要配置多个端口的话，仅配置单个端口，则可以使用以下配置： 123456&#123; &quot;server&quot;: &quot;0.0.0.0&quot;, &quot;server_port&quot;: 8080, &quot;password&quot;: &quot;填写密码&quot;, &quot;method&quot;: &quot;aes-256-cfb&quot;&#125; 说明： server_port为服务监听端口 password为密码 配置自启动123456789101112131415vim /etc/systemd/system/shadowsocks.service[Unit]Description=Shadowsocks[Service]TimeoutStartSec=0ExecStart=/usr/bin/ssserver -c /etc/shadowsocks.json[Install]WantedBy=multi-user.targetsystemctl enable shadowsockssystemctl start shadowsockssystemctl status shadowsocks -l配置完成之后确认服务启动成功后，配置防火墙规则，开放配置的端口。 一键搭建脚本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#!/bin/bash# Install Shadowsocks on CentOS 7echo &quot;Installing Shadowsocks...&quot;random-string()&#123; cat /dev/urandom | tr -dc &apos;a-zA-Z0-9&apos; | fold -w $&#123;1:-32&#125; | head -n 1&#125;CONFIG_FILE=/etc/shadowsocks.jsonSERVICE_FILE=/etc/systemd/system/shadowsocks.serviceSS_PASSWORD=$(random-string 32)SS_PORT=8388SS_METHOD=aes-256-cfbSS_IP=`ip route get 1 | awk &apos;&#123;print $NF;exit&#125;&apos;`GET_PIP_FILE=/tmp/get-pip.py# install pipcurl &quot;https://bootstrap.pypa.io/get-pip.py&quot; -o &quot;$&#123;GET_PIP_FILE&#125;&quot;python $&#123;GET_PIP_FILE&#125;# install shadowsockspip install --upgrade pippip install shadowsocks# create shadowsocls configcat &lt;&lt;EOF | sudo tee $&#123;CONFIG_FILE&#125;&#123; &quot;server&quot;: &quot;0.0.0.0&quot;, &quot;server_port&quot;: $&#123;SS_PORT&#125;, &quot;password&quot;: &quot;$&#123;SS_PASSWORD&#125;&quot;, &quot;method&quot;: &quot;$&#123;SS_METHOD&#125;&quot;&#125;EOF# create servicecat &lt;&lt;EOF | sudo tee $&#123;SERVICE_FILE&#125;[Unit]Description=Shadowsocks[Service]TimeoutStartSec=0ExecStart=/usr/bin/ssserver -c $&#123;CONFIG_FILE&#125;[Install]WantedBy=multi-user.targetEOF# start servicesystemctl enable shadowsockssystemctl start shadowsocks# view service statussleep 5systemctl status shadowsocks -lecho &quot;================================&quot;echo &quot;&quot;echo &quot;Congratulations! Shadowsocks has been installed on your system.&quot;echo &quot;You shadowsocks connection info:&quot;echo &quot;--------------------------------&quot;echo &quot;server: $&#123;SS_IP&#125;&quot;echo &quot;server_port: $&#123;SS_PORT&#125;&quot;echo &quot;password: $&#123;SS_PASSWORD&#125;&quot;echo &quot;method: $&#123;SS_METHOD&#125;&quot;echo &quot;--------------------------------&quot; Windows客户端下载地址： https://github.com/shadowsocks/shadowsocks-windows/releases Mac客户端下载地址： https://github.com/shadowsocks/ShadowsocksX-NG/releases Android客户端下载地址： https://github.com/shadowsocks/shadowsocks-android/releases 参考https://blog.51cto.com/zero01/2064660]]></content>
      <categories>
        <category>shadowsocks</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[find命令详解]]></title>
    <url>%2F2019%2F07%2F24%2Ffind%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[find详解find目的：查找符合条件的文件 1格式：find 目录名 选项 查找条件 find是我们很常用的一个Linux命令，但是我们一般查找出来的并不仅仅是看看而已，还会有进一步的操作，这个时候exec的作用就显现出来了。 exec解释：-exec参数后面跟的是command命令，它的终止是以;为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会有不同的意义，所以前面加反斜杠。 {} 花括号代表前面find查找出来的文件名。 使用find时，只要把想要的操作写在一个文件里，就可以用exec来配合find查找，很方便的。在有些操作系统中只允许-exec选项执行诸如ls或ls l这样的命令。大多数用户使用这一选项是为了查找旧文件并删除它们。建议在真正执行rm命令删除文件之前，最好先用ls命令看一下，确认它们是所要删除的文件。 exec选项后面跟随着所要执行的命令或脚本，然后是一对儿{}，一个空格和一个\，最后是一个分号。为了使用exec选项，必须要同时使用print选项。如果验证一下find命令，会发现该命令只输出从当前路径起的相对路径及文件名。 find工具一、工作特点优点 实时查找 准确查找，遍历整个目录下的所有文件 可以对查询到的文件进行指定动作，即查看，删除，移动等操作. 缺点 查询速度略慢 查询条件查询条件由选项及测试条件组成：【测试条件】一、以文件名查找1231.-name pattern：以文件名查找2.-name pattern: 不区分文件名的大小写，只支持glob风格的查找文件：*,?,[],[^]3.-regex pattern:基于正则表达式查找文件，精确匹配文件名 二、以文件从属关系查找1234561.-user USERNAME:以用户名查找2.-group GROUPNAME：以组名查找3.-uid UID：以UID查找4.-gid GID：以GID查找5.-nouser：查找没有属主的文件6.-nogroup:查找没有属组的文件 三、以文件类型查找123456781.-type TYPE：以文件类型查找f : 普通文件d : 目录文件b : 块设备文件c : 字符设备文件l : 连接文件s : 套接字文件p : 管道文件 四、根据文件大小查找1234-size [+|-]SIZE : 以文件大小查询,大小包含K，M，G的单位-size 5M ： 精确查找大小为5M的文件，大小上面浮动稍微有偏差-size -5M : 查询大小小于5M的文件-size +5M : 查询大小大于5M的文件 五、根据时间查找1234567891011-atime [+|-]TIME : 以访问时间（天）查找-mtime [+|-]TIME : 以数据修改时间（天）查找-ctime [+|-]TIME : 以元数据修改时间（天）查找-amin [+|-]TIME : 以访问时间（分钟）查找-mmin [+|-]TIME : 以数据修改时间（分钟）查找-cmin [+|-]TIME : 以元数据修改时间（分钟）查找-newer FILE : 以FILE文件为条件，判断比它新的文件准确时间，7表示刚好7（天|分钟）起始位置+7: 7(天|分钟)以前的-7: 7(天|分钟)以内的 六、根据权限查找123456789-perm MODE : 精确权限查找find . -perm 644-perm /MODE : 任何一类用户(u,g,o)中的任何一位(r,w,x)符合条件即满足，理解为或关系find . -perm /222 : 查找至少有一个类用户有写权限find . -perm /666 : 查找至少有一个类用户有读写权限find . -perm /001 :查找其他用户有执行权限-perm -MODE : 每一类用户(u,g,o)的权限中的每一位(r,w,x)同时符合条件即满足，理解为与关系find . -perm -222 : 查找三类用户都有写权限find . -not -perm -222 :至少有一类用户没有写权限 七、 组合条件测试123456789与 : -a默认组合逻辑，可以加-a，也可以取消，例：find . -type f -a -user mariadb，两个条件同时满足或 : -o可以加-o参数，例：find . -type f -o -nouser ,两个条件只满足一个即可非 : -not 或者 !表示find . -not -type f,不是普通文件find . ! -type f ,同上 八、处理动作1234567891011-print : 默认为打印，不需要添加，输出入屏幕-ls : 以ls长文件的格式形式输出-delete : 删除查找到的文件-fls /PATH/TO/SOMEFILE :把查询到的文件以ls详细信息格式保存到SOMEFILE文件中-ok COMMAND &#123;&#125; \; : 查找到的文件传递给COMMAND命令，提每步都给用户提示确认操作-exec COMMAND &#123;&#125; \; : 查找到的文件传递给COMMAND命令，直接修改完成，不给用户确认 注意：find将查找到的文件路径一次性传递给后面的命令，但有很多的命令不能接受过长的参数，导致命令的执行失败，使用如下方式可避免此错误的发生： 12find /etc -type f | xargs -i COMMAND ： -i参数是由find的结果传给xargs命令后，由-i指定结果代替符find /etc -type f | xargs -i cp &#123;&#125; /tmp : -i 指定代替符为&#123;&#125; ##选项 一、指定查找目录范围12-maxdepth NUM : 指定最多搜索目录层级到NUM层-mindepth NUM : 指定最少搜索目录NUM层级 二、查找空文件 12-empty : 查询内容为空的文件find ./ -empty : 查找当前目录下的所有空文件 三、排除符号连接 1-follow : 排除符号连接 Find与xagrs的命令结合 1234Usage:find /etc -type f | xargs -i cp &#123;&#125; /tmp/ -i : 由xargs接收的参数，由-i声名由后一个命令&#123;&#125;代替 示例1234567891011121314151617181920212223242526272829303132333435363738394041找出/tmp目录下属主为非root,且文件名包含fstab字符串的文件find /tmp -not -user root -a -name *fstab*找出/tmp目录下文件名中不包含fstab字符串的文件find /tmp -not -name *fstab*找出/tmp目录下属主为非root,而且文件名不包含fstab字符串的文件find /tmp -not -user root -a -not -name *fstab*find /tmp -not \(-user root -o -name *fstab*\)至少有一周没有访问过的文件find /etc -atime +7 -ls24小时内修改过的文件find /etc -mtime -1 -ls查找 /var目录下属主为root，且属组为mail的所有文件或目录find /var/ -user root -group mail查找/usr目录下不属于root,bin,或者hadoop的所有文件或目录，用两种方法find ./ -not -user bin -not -user user3 -not -user user4find ./ -not \( -user bin -o -user user3 -o -user user4 \)查找/etc目录下最近一周其内容修改过， 且属主不是root用户也不是hadoop用户的文件或目录find /etc -mtime -7 -not -user root -not -user hadoop查找当前系统上没有属主或属组，且最近一周内曾被访问过的文件或目录find / -atime -7 -nouser -o -nogroup查找/etc目录下大于1M且类型为普通文件的所有文件find /etc/ -size +1M -type f -ls查找/etc目录下所有用户都没有写权限的文件find /etc/ -type f -not -perm /222 -ls查找/etc目录至少有一类用户没有执行权限的文件find /etc/ -not -perm -111 -ls查找/etc/init.d目录下，所有用户都有执行权限，且其它用户有写权限的所有文件find /etc/ -perm -111 -perm -002find /etc/ -perm -113 摩根定律： ** !A -a !B = !(A -o B)** ** !A -o !B = !(A -a B)**]]></content>
      <categories>
        <category>linux基础</category>
      </categories>
      <tags>
        <tag>linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim技巧]]></title>
    <url>%2F2019%2F07%2F24%2Fvim%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[vim技巧1. vim模式1234正常模式（按Esc或Ctrl+[进入） 左下角显示文件名或为空插入模式（按i进入） 左下角显示--INSERT--可视模式（按v进入） 左下角显示--VISUAL--复制代码 2. 打开文件1234567891011121314151617181920212223242526# 打开单个文件vim file # 同时打开多个文件vim file1 file2.. # 在vim窗口中打开一个新文件:open [file] 【举个例子】# 当前打开1.txt，做了一些编辑没保存:open! 放弃这些修改，并重新打开未修改的文件# 当前打开1.txt，做了一些编辑并保存:open 2.txt 直接退出对1.txt的编辑，直接打开2.txt编辑，省了退出:wq再重新vim 2.txt的步骤# 打开远程文件，比如ftp或者share folder:e ftp://192.168.10.76/abc.txt:e \qadrive\test\1.txt# 以只读形式打开文件，但是仍然可以使用 :wq! 写入vim -R file # 强制性关闭修改功能，无法使用 :wq! 写入vim -M file 复制代码 3. 插入命令123456789i 在当前位置生前插入I 在当前行首插入a 在当前位置后插入A 在当前行尾插入o 在当前行之后插入一行O 在当前行之前插入一行复制代码 4. 查找命令最简单的查找 12345678/text&amp;emsp;&amp;emsp;查找text，按n健查找下一个，按N健查找前一个。?text&amp;emsp;&amp;emsp;查找text，反向查找，按n健查找下一个，按N健查找前一个。vim中有一些特殊字符在查找时需要转义&amp;emsp;&amp;emsp;.*[]^%/?~$:set ignorecase&amp;emsp;&amp;emsp;忽略大小写的查找:set noignorecase&amp;emsp;&amp;emsp;不忽略大小写的查找复制代码 快速查找，不需要手打字符即可查找 123456* 向后（下）寻找游标所在处的单词# 向前（上）寻找游标所在处的单词以上两种查找，n,N 的继续查找命令依然可以适用复制代码 精准查找：匹配单词查找 如果文本中有 hello，helloworld，hellopython 那我使用 /hello ，这三个词都会匹配到。 有没有办法实现精准查找呢？可以使用 12/hello\&gt;复制代码 精准查找：匹配行首、行末 123456# hello位于行首/^hello# world位于行末/world$复制代码 5. 替换命令1234567891011121314151617181920~ 反转游标字母大小写r&lt;字母&gt; 将当前字符替换为所写字母R&lt;字母&gt;&lt;字母&gt;... 连续替换字母cc 替换整行（就是删除当前行，并在下一行插入）cw 替换一个单词（就是删除一个单词，就进入插入模式），前提是游标处于单词第一个字母（可用b定位）C (大写C)替换至行尾（和D有所区别，D是删除（剪切）至行尾，C是删除至行位并进入插入模式）:s/old/new/ 用old替换new，替换当前行的第一个匹配:s/old/new/g 用old替换new，替换当前行的所有匹配:%s/old/new/ 用old替换new，替换所有行的第一个匹配:%s/old/new/g 用old替换new，替换整个文件的所有匹配:10,20 s/^/ /g 在第10行至第20行每行前面加四个空格，用于缩进。ddp 交换光标所在行和其下紧邻的一行。复制代码 6. 撤销与重做123456u 撤销（Undo）U 撤销对整行的操作Ctrl + r 重做（Redo），即撤销的撤销。复制代码 7. 删除命令需要说明的是，vim 其实并没有单纯的删除命令，下面你或许理解为剪切更加准确。 以字符为单位删除 12345678910111213x 删除当前字符3x 删除当前字符3次X 删除当前字符的前一个字符。3X 删除当前光标向前三个字符dl 删除当前字符， dl=xdh 删除前一个字符，X=dhD 删除当前字符至行尾。D=d$d$ 删除当前字符至行尾d^ 删除当前字符之前至行首复制代码 以单词为单位删除 123dw 删除当前字符到单词尾daw 删除当前字符所在单词复制代码 以行为单位删除 12345678910111213141516171819dd 删除当前行dj 删除下一行dk 删除上一行dgg 删除当前行至文档首部d1G 删除当前行至文档首部dG 删除当前行至文档尾部kdgg 删除当前行之前所有行（不包括当前行）jdG 删除当前行之后所有行（不包括当前行）10d 删除当前行开始的10行。:1,10d 删除1-10行:11,$d 删除11行及以后所有的行:1,$d 删除所有行J&amp;emsp;&amp;emsp; 删除两行之间的空行，实际上是合并两行。复制代码 8. 复制粘贴普通模式中使用y复制 1234567891011yy 复制游标所在的整行（3yy表示复制3行）y^ 复制至行首，或y0。不含光标所在处字符。y$ 复制至行尾。含光标所在处字符。yw 复制一个单词。y2w 复制两个单词。yG 复制至文本末。y1G 复制至文本开头。复制代码 普通模式中使用p粘贴 123p(小写)：代表粘贴至光标后（下边，右边）P(大写)：代表粘贴至光标前（上边，左边）复制代码 9. 剪切粘贴12345678910111213dd 其实就是剪切命令，剪切当前行ddp 剪切当前行并粘贴，可实现当前行和下一行调换位置正常模式下按v（逐字）或V（逐行）进入可视模式然后用jklh命令移动即可选择某些行或字符，再按d即可剪切ndd 剪切当前行之后的n行。利用p命令可以对剪切的内容进行粘贴:1,10d 将1-10行剪切。利用p命令可将剪切后的内容进行粘贴。:1, 10 m 20 将第1-10行移动到第20行之后。复制代码 10. 退出保存12345678910111213:wq 保存并退出ZZ 保存并退出:q! 强制退出并忽略所有更改:e! 放弃所有修改，并打开原来文件。ZZ 保存并退出:sav(eas) new.txt 另存为一个新文件，退出原文件的编辑且不会保存:f(ile) new.txt 新开一个文件，并不保存，退出原文件的编辑且不会保存复制代码 11. 移动命令以字符为单位移动 123456789101112h 左移一个字符l 右移一个字符k 上移一个字符j 下移一个字符# 【定位字符】f和Ffx 找到光标后第一个为x的字符3fd 找到光标后第三个为d的字符F 同f，反向查找。复制代码 以行为单位移动 123456789# 10指代所有数字，可任意指定10h 左移10个字符10l 右移10个字符10k 上移10行10j 下移10行$ 移动到行尾 3$ 移动到下面3行的行尾复制代码 以单词为单位移动 12345w 向前移动一个单词（光标停在单词首部）b 向后移动一个单词e，同w，只不过是光标停在单词尾部ge 同b，光标停在单词尾部。复制代码 以句为单位移动 123( 移动到句首) 移动到句尾复制代码 跳转到文件的首尾 123gg 移动到文件头。 = [[ == ``G 移动到文件尾。 = ]]复制代码 其他移动方法 123^ 移动到本行第一个非空白字符上。0 移动到本行第一个字符上(可以是空格)复制代码 使用 具名标记 跳转，个人感觉这个很好用，因为可以跨文件。 1234使用 ma ，可以将此处标记为 a，使用 &apos;a 进行跳转使用 :marks 可以查看所有的标记使用 :delm！可以删除所有的标记复制代码 当在查看错误日志时，正常的步骤是，vim打开文件，然后使用 shift+g 再跳转到最后一行，这里有个更简单的操作可以在打开文件时立即跳到最后一行。只要在 vim 和 文件 中间加个 + 即可。 12vim + you.log复制代码 举一反三，当你想打开文件立即跳转到指定行时，可以这样 123# 打开文件并跳转到 20 行vim you.log +20复制代码 当你使用 / 搜索定位跳转或者使用 :行号 进行精准跳转时，有时我们想返回到上一次的位置，如何实现？ 只要使用 Ctrl+o 即可返回上一次的位置。 12. 排版功能缩进 123456789101112:set shiftwidth? 查看缩进值:set shiftwidth=4 设置缩进值为4# 缩进相关 最好写到配置文件中 ~/.vimrc:set tabstop=4:set softtabstop=4:set shiftwidth=4:set expandtab&gt;&gt; 向右缩进&lt;&lt; 取消缩进复制代码 如何你要对代码进行缩进，还可以用 == 对当前行缩进，如果要对多行对待缩进，则使用 n==，这种方式要求你所编辑的文件的扩展名是被vim所识别的，比如.py文件。 排版 1234:ce 居中:le 靠左:ri 靠右复制代码 13. 注释命令多行注释 1234567进入命令行模式，按ctrl + v进入 visual block模式，然后按j, 或者k选中多行，把需要注释的行标记起来按大写字母I，再插入注释符，例如//按esc键就会全部注释了复制代码 取消多行注释 123456进入命令行模式，按ctrl + v进入 visual block模式，按字母l横向选中列的个数，例如 // 需要选中2列按字母j，或者k选中注释符号按d键就可全部取消注释复制代码 复杂注释 1234567891011:3,5 s/^/#/g 注释第3-5行:3,5 s/^#//g 解除3-5行的注释:1,$ s/^/#/g 注释整个文档:1,$ s/^#//g 取消注释整个文档:%s/^/#/g 注释整个文档，此法更快:%s/^#//g 取消注释整个文档复制代码 14. 调整视野12345678910111213141516171819202122&quot;zz&quot;：命令会把当前行置为屏幕正中央，&quot;zt&quot;：命令会把当前行置于屏幕顶端&quot;zb&quot;：则把当前行置于屏幕底端.Ctrl + e 向下滚动一行Ctrl + y 向上滚动一行Ctrl + d 向下滚动半屏Ctrl + u 向上滚动半屏Ctrl + f 向下滚动一屏Ctrl + b 向上滚动一屏【跳到指定行】：两种方法可以先把行号打开:set nu 打开行号:20 跳到第20行20G 跳到第20行复制代码 15. 区域选择1234567891011121314151617要进行区域选择，要先进入可视模式v 以字符为单位，上下左右选择V 以行为单位，上下选择选择后可进行操作d 剪切/删除y 复制Ctrl+v 如果当前是V(大写)模式，就变成v(小写) 如果当前是v(小写)模式，就变成普通模式。 如果当前是普通模式，就进入v(小写)模式利用这个，可以进行多行缩进。ggVG 选择全文复制代码 16. 窗口控制新建窗口 123456789101112131415161718192021# 打开两个文件分属两个窗口vim -o 1.txt 2.txt# 假设现在已经打开了1.txt:sp 2.txt 开启一个横向的窗口，编辑2.txt:vsp 2.txt 开启一个竖向的窗口，编辑2.txt:split 将当前窗口再复制一个窗口出来，内容同步，游标可以不同:split 2.txt 在新窗口打开2.txt的横向窗口# 需要注意：内容同步，但是游标位置是独立的Ctrl-w s 将当前窗口分成水平窗口Ctrl-w v 将当前窗口分成竖直窗口Ctrl-w q 等同:q 结束分割出来的视窗。Ctrl-w q! 等同:q! 结束分割出来的视窗。Ctrl-w o 打开一个视窗并且隐藏之前的所有视窗复制代码 窗口切换 12345678910111213141516171819# 特别说明：Ctrl w &lt;字母&gt; 不需要同时按Ctrl-w h 切换到左边窗口Ctrl-w l 切换到右边窗口Ctrl-w j 切换到下边窗口Ctrl-w k 切换到上边窗口# 特别说明：全屏模式下:n 切换下一个窗口:N 切换上一个窗口:bp 切换上一个窗口# 特别说明：非全屏模式:bn 切换下一个窗口，就当前位置的窗口的内容变了，其他窗口不变:bN 切换上一个窗口，就当前位置的窗口的内容变了，其他窗口不变复制代码 窗口移动 12345678910# 特别说明：Ctrl w &lt;字母&gt; 不需要同时按Ctrl-w J 将当前视窗移至最下面Ctrl-w K 将当前视窗移最上面Ctrl-w H 将当前视窗移至最左边Ctrl-w L 将当前视窗移至最右边Ctrl-ww 按顺序切换窗口复制代码 调整尺寸 12345# 友情提示：键盘切记不要处于中文状态Ctrl-w + 增加窗口高度Ctrl-w - 减少窗口高度复制代码 退出窗口 12345678910111213141516171819202122:close 关闭当前窗口:close! 强制关闭当前窗口:q 退出，不保存:q! 强制退出，不保存:x 保存退出:wq 保存退出:wq! 强制保存退出:w &lt;[路径/]文件名&gt; 另存为:savesa &lt;[路径/]文件名&gt; 另存为ZZ 保存并退出。:only 关闭所有窗口，只保留当前窗口(前提：其他窗口内容有改变的话都要先保存):only! 关闭所有窗口，只保留当前窗口:qall 放弃所有操作并退出:wall 保存所有，:wqall 保存所有并退出。复制代码 17. 文档加密1234567vim -x file_name然后输入密码：确认密码：如果不修改内容也要保存。:wq，不然密码设定不会生效。复制代码 18. 录制宏按q键加任意字母开始录制，再按q键结束录制（这意味着vim中的宏不可嵌套），使用的时候@加宏名，比如qa。。。q录制名为a的宏，@a使用这个宏。 19. 执行命令123456789101112131415# 重复前一次命令. # 执行shell命令:!command# 比如列出当前目录下文件:!ls # 执行脚本:!perl -c script.pl 检查perl脚本语法，可以不用退出vim，非常方便。:!perl script.pl 执行perl脚本，可以不用退出vim，非常方便。:suspend或Ctrl - Z 挂起vim，回到shell，按fg可以返回vim。复制代码 20. 帮助命令12345678910111213141516在Unix/Linux系统上$ vimtutor# 普通模式下键盘输入vim或F1# 命令行模式下:help 显示整个帮助:help xxx 显示xxx的帮助，比如 :help i, :help CTRL-[（即Ctrl+[的帮助）。:help &apos;number&apos; Vim选项的帮助用单引号括起在Windows系统上:help tutor复制代码 21. 配置命令显示当前设定 123456789:set或者:se显示所有修改过的配置:set all 显示所有的设定值:set option? 显示option的设定值:set nooption 取消当期设定值:ver 显示vim的所有信息（包括版本和参数等）# 需要注意：全屏模式下:args 查看当前打开的文件列表，当前正在编辑的文件会用[]括起来复制代码 更改设定 12345678910111213141516171819202122232425262728293031323334:set nu 显示行号set autoindent(ai) 设置自动缩进set autowrite(aw) 设置自动存档，默认未打开set backup(bk) 设置自动备份，默认未打开set background=dark或light，设置背景风格set cindent(cin) 设置C语言风格缩进:set ts=4 设置tab键转换为4个空格:set ff=unix # 修改文件dos文件为unix:set shiftwidth? 查看缩进值:set shiftwidth=4 设置缩进值为4:set ignorecase&amp;emsp;&amp;emsp;忽略大小写的查找:set noignorecase&amp;emsp;&amp;emsp;不忽略大小写的查找:set paste # insert模式下，粘贴格式不会乱掉:set ruler?&amp;emsp;&amp;emsp;查看是否设置了ruler，在.vimrc中，使用set命令设制的选项都可以通过这个命令查看:scriptnames&amp;emsp;&amp;emsp;查看vim脚本文件的位置，比如.vimrc文件，语法文件及plugin等。:set list 显示非打印字符，如tab，空格，行尾等。如果tab无法显示，请确定用set lcs=tab:&gt;-命令设置了.vimrc文件，并确保你的文件中的确有tab，如果开启了expendtab，那么tab将被扩展为空格。:syntax 列出已经定义的语法项:syntax clear 清除已定义的语法规则:syntax case match 大小写敏感，int和Int将视为不同的语法元素:syntax case ignore 大小写无关，int和Int将视为相同的语法元素，并使用同样的配色方案]]></content>
      <categories>
        <category>linux基础</category>
      </categories>
      <tags>
        <tag>linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类和对象]]></title>
    <url>%2F2019%2F07%2F15%2F%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[类和对象面向对象编程介绍面向对象编程简称OOP，是一种程序设计思想，OOP把对象作为程序的基本单元，一个对象包含了数据和操作数据的函数，面向对像是一种对现实世界理解和抽象的方法。 面向过程是一种以过程为中心的编程思想。面向过程也可称之为面向记录编程思想，他们不支持丰富的“面向对象”特性，比如：继承，多态。并且它们不允许混含持久化状态和域逻辑。就是分析出解决问题所需要的步骤，然后用函数把这些步骤一步一步实现，使用的时候一个一个依次调用就可以了。 面向过程是一件事“该怎么做”，面向对象是一件事“该让谁来做”，然后那个“谁”就是对象，他要怎么做是他自己的事，反正最后一群对象合力把事做好就行了。 面向对象：狗吃（屎） 面向过程：吃（狗屎） 类和对象面向对象编程的2个非常重要的概念：类和对象 对象是面向对象编程的核心，在使用对象的过程中，为了将具有共同特性和行为的一组对象抽象定义，提出了另外一个新的概念————类 类就相当于制造飞机的图纸，用它来进行创建的飞机就相当于对象。 1.类人以类聚，物以群分。 具有相似内部状态和运行规律的实体的集合（或统称为抽象） 具有相同属性和行为事物的统称 类是抽象的，在使用的时候通常会找到这个类的一个具体的存在，一个类可以找到多个对象。 2,对象某一个具体事物的存在，在现实世界中可以看得见摸得着的。 可以是直接使用的。 3.类和对象之间的关系相当于玩具模型和某一个具体的玩具 4.区分类和对象比如： 12345678910奔驰汽车：类奔驰smart：类张三的那辆奔驰：对象狗：类李四家那只大黄狗:对象水果：类苹果：类红苹果：类红富士苹果：类我嘴里吃了一半的苹果：对象 5.类的构成类（class）由了个部分构成 （1）.类的名称 类名 （2）.类的属性 一组数据 （3）.类的方法 允许你进行操作的方法（行为） 比如： 123456781）人类设计，只关心3样东西1.事物名称（类名）:人（persion）2.属性:身高（height）、年龄（age）3.方法（行为/功能）：跑（run）、打架（fight）2)狗类的设计1.类名：狗（dog）2.属性：品种、毛色、性别、名字、腿儿的数量3.方法（行为/功能）:叫、跑、咬人、吃、摇尾巴 6.类的抽象拥有相同（或者类似）属性和行为的对象都可以抽象出一个类 方法：一般名词都是类（名词提炼法） 如： 12341.坦克发射3个3颗炮弹轰掉了2架飞机坦克、炮弹、飞机可以抽象成类2.小明在公交车上牵着一条叼着热狗的狗小明:人类，公车：交通工具类，热狗：食物类，狗：狗类]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk数组详解]]></title>
    <url>%2F2019%2F07%2F15%2Fawk%E6%95%B0%E7%BB%84%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[awk数组详解一般我们是通过数组的下标(或者称索引)，引用数组中的元素，有一些语言中，数组的下标由0开始，也就是说，如果想要引用数组中的第1个元素，则需要引用对就的下标”[0]”,awk中的数组也是通过引用下标的方法，获取数组中的元素的，但是在awk中，数组元素的下标默认是从1开始，但是为了兼容你的使用习惯，也可以从0开始设置下标。 在其它语言中，我们一般要先声明一个数组，在awk中我们可以直接为数组中的元素赋值如: 1awk &apos;BEGIN&#123;test[0]=1;test[1]=2;test[2]=3;print test[1]&#125;&apos; 上例中我们使用了BEGIN模式，其中有一个test数组，如果想要引用里面的值，只要用数组名加下标即可。linux中，命令行的换行符为反斜杠”&quot;。 我们也可以把数组的元素值设置为”空字符串”。如： 123awk &apos;BEGIN&#123;test[0]=1;test[1]=2;test[2]=&quot;&quot;;print test[2]&#125;&apos;注：上面一行没有东西输出是空值 当我们打印数组的第2个元素的值时，打印的值就是空（注：空格不为空） 根据以上说明：在awk中，将元素的值设置为”空字符串”是合法的。 在awk中，元素的值可以为空，那么我们就不能再根据元素的值是否为”空”去判断元素是否存在了。如： 12awk &apos;BEGIN&#123;test[0]=1;test[1]=2;test[2]=&quot;&quot;;if (test[2]==&quot;&quot;)print &quot;数组第3个元素不存在&quot;&#125;&apos;这是一种错误的写法 还有另外一个原因，就是当一个元素不存在于数组时，如果我们直接引用这个不存在的元素，awk会自动创建这个元素，并且默认认为这个元素赋值为“空字符串”，示例如下： 123awk &apos;BEGIN&#123;test[0]=1;test[1]=2;test[2]=&quot;&quot;;&#123;print test[3]&#125;&#125;&apos;上面一行没有东西输出是空值 如上，数组中没有第4个元素，但是当我们输出第4个元素是，输出了空，所以，出于此原因，在awk中使用之前的方法判断元素是否为空也是不合理的，因为当我们引用一个不存在的数组中的元素时，这个元素其实已经被赋值为“空字符串”了。如： 12awk &apos;BEGIN&#123;test[0]=1;test[1]=2;test[2]=&quot;&quot;;if(test[3]==&quot;&quot;)&#123;print &quot;数组第4个元素不存在 &quot;&#125;&apos;数组的第4个元素不存在 上面写法是错的怎样判断元素是否存在，我们使用如下语法： 1awk &apos;BEGIN&#123;test[0]=1;test[1]=2;test[2]=&quot;&quot;;if(2 in test)&#123;print &quot;数组第3个元素存在 &quot;&#125;&apos; 如上我们可以使用语法”if(下标 in 数组名)”，从而判断数组中是否存在对应的元素。当然我们可以使用”!”对条件进行取反，如： 1awk &apos;BEGIN&#123;test[0]=1;test[1]=2;test[2]=&quot;&quot;;if(!(2 in test))&#123;print &quot;数组第3个元素不存在 &quot;&#125;&apos; 在awk中，数组的下标不仅可以为”数字”，还可以为”任意字符串”，如果你使用过shell中的数组，你可以把awk的数组比作bash中的”关联数组”，如: 1awk &apos;BEGIN&#123;test[&quot;jon&quot;];test[&quot;son&quot;]&#125;&apos; 其实，awk中的数组本来就是”关联数组”，之所以先用以数字作为下标的数组举例，是为了让读者能够更好的过度，不过，以数字作为数组下标的数组在某些场景有一定的优势，但是它本质也是关联数组，awk默认会把”数字”下标转换为”字符串”，所以本质上它还是一个使用字符串作为下标的关联数组。 使用delete可以删除数组中的元素，如下： 12awk &apos;BEGIN&#123;test[&quot;jon&quot;]=&quot;j&quot;;test[&quot;son&quot;]=&quot;s&quot;;delete test[&quot;son&quot;];print test[&quot;son&quot;]&#125;&apos;#为空，也可以使用delete删除整个数组&quot;delete 数组名&quot; 在awk中想要输出数组中的所有元素，则需要借助for循环语句，有两种for循环语法如： 123456789#for循环语法格式1for(初始化; 布尔表达式; 更新) &#123;//代码语句&#125; #for循环语法格式2for(变量 in 数组) &#123;//代码语句&#125; 这两种for循环都能够遍历出数组中的元素，不过第一种for循环只能输出以数字作为下标的数组，如： 123awk &apos;BEGIN&#123;test[1]=&quot;jon&quot;;test[2]=&quot;son&quot;for(i=1;i&lt;=2;i++)&#123;print i,test[i]&#125;;&#125;&apos;1 jon2 son 利用for循环中的变量”i“数组中的下标都是数字的这一特性，按照顺序输出了数组中的元素值。 当数组中的元素的下标为”无规律的字符串“时，我们就可心使用for循环的第二种语法。 1awk &apos;BEGIN&#123;test[1]=&quot;jon&quot;;test[2]=&quot;son&quot;for(i in test)&#123;print i,test[i]&#125;;&#125;&apos; 注意，在这种语法中，for循环中的变量”i“表示的是元素的下标，而并非表示元素的值，所以，如果想要输出元素的值，则需要使用print 数组名[变量]。 当数组中的下标为字符串时，元素值输出的顺序与元素在数组中的顺序不同，这是因为awk中的数组本质上是关联数组，所以默认打印出来的元素是无序的。 实际应用在实际工作中，我们往往会使用数组，统计某些字符出现的次数，比如，统计日志中每个ip地址出现了多少次，我们就可以利用数组去统计。 在awk中当变量的值为字符串时，也可以进行加法运算，如果字符串参与运算，字符串将被当做数字0进行运算,如： 123awk &apos;BEGIN&#123;a=&quot;test&quot;,print a;a=a+1;print a;a++;print a&#125;&apos;12 当我们直接引用一个数组中不存在的元素时，awk会自动创建一个元素，并且为其赋值为”空字符串”。 12awk &apos;BEGIN&#123;pirnt testartt[&quot;ip&quot;];testarr[&quot;ip&quot;]++;print testarr[&quot;ip&quot;]&#125;&apos;2 当引用一个不存在的的元素时，元素被赋值为空字符串，当对这个元素进行自动运算时，元素的值就变成了1，因为，空字符串在参与运算时，被当做0使用了，所以，综上所述，我们对一个不存在的元素进行自加运算后，这个元素的值就变成了自加运算的次数，自加x次，元素的值就被赋值为x,自加y次,元素的值就被赋值为y.如： 12awk &apos;BEGIN&#123;print testarr[&quot;ip&quot;];testarr[&quot;ip&quot;]++;testarr[&quot;ip&quot;]++;print testarr[&quot;ip&quot;]&#125;&apos;2 如统计次数常用： 1awk &apos;&#123;a[$2]++&#125;END&#123;for(i in a)&#123;print i,a[i]&#125;&#125;&apos; 文件名 上面我们使用一个空模式，一个END模式。 空模式中，我们随便创建一个数组，并且将ip地址作为引用元素的下标，进行了引用，如果这个元素并不存在，它会被空模式中的动作处理完毕后，a[“198.168.1.1”]的值会被赋值为1. 由于END模式中的动作会最后执行，所以我们可以先不考虑END模式。 这时，空模式中的动作继续处理下一行。 当遇到相同的ip地址时，使用同样一个ip地址作为下标的元素将会再次被自加，每次遇到相同的ip地址，对应元素的值都会加1 直到处理完所有行，开始执行END模式中的动作。 而END模式中，我们打印出a数组中的所有元素的下标，以及元素对应的值。 此刻，a的数组中的下标即为ip地址，元素的值即为对应ip地址出现的次数。 最终就统计出每个ip地址出现的次数。 总结： 我们对一个不存在的元素进行自加运算后，这个元素的值就变成了自加运算的次数。]]></content>
      <categories>
        <category>linux基础</category>
      </categories>
      <tags>
        <tag>linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[定义类]]></title>
    <url>%2F2019%2F07%2F15%2F%E5%AE%9A%E4%B9%89%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[定义类定义一个类，格式如下： 12class 类名: 方法列表 说明: 1.定义类时有2种，新式类和经典类，上面那样定义为经典类，如果是类名(object)则为新式类 2.类名的命名规则按照“大驼峰” 创建对象格式:对象名=类名()#一定记住括号必须加上，没有括号不会创建对象 总结：bmw=Car(),这样就产生了一个Car的实例对象，一定在内存中有一块存放对象的 1.数据信息，换一句话创建一个对象是占内存的，些时也可以通过实例对象bmw来访问属性或方法。 2.给对象设置属性的时候，对象后面直接接属性名后面一个赋值符号然后接属性的值，依次还可以给多个对象赋值。 如：bmw color=’黑色’表示给bmw这个对象添加属性，如果后面再次出现bmw color=xxx表示对象属性进行修改 bmw是一个对象，它拥有属性（数据）和方法（函数） 3.一个对象可以有很多的属性，也可以有很多的函数 4.当创建一个对象时，就是用一个模具，来制造一个实物，如做月饼例子]]></content>
      <categories>
        <category>python基础</category>
      </categories>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo报错解决]]></title>
    <url>%2F2019%2F07%2F15%2Fhexo%E6%8A%A5%E9%94%99%E5%8E%9F%E5%9B%A0%2F</url>
    <content type="text"><![CDATA[第一种方法: 检查目录是否.git目录存在没有就手动生成 12git initgit config --global user.email &quot;1548702940@qq.com&quot; &amp;&amp; git config --global user.name &quot;wjg9786&quot; 第二种方法： 如果报Error: Spawn failed,在主目录删除.deploy_git文件夹并且执行hero clean后，重新hero deploy]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
